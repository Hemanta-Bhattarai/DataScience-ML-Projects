#Id||||og:title||||og:site_name||||citation_online_date||||citation_title||||citation_author||||citation_pdf_url||||citation_arxiv_id||||og:description||||twitter:site||||citation_date||||twitter:title||||twitter:description||||og:url||||category||||
0||||None||||Regression Discontinuity Design under Self-selection||||arXiv.org||||2019/11/21||||Regression Discontinuity Design under Self-selection||||Peng, Sida || Ning, Yang||||https://arxiv.org/pdf/1911.09248||||1911.09248||||In Regression Discontinuity (RD) design, self-selection leads to different distributions of covariates on two sides of the policy intervention, which essentially violates the continuity of potential outcome assumption. The standard RD estimand becomes difficult to interpret due to the existence of some indirect effect, i.e. the effect due to self selection. We show that the direct causal effect of interest can still be recovered under a class of estimands. Specifically, we consider a class of weighted average treatment effects tailored for potentially different target populations. We show that a special case of our estimands can recover the average treatment effect under the conditional independence assumption per Angrist and Rokkanen (2015), and another example is the estimand recently proposed in Frölich and Huber (2018). We propose a set of estimators through a weighted local linear regression framework and prove the consistency and asymptotic normality of the estimators. Our approach can be further extended to the fuzzy RD case. In simulation exercises, we compare the performance of our estimator with the standard RD estimator. Finally, we apply our method to two empirical data sets: the U.S. House elections data in Lee (2008) and a novel data set from Microsoft Bing on Generalized Second Price (GSP) auction.||||@arxiv||||2019/11/21||||Regression Discontinuity Design under Self-selection||||In Regression Discontinuity (RD) design, self-selection leads to different distributions of covariates on two sides of the policy intervention, which essentially violates the continuity of...||||https://arxiv.org/abs/1911.09248v1||||econ||||
1||||None||||Tests for qualitative features in the random coefficients model||||arXiv.org||||2018/03/14||||Tests for qualitative features in the random coefficients model||||Dunker, Fabian || Eckle, Konstantin || Proksch, Katharina || Schmidt-Hieber, Johannes||||https://arxiv.org/pdf/1704.01066||||1704.01066||||The random coefficients model is an extension of the linear regression model that allows for unobserved heterogeneity in the population by modeling the regression coefficients as random variables. Given data from this model, the statistical challenge is to recover information about the joint density of the random coefficients which is a multivariate and ill-posed problem. Because of the curse of dimensionality and the ill-posedness, pointwise nonparametric estimation of the joint density is difficult and suffers from slow convergence rates. Larger features, such as an increase of the density along some direction or a well-accentuated mode can, however, be much easier detected from data by means of statistical tests. In this article, we follow this strategy and construct tests and confidence statements for qualitative features of the joint density, such as increases, decreases and modes. We propose a multiple testing approach based on aggregating single tests which are designed to extract shape information on fixed scales and directions. Using recent tools for Gaussian approximations of multivariate empirical processes, we derive expressions for the critical value. We apply our method to simulated and real data.||||@arxiv||||2017/04/04||||Tests for qualitative features in the random coefficients model||||The random coefficients model is an extension of the linear regression model that allows for unobserved heterogeneity in the population by modeling the regression coefficients as random variables....||||https://arxiv.org/abs/1704.01066v3||||econ||||
2||||None||||Probabilistic Forecasting in Day-Ahead Electricity Markets: Simulating Peak and Off-Peak Prices||||arXiv.org||||2019/12/02||||Probabilistic Forecasting in Day-Ahead Electricity Markets: Simulating Peak and Off-Peak Prices||||Muniain, Peru || Ziel, Florian||||https://arxiv.org/pdf/1810.08418||||1810.08418||||In this paper we include dependency structures for electricity price forecasting and forecasting evaluation. We work with off-peak and peak time series from the German-Austrian day-ahead price, hence we analyze bivariate data. We first estimate the mean of the two time series, and then in a second step we estimate the residuals. The mean equation is estimated by OLS and elastic net and the residuals are estimated by maximum likelihood. Our contribution is to include a bivariate jump component on a mean reverting jump diffusion model in the residuals. The models' forecasts are evaluated using four different criteria, including the energy score to measure whether the correlation structure between the time series is properly included or not. In the results it is observed that the models with bivariate jumps provide better results with the energy score, which means that it is important to consider this structure in order to properly forecast correlated time series.||||@arxiv||||2018/10/19||||Probabilistic Forecasting in Day-Ahead Electricity Markets:...||||In this paper we include dependency structures for electricity price forecasting and forecasting evaluation. We work with off-peak and peak time series from the German-Austrian day-ahead price,...||||https://arxiv.org/abs/1810.08418v2||||econ||||
3||||None||||University rankings from the revealed preferences of the applicants||||arXiv.org||||2019/09/09||||University rankings from the revealed preferences of the applicants||||Csató, László || Tóth, Csaba||||https://arxiv.org/pdf/1810.04087||||1810.04087||||A methodology is presented to rank universities on the basis of the lists of programmes the students applied for. We exploit a crucial feature of the centralised assignment system to higher education in Hungary: a student is admitted to the first programme where the score limit is achieved. This makes it possible to derive a partial preference order of each applicant. Our approach integrates the information from all students participating in the system, is free of multicollinearity among the indicators, and contains few ad hoc parameters. The procedure is implemented to rank faculties in the Hungarian higher education between 2001 and 2016. We demonstrate that the ranking given by the least squares method has favourable theoretical properties, is robust with respect to the aggregation of preferences, and performs well in practice. The suggested ranking is worth considering as a reasonable alternative to the standard composite indices.||||@arxiv||||2018/10/09||||University rankings from the revealed preferences of the applicants||||A methodology is presented to rank universities on the basis of the lists of programmes the students applied for. We exploit a crucial feature of the centralised assignment system to higher...||||https://arxiv.org/abs/1810.04087v5||||econ||||
4||||None||||Regularized Estimation of High-dimensional Factor-Augmented Autoregressive (FAVAR) Models||||arXiv.org||||2019/12/09||||Regularized Estimation of High-dimensional Factor-Augmented Autoregressive (FAVAR) Models||||Lin, Jiahe || Michailidis, George||||https://arxiv.org/pdf/1912.04146||||1912.04146||||A factor-augmented vector autoregressive (FAVAR) model is defined by a VAR equation that captures lead-lag correlations amongst a set of observed variables $X$ and latent factors $F$, and a calibration equation that relates another set of observed variables $Y$ with $F$ and $X$. The latter equation is used to estimate the factors that are subsequently used in estimating the parameters of the VAR system. The FAVAR model has become popular in applied economic research, since it can summarize a large number of variables of interest as a few factors through the calibration equation and subsequently examine their influence on core variables of primary interest through the VAR equation. However, there is increasing need for examining lead-lag relationships between a large number of time series, while incorporating information from another high-dimensional set of variables. Hence, in this paper we investigate the FAVAR model under high-dimensional scaling. We introduce an appropriate identification constraint for the model parameters, which when incorporated into the formulated optimization problem yields estimates with good statistical properties. Further, we address a number of technical challenges introduced by the fact that estimates of the VAR system model parameters are based on estimated rather than directly observed quantities. The performance of the proposed estimators is evaluated on synthetic data. Further, the model is applied to commodity prices and reveals interesting and interpretable relationships between the prices and the factors extracted from a set of global macroeconomic indicators.||||@arxiv||||2019/12/09||||Regularized Estimation of High-dimensional Factor-Augmented...||||A factor-augmented vector autoregressive (FAVAR) model is defined by a VAR equation that captures lead-lag correlations amongst a set of observed variables $X$ and latent factors $F$, and a...||||https://arxiv.org/abs/1912.04146v1||||econ||||
5||||None||||Ramsey Optimal Policy versus Multiple Equilibria with Fiscal and Monetary Interactions||||arXiv.org||||2020/02/11||||Ramsey Optimal Policy versus Multiple Equilibria with Fiscal and Monetary Interactions||||Chatelain, Jean-Bernard || Ralf, Kirsten||||https://arxiv.org/pdf/2002.04508||||2002.04508||||We consider a frictionless constant endowment economy based on Leeper (1991). In this economy, it is shown that, under an ad-hoc monetary rule and an ad-hoc fiscal rule, there are two equilibria. One has active monetary policy and passive fiscal policy, while the other has passive monetary policy and active fiscal policy. We consider an extended setup in which the policy maker minimizes a loss function under quasi-commitment, as in Schaumburg and Tambalotti (2007). Under this formulation there exists a unique Ramsey equilibrium, with an interest rate peg and a passive fiscal policy. We thank John P. Conley, Luis de Araujo and one referree for their very helpful comments.||||@arxiv||||2020/02/11||||Ramsey Optimal Policy versus Multiple Equilibria with Fiscal and...||||We consider a frictionless constant endowment economy based on Leeper (1991). In this economy, it is shown that, under an ad-hoc monetary rule and an ad-hoc fiscal rule, there are two equilibria....||||https://arxiv.org/abs/2002.04508v1||||econ||||
6||||None||||On Heckits, LATE, and Numerical Equivalence||||arXiv.org||||2018/10/24||||On Heckits, LATE, and Numerical Equivalence||||Kline, Patrick || Walters, Christopher R.||||https://arxiv.org/pdf/1706.05982||||1706.05982||||Structural econometric methods are often criticized for being sensitive to functional form assumptions. We study parametric estimators of the local average treatment effect (LATE) derived from a widely used class of latent threshold crossing models and show they yield LATE estimates algebraically equivalent to the instrumental variables (IV) estimator. Our leading example is Heckman's (1979) two-step ("Heckit") control function estimator which, with two-sided non-compliance, can be used to compute estimates of a variety of causal parameters. Equivalence with IV is established for a semi-parametric family of control function estimators and shown to hold at interior solutions for a class of maximum likelihood estimators. Our results suggest differences between structural and IV estimates often stem from disagreements about the target parameter rather than from functional form assumptions per se. In cases where equivalence fails, reporting structural estimates of LATE alongside IV provides a simple means of assessing the credibility of structural extrapolation exercises.||||@arxiv||||2017/06/19||||On Heckits, LATE, and Numerical Equivalence||||Structural econometric methods are often criticized for being sensitive to functional form assumptions. We study parametric estimators of the local average treatment effect (LATE) derived from a...||||https://arxiv.org/abs/1706.05982v4||||econ||||
7||||None||||Forecasting the Impact of Connected and Automated Vehicles on Energy Use A Microeconomic Study of Induced Travel and Energy Rebound||||arXiv.org||||2019/05/09||||Forecasting the Impact of Connected and Automated Vehicles on Energy Use A Microeconomic Study of Induced Travel and Energy Rebound||||Taiebat, Morteza || Stolper, Samuel || Xu, Ming||||https://arxiv.org/pdf/1902.00382||||1902.00382||||Connected and automated vehicles (CAVs) are expected to yield significant improvements in safety, energy efficiency, and time utilization. However, their net effect on energy and environmental outcomes is unclear. Higher fuel economy reduces the energy required per mile of travel, but it also reduces the fuel cost of travel, incentivizing more travel and causing an energy "rebound effect." Moreover, CAVs are predicted to vastly reduce the time cost of travel, inducing further increases in travel and energy use. In this paper, we forecast the induced travel and rebound from CAVs using data on existing travel behavior. We develop a microeconomic model of vehicle miles traveled (VMT) choice under income and time constraints; then we use it to estimate elasticities of VMT demand with respect to fuel and time costs, with fuel cost data from the 2017 United States National Household Travel Survey (NHTS) and wage-derived predictions of travel time cost. Our central estimate of the combined price elasticity of VMT demand is -0.4, which differs substantially from previous estimates. We also find evidence that wealthier households have more elastic demand, and that households at all income levels are more sensitive to time costs than to fuel costs. We use our estimated elasticities to simulate VMT and energy use impacts of full, private CAV adoption under a range of possible changes to the fuel and time costs of travel. We forecast a 2-47% increase in travel demand for an average household. Our results indicate that backfire - i.e., a net rise in energy use - is a possibility, especially in higher income groups. This presents a stiff challenge to policy goals for reductions in not only energy use but also traffic congestion and local and global air pollution, as CAV use increases.||||@arxiv||||2019/01/31||||Forecasting the Impact of Connected and Automated Vehicles on...||||Connected and automated vehicles (CAVs) are expected to yield significant improvements in safety, energy efficiency, and time utilization. However, their net effect on energy and environmental...||||https://arxiv.org/abs/1902.00382v4||||econ||||
8||||None||||Many Average Partial Effects: with An Application to Text Regression||||arXiv.org||||2019/09/27||||Many Average Partial Effects: with An Application to Text Regression||||Chiang, Harold D.||||https://arxiv.org/pdf/1812.09397||||1812.09397||||We study estimation, pointwise and simultaneous inference, and confidence intervals for many average partial effects of lasso Logit. Focusing on high-dimensional, cluster-sampling environments, we propose a new average partial effect estimator and explore its asymptotic properties. Practical penalty choices compatible with our asymptotic theory are also provided. The proposed estimator allow for valid inference without requiring oracle property. We provide easy-to-implement algorithms for cluster-robust high-dimensional hypothesis testing and construction of simultaneously valid confidence intervals using a multiplier cluster bootstrap. We apply the proposed algorithms to the text regression model of Wu (2018) to examine the presence of gendered language on the internet.||||@arxiv||||2018/12/21||||Many Average Partial Effects: with An Application to Text Regression||||We study estimation, pointwise and simultaneous inference, and confidence intervals for many average partial effects of lasso Logit. Focusing on high-dimensional, cluster-sampling environments, we...||||https://arxiv.org/abs/1812.09397v4||||econ||||
9||||None||||Multivariate Stochastic Volatility Model with Realized Volatilities and Pairwise Realized Correlations||||arXiv.org||||2019/03/13||||Multivariate Stochastic Volatility Model with Realized Volatilities and Pairwise Realized Correlations||||Yamauchi, Yuta || Omori, Yasuhiro||||https://arxiv.org/pdf/1809.09928||||1809.09928||||Although stochastic volatility and GARCH (generalized autoregressive conditional heteroscedasticity) models have successfully described the volatility dynamics of univariate asset returns, extending them to the multivariate models with dynamic correlations has been difficult due to several major problems. First, there are too many parameters to estimate if available data are only daily returns, which results in unstable estimates. One solution to this problem is to incorporate additional observations based on intraday asset returns, such as realized covariances. Second, since multivariate asset returns are not synchronously traded, we have to use the largest time intervals such that all asset returns are observed in order to compute the realized covariance matrices. However, in this study, we fail to make full use of the available intraday informations when there are less frequently traded assets. Third, it is not straightforward to guarantee that the estimated (and the realized) covariance matrices are positive definite. Our contributions are the following: (1) we obtain the stable parameter estimates for the dynamic correlation models using the realized measures, (2) we make full use of intraday informations by using pairwise realized correlations, (3) the covariance matrices are guaranteed to be positive definite, (4) we avoid the arbitrariness of the ordering of asset returns, (5) we propose the flexible correlation structure model (e.g., such as setting some correlations to be zero if necessary), and (6) the parsimonious specification for the leverage effect is proposed. Our proposed models are applied to the daily returns of nine U.S. stocks with their realized volatilities and pairwise realized correlations and are shown to outperform the existing models with respect to portfolio performances.||||@arxiv||||2018/09/26||||Multivariate Stochastic Volatility Model with Realized...||||Although stochastic volatility and GARCH (generalized autoregressive conditional heteroscedasticity) models have successfully described the volatility dynamics of univariate asset returns,...||||https://arxiv.org/abs/1809.09928v2||||econ||||
10||||None||||Synthetic Controls with Imperfect Pre-Treatment Fit||||arXiv.org||||2019/11/19||||Synthetic Controls with Imperfect Pre-Treatment Fit||||Ferman, Bruno || Pinto, Cristine||||https://arxiv.org/pdf/1911.08521||||1911.08521||||We analyze the properties of the Synthetic Control (SC) and related estimators when the pre-treatment fit is imperfect. In this framework, we show that these estimators are generally biased if treatment assignment is correlated with unobserved confounders, even when the number of pre-treatment periods goes to infinity. Still, we also show that a modified version of the SC method can substantially improve in terms of bias and variance relative to the difference-in-difference estimator. We also consider the properties of these estimators in settings with non-stationary common factors.||||@arxiv||||2019/11/19||||Synthetic Controls with Imperfect Pre-Treatment Fit||||We analyze the properties of the Synthetic Control (SC) and related estimators when the pre-treatment fit is imperfect. In this framework, we show that these estimators are generally biased if...||||https://arxiv.org/abs/1911.08521v1||||econ||||
11||||None||||Time will tell - Recovering Preferences when Choices are Noisy||||arXiv.org||||2018/11/06||||Time will tell - Recovering Preferences when Choices are Noisy||||Alos-Ferrer, Carlos || Fehr, Ernst || Netzer, Nick||||https://arxiv.org/pdf/1811.02497||||1811.02497||||The ability to uncover preferences from choices is fundamental for both positive economics and welfare analysis. Overwhelming evidence shows that choice is stochastic, which has given rise to random utility models as the dominant paradigm in applied microeconomics. However, as is well known, it is not possible to infer the structure of preferences in the absence of assumptions on the structure of noise. This makes it impossible to empirically test the structure of noise independently from the structure of preferences. Here, we show that the difficulty can be bypassed if data sets are enlarged to include response times. A simple condition on response time distributions (a weaker version of first order stochastic dominance) ensures that choices reveal preferences without assumptions on the structure of utility noise. Sharper results are obtained if the analysis is restricted to specific classes of models. Under symmetric noise, response times allow to uncover preferences for choice pairs outside the data set, and if noise is Fechnerian, even choice probabilities can be forecast out of sample. We conclude by showing that standard random utility models from economics and standard drift-diffusion models from psychology necessarily generate data sets fulfilling our sufficient condition on response time distributions.||||@arxiv||||2018/11/06||||Time will tell - Recovering Preferences when Choices are Noisy||||The ability to uncover preferences from choices is fundamental for both positive economics and welfare analysis. Overwhelming evidence shows that choice is stochastic, which has given rise to...||||https://arxiv.org/abs/1811.02497v1||||econ||||
12||||None||||The interplay between migrants and natives as a determinant of migrants' assimilation: A coevolutionary approach||||arXiv.org||||2019/06/06||||The interplay between migrants and natives as a determinant of migrants' assimilation: A coevolutionary approach||||Bielawski, Jakub || Jakubek, Marcin||||https://arxiv.org/pdf/1906.02657||||1906.02657||||We study the migrants' assimilation, which we conceptualize as forming human capital productive on the labor market of a developed host country, and we link the observed frequent lack of assimilation with the relative deprivation that the migrants start to feel when they move in social space towards the natives. In turn, we presume that the native population is heterogenous and consists of high-skill and low-skill workers. The presence of assimilated migrants might shape the comparison group of the natives, influencing the relative deprivation of the low-skill workers and, in consequence, the choice to form human capital and become highly skilled. To analyse this interrelation between assimilation choices of migrants and skill formation of natives, we construct a coevolutionary model of the open-to-migration economy. Showing that the economy might end up in a non-assimilation equilibrium, we discuss welfare consequences of an assimilation policy funded from tax levied on the native population. We identify conditions under which such costly policy can bring the migrants to assimilation and at the same time increase the welfare of the natives, even though the incomes of the former take a beating.||||@arxiv||||2019/06/06||||The interplay between migrants and natives as a determinant of...||||We study the migrants' assimilation, which we conceptualize as forming human capital productive on the labor market of a developed host country, and we link the observed frequent lack of...||||https://arxiv.org/abs/1906.02657v1||||econ||||
13||||None||||The method of Eneström and Phragmén for parliamentary elections by means of approval voting||||arXiv.org||||2019/07/23||||The method of Enestr\"om and Phragm\'en for parliamentary elections by means of approval voting||||Camps, Rosa || Mora, Xavier || Saumell, Laia||||https://arxiv.org/pdf/1907.10590||||1907.10590||||We study a method for proportional representation that was proposed at the turn from the nineteenth to the twentieth century by Gustav Eneström and Edvard Phragmén. Like Phragmén's better-known iterative minimax method, it is assumed that the voters express themselves by means of approval voting. In contrast to the iterative minimax method, however, here one starts by fixing a quota, i.e. the number of votes that give the right to a seat. As a matter of fact, the method of Eneström and Phragmén can be seen as an extension of the method of largest remainders from closed lists to open lists, or also as an adaptation of the single transferable vote to approval rather than preferential voting. The properties of this method are studied and compared with those of other methods of the same kind.||||@arxiv||||2019/07/23||||The method of Eneström and Phragmén for parliamentary elections by...||||We study a method for proportional representation that was proposed at the turn from the nineteenth to the twentieth century by Gustav Eneström and Edvard Phragmén. Like Phragmén's...||||https://arxiv.org/abs/1907.10590v1||||econ||||
14||||None||||Analyzing Commodity Futures Using Factor State-Space Models with Wishart Stochastic Volatility||||arXiv.org||||2019/08/21||||Analyzing Commodity Futures Using Factor State-Space Models with Wishart Stochastic Volatility||||Kleppe, Tore Selland || Liesenfeld, Roman || Moura, Guilherme Valle || Oglend, Atle||||https://arxiv.org/pdf/1908.07798||||1908.07798||||We propose a factor state-space approach with stochastic volatility to model and forecast the term structure of future contracts on commodities. Our approach builds upon the dynamic 3-factor Nelson-Siegel model and its 4-factor Svensson extension and assumes for the latent level, slope and curvature factors a Gaussian vector autoregression with a multivariate Wishart stochastic volatility process. Exploiting the conjugacy of the Wishart and the Gaussian distribution, we develop a computationally fast and easy to implement MCMC algorithm for the Bayesian posterior analysis. An empirical application to daily prices for contracts on crude oil with stipulated delivery dates ranging from one to 24 months ahead show that the estimated 4-factor Svensson model with two curvature factors provides a good parsimonious representation of the serial correlation in the individual prices and their volatility. It also shows that this model has a good out-of-sample forecast performance.||||@arxiv||||2019/08/21||||Analyzing Commodity Futures Using Factor State-Space Models with...||||We propose a factor state-space approach with stochastic volatility to model and forecast the term structure of future contracts on commodities. Our approach builds upon the dynamic 3-factor...||||https://arxiv.org/abs/1908.07798v1||||econ||||
15||||None||||Overconfidence and Prejudice||||arXiv.org||||2019/09/18||||Overconfidence and Prejudice||||Heidhues, Paul || Kőszegi, Botond || Strack, Philipp||||https://arxiv.org/pdf/1909.08497||||1909.08497||||We explore conclusions a person draws from observing society when he allows for the possibility that individuals' outcomes are affected by group-level discrimination. Injecting a single non-classical assumption, that the agent is overconfident about himself, we explain key observed patterns in social beliefs, and make a number of additional predictions. First, the agent believes in discrimination against any group he is in more than an outsider does, capturing widely observed self-centered views of discrimination. Second, the more group memberships the agent shares with an individual, the more positively he evaluates the individual. This explains one of the most basic facts about social judgments, in-group bias, as well as "legitimizing myths" that justify an arbitrary social hierarchy through the perceived superiority of the privileged group. Third, biases are sensitive to how the agent divides society into groups when evaluating outcomes. This provides a reason why some ethnically charged questions should not be asked, as well as a potential channel for why nation-building policies might be effective. Fourth, giving the agent more accurate information about himself increases all his biases. Fifth, the agent is prone to substitute biases, implying that the introduction of a new outsider group to focus on creates biases against the new group but lowers biases vis a vis other groups. Sixth, there is a tendency for the agent to agree more with those in the same groups. As a microfoundation for our model, we provide an explanation for why an overconfident agent might allow for potential discrimination in evaluating outcomes, even when he initially did not conceive of this possibility.||||@arxiv||||2019/09/18||||Overconfidence and Prejudice||||We explore conclusions a person draws from observing society when he allows for the possibility that individuals' outcomes are affected by group-level discrimination. Injecting a single...||||https://arxiv.org/abs/1909.08497v1||||econ||||
16||||None||||Control Variables, Discrete Instruments, and Identification of Structural Functions||||arXiv.org||||2019/12/05||||Control Variables, Discrete Instruments, and Identification of Structural Functions||||Newey, Whitney || Stouli, Sami||||https://arxiv.org/pdf/1809.05706||||1809.05706||||Control variables provide an important means of controlling for endogeneity in econometric models with nonseparable and/or multidimensional heterogeneity. We allow for discrete instruments, giving identification results under a variety of restrictions on the way the endogenous variable and the control variables affect the outcome. We consider many structural objects of interest, such as average or quantile treatment effects. We illustrate our results with an empirical application to Engel curve estimation.||||@arxiv||||2018/09/15||||Control Variables, Discrete Instruments, and Identification of...||||Control variables provide an important means of controlling for endogeneity in econometric models with nonseparable and/or multidimensional heterogeneity. We allow for discrete instruments, giving...||||https://arxiv.org/abs/1809.05706v2||||econ||||
17||||None||||The Persuasion Duality||||arXiv.org||||2019/10/24||||The Persuasion Duality||||Dworczak, Piotr || Kolotilin, Anton||||https://arxiv.org/pdf/1910.11392||||1910.11392||||We present a unified duality approach to Bayesian persuasion. The optimal dual variable, interpreted as a price function, is shown to be a supergradient of the concave closure of the objective function at the prior belief. Under regularity conditions, our general duality result implies known results for the case when the objective function depends only on the expected state. We apply our approach to characterize the optimal signal in the case when the state is two-dimensional.||||@arxiv||||2019/10/24||||The Persuasion Duality||||We present a unified duality approach to Bayesian persuasion. The optimal dual variable, interpreted as a price function, is shown to be a supergradient of the concave closure of the objective...||||https://arxiv.org/abs/1910.11392v1||||econ||||
18||||None||||Bayesian Estimation of Mixed Multinomial Logit Models: Advances and Simulation-Based Evaluations||||arXiv.org||||2019/12/12||||Bayesian Estimation of Mixed Multinomial Logit Models: Advances and Simulation-Based Evaluations||||Bansal, Prateek || Krueger, Rico || Bierlaire, Michel || Daziano, Ricardo A. || Rashidi, Taha H.||||https://arxiv.org/pdf/1904.03647||||1904.03647||||Variational Bayes (VB) methods have emerged as a fast and computationally-efficient alternative to Markov chain Monte Carlo (MCMC) methods for scalable Bayesian estimation of mixed multinomial logit (MMNL) models. It has been established that VB is substantially faster than MCMC at practically no compromises in predictive accuracy. In this paper, we address two critical gaps concerning the usage and understanding of VB for MMNL. First, extant VB methods are limited to utility specifications involving only individual-specific taste parameters. Second, the finite-sample properties of VB estimators and the relative performance of VB, MCMC and maximum simulated likelihood estimation (MSLE) are not known. To address the former, this study extends several VB methods for MMNL to admit utility specifications including both fixed and random utility parameters. To address the latter, we conduct an extensive simulation-based evaluation to benchmark the extended VB methods against MCMC and MSLE in terms of estimation times, parameter recovery and predictive accuracy. The results suggest that all VB variants with the exception of the ones relying on an alternative variational lower bound constructed with the help of the modified Jensen's inequality perform as well as MCMC and MSLE at prediction and parameter recovery. In particular, VB with nonconjugate variational message passing and the delta-method (VB-NCVMP-Delta) is up to 16 times faster than MCMC and MSLE. Thus, VB-NCVMP-Delta can be an attractive alternative to MCMC and MSLE for fast, scalable and accurate estimation of MMNL models.||||@arxiv||||2019/04/07||||Bayesian Estimation of Mixed Multinomial Logit Models: Advances...||||Variational Bayes (VB) methods have emerged as a fast and computationally-efficient alternative to Markov chain Monte Carlo (MCMC) methods for scalable Bayesian estimation of mixed multinomial...||||https://arxiv.org/abs/1904.03647v4||||cs||||
19||||None||||Towards a General Large Sample Theory for Regularized Estimators||||arXiv.org||||2019/11/21||||Towards a General Large Sample Theory for Regularized Estimators||||Jansson, Michael || Pouzo, Demian||||https://arxiv.org/pdf/1712.07248||||1712.07248||||We present a general framework for studying regularized estimators; such estimators are pervasive in estimation problems wherein "plug-in" type estimators are either ill-defined or ill-behaved. Within this framework, we derive, under primitive conditions, consistency and a generalization of the asymptotic linearity property. We also provide data-driven methods for choosing tuning parameters that, under some conditions, achieve the aforementioned properties. We illustrate the scope of our approach by studying a wide range of applications, revisiting known results and deriving new ones.||||@arxiv||||2017/12/19||||Towards a General Large Sample Theory for Regularized Estimators||||We present a general framework for studying regularized estimators; such estimators are pervasive in estimation problems wherein "plug-in" type estimators are either ill-defined or ill-behaved....||||https://arxiv.org/abs/1712.07248v3||||econ||||
20||||None||||Estimation of Dynamic Panel Threshold Model using Stata||||arXiv.org||||2019/02/27||||Estimation of Dynamic Panel Threshold Model using Stata||||Seo, Myung Hwan || Kim, Sueyoul || Kim, Young-Joo||||https://arxiv.org/pdf/1902.10318||||1902.10318||||We develop a Stata command xthenreg to implement the first-differenced GMM estimation of the dynamic panel threshold model, which Seo and Shin (2016, Journal of Econometrics 195: 169-186) have proposed. Furthermore, We derive the asymptotic variance formula for a kink constrained GMM estimator of the dynamic threshold model and include an estimation algorithm. We also propose a fast bootstrap algorithm to implement the bootstrap for the linearity test. The use of the command is illustrated through a Monte Carlo simulation and an economic application.||||@arxiv||||2019/02/27||||Estimation of Dynamic Panel Threshold Model using Stata||||We develop a Stata command xthenreg to implement the first-differenced GMM estimation of the dynamic panel threshold model, which Seo and Shin (2016, Journal of Econometrics 195: 169-186) have...||||https://arxiv.org/abs/1902.10318v1||||econ||||
21||||None||||Sensitivity Analysis using Approximate Moment Condition Models||||arXiv.org||||2019/02/15||||Sensitivity Analysis using Approximate Moment Condition Models||||Armstrong, Timothy B. || Kolesár, Michal||||https://arxiv.org/pdf/1808.07387||||1808.07387||||We consider inference in models defined by approximate moment conditions. We show that near-optimal confidence intervals (CIs) can be formed by taking a generalized method of moments (GMM) estimator, and adding and subtracting the standard error times a critical value that takes into account the potential bias from misspecification of the moment conditions. In order to optimize performance under potential misspecification, the weighting matrix for this GMM estimator takes into account this potential bias, and therefore differs from the one that is optimal under correct specification. To formally show the near-optimality of these CIs, we develop asymptotic efficiency bounds for inference in the locally misspecified GMM setting. These bounds may be of independent interest, due to their implications for the possibility of using moment selection procedures when conducting inference in moment condition models. We apply our methods in an empirical application to automobile demand, and show that adjusting the weighting matrix can shrink the CIs by a factor of 3 or more.||||@arxiv||||2018/08/22||||Sensitivity Analysis using Approximate Moment Condition Models||||We consider inference in models defined by approximate moment conditions. We show that near-optimal confidence intervals (CIs) can be formed by taking a generalized method of moments (GMM)...||||https://arxiv.org/abs/1808.07387v3||||econ||||
22||||None||||Singularities and Catastrophes in Economics: Historical Perspectives and Future Directions||||arXiv.org||||2019/07/12||||Singularities and Catastrophes in Economics: Historical Perspectives and Future Directions||||Harré, Michael S. || Harris, Adam || McCallum, Scott||||https://arxiv.org/pdf/1907.05582||||1907.05582||||Economic theory is a mathematically rich field in which there are opportunities for the formal analysis of singularities and catastrophes. This article looks at the historical context of singularities through the work of two eminent Frenchmen around the late 1960s and 1970s. René Thom (1923-2002) was an acclaimed mathematician having received the Fields Medal in 1958, whereas Gérard Debreu (1921-2004) would receive the Nobel Prize in economics in 1983. Both were highly influential within their fields and given the fundamental nature of their work, the potential for cross-fertilisation would seem to be quite promising. This was not to be the case: Debreu knew of Thom's work and cited it in the analysis of his own work, but despite this and other applied mathematicians taking catastrophe theory to economics, the theory never achieved a lasting following and relatively few results were published. This article reviews Debreu's analysis of the so called ${\it regular}$ and ${\it crtitical}$ economies in order to draw some insights into the economic perspective of singularities before moving to how singularities arise naturally in the Nash equilibria of game theory. Finally a modern treatment of stochastic game theory is covered through recent work on the quantal response equilibrium. In this view the Nash equilibrium is to the quantal response equilibrium what deterministic catastrophe theory is to stochastic catastrophe theory, with some caveats regarding when this analogy breaks down discussed at the end.||||@arxiv||||2019/07/12||||Singularities and Catastrophes in Economics: Historical...||||Economic theory is a mathematically rich field in which there are opportunities for the formal analysis of singularities and catastrophes. This article looks at the historical context of...||||https://arxiv.org/abs/1907.05582v1||||econ||||
23||||None||||Forecasting Time Series with VARMA Recursions on Graphs||||arXiv.org||||2019/07/10||||Forecasting Time Series with VARMA Recursions on Graphs||||Isufi, Elvin || Loukas, Andreas || Perraudin, Nathanael || Leus, Geert||||https://arxiv.org/pdf/1810.08581||||1810.08581||||Graph-based techniques emerged as a choice to deal with the dimensionality issues in modeling multivariate time series. However, there is yet no complete understanding of how the underlying structure could be exploited to ease this task. This work provides contributions in this direction by considering the forecasting of a process evolving over a graph. We make use of the (approximate) time-vertex stationarity assumption, i.e., timevarying graph signals whose first and second order statistical moments are invariant over time and correlated to a known graph topology. The latter is combined with VAR and VARMA models to tackle the dimensionality issues present in predicting the temporal evolution of multivariate time series. We find out that by projecting the data to the graph spectral domain: (i) the multivariate model estimation reduces to that of fitting a number of uncorrelated univariate ARMA models and (ii) an optimal low-rank data representation can be exploited so as to further reduce the estimation costs. In the case that the multivariate process can be observed at a subset of nodes, the proposed models extend naturally to Kalman filtering on graphs allowing for optimal tracking. Numerical experiments with both synthetic and real data validate the proposed approach and highlight its benefits over state-of-the-art alternatives.||||@arxiv||||2018/10/19||||Forecasting Time Series with VARMA Recursions on Graphs||||Graph-based techniques emerged as a choice to deal with the dimensionality issues in modeling multivariate time series. However, there is yet no complete understanding of how the underlying...||||https://arxiv.org/abs/1810.08581v2||||cs||||
24||||None||||Payoff Information and Learning in Signaling Games||||arXiv.org||||2020/01/14||||Payoff Information and Learning in Signaling Games||||Fudenberg, Drew || He, Kevin||||https://arxiv.org/pdf/1709.01024||||1709.01024||||We add the assumption that players know their opponents' payoff functions and rationality to a model of non-equilibrium learning in signaling games. Agents are born into player roles and play against random opponents every period. Inexperienced agents are uncertain about the prevailing distribution of opponents' play, but believe that opponents never choose conditionally dominated strategies. Agents engage in active learning and update beliefs based on personal observations. Payoff information can refine or expand learning predictions, since patient young senders' experimentation incentives depend on which receiver responses they deem plausible. We show that with payoff knowledge, the limiting set of long-run learning outcomes is bounded above by rationality-compatible equilibria (RCE), and bounded below by uniform RCE. RCE refine the Intuitive Criterion (Cho and Kreps, 1987) and include all divine equilibria (Banks and Sobel, 1987). Uniform RCE sometimes but not always exists, and implies universally divine equilibrium.||||@arxiv||||2017/08/31||||Payoff Information and Learning in Signaling Games||||We add the assumption that players know their opponents' payoff functions and rationality to a model of non-equilibrium learning in signaling games. Agents are born into player roles and play...||||https://arxiv.org/abs/1709.01024v5||||econ||||
25||||None||||Forecasting U.S. Textile Comparative Advantage Using Autoregressive Integrated Moving Average Models and Time Series Outlier Analysis||||arXiv.org||||2019/08/13||||Forecasting U.S. Textile Comparative Advantage Using Autoregressive Integrated Moving Average Models and Time Series Outlier Analysis||||Saki, Zahra || Rothenberg, Lori || Moor, Marguerite || Kandilov, Ivan || Godfrey, A. Blanton||||https://arxiv.org/pdf/1908.04852||||1908.04852||||To establish an updated understanding of the U.S. textile and apparel (TAP) industrys competitive position within the global textile environment, trade data from UN-COMTRADE (1996-2016) was used to calculate the Normalized Revealed Comparative Advantage (NRCA) index for 169 TAP categories at the four-digit Harmonized Schedule (HS) code level. Univariate time series using Autoregressive Integrated Moving Average (ARIMA) models forecast short-term future performance of Revealed categories with export advantage. Accompanying outlier analysis examined permanent level shifts that might convey important information about policy changes, influential drivers and random events.||||@arxiv||||2019/08/13||||Forecasting U.S. Textile Comparative Advantage Using...||||To establish an updated understanding of the U.S. textile and apparel (TAP) industrys competitive position within the global textile environment, trade data from UN-COMTRADE (1996-2016) was used...||||https://arxiv.org/abs/1908.04852v1||||econ||||
26||||None||||Optimal Dynamic Treatment Regimes and Partial Welfare Ordering||||arXiv.org||||2019/12/20||||Optimal Dynamic Treatment Regimes and Partial Welfare Ordering||||Han, Sukjin||||https://arxiv.org/pdf/1912.10014||||1912.10014||||Dynamic treatment regimes are treatment allocations tailored to heterogeneous individuals. The optimal dynamic treatment regime is a regime that maximizes counterfactual welfare. This paper investigates the possibility of identification of optimal dynamic regimes when data are generated from sequential (natural) experiments. We propose a framework in which we can partially learn the optimal dynamic regime and ordering of welfares, relaxing sequential randomization assumptions commonly employed in the literature. We establish the sharp partial ordering of counterfactual welfares with respect to dynamic regimes by using a series of linear programs. A distinct feature of our approach is that, instead of solving a large number of large-scale linear programs, we provide simple analytical conditions for the ordering. The identified set of the optimal regime is then characterized as the set of maximal elements of the partial order. We also propose topological sorts of the partial order as a policy menu. We show how policymaking can be further guided by imposing assumptions such as monotonicity/uniformity of different stringency, agent's learning, Markovian structure, and stationarity.||||@arxiv||||2019/12/20||||Optimal Dynamic Treatment Regimes and Partial Welfare Ordering||||Dynamic treatment regimes are treatment allocations tailored to heterogeneous individuals. The optimal dynamic treatment regime is a regime that maximizes counterfactual welfare. This paper...||||https://arxiv.org/abs/1912.10014v1||||econ||||
27||||None||||Learning and Type Compatibility in Signaling Games||||arXiv.org||||2018/06/30||||Learning and Type Compatibility in Signaling Games||||Fudenberg, Drew || He, Kevin||||https://arxiv.org/pdf/1702.01819||||1702.01819||||Which equilibria will arise in signaling games depends on how the receiver interprets deviations from the path of play. We develop a micro-foundation for these off-path beliefs, and an associated equilibrium refinement, in a model where equilibrium arises through non-equilibrium learning by populations of patient and long-lived senders and receivers. In our model, young senders are uncertain about the prevailing distribution of play, so they rationally send out-of-equilibrium signals as experiments to learn about the behavior of the population of receivers. Differences in the payoff functions of the types of senders generate different incentives for these experiments. Using the Gittins index (Gittins, 1979), we characterize which sender types use each signal more often, leading to a constraint on the receiver's off-path beliefs based on "type compatibility" and hence a learning-based equilibrium selection.||||@arxiv||||2017/02/06||||Learning and Type Compatibility in Signaling Games||||Which equilibria will arise in signaling games depends on how the receiver interprets deviations from the path of play. We develop a micro-foundation for these off-path beliefs, and an associated...||||https://arxiv.org/abs/1702.01819v3||||econ||||
28||||None||||Clustering and External Validity in Randomized Controlled Trials with Stochastic Potential Outcomes||||arXiv.org||||2019/12/02||||Clustering and External Validity in Randomized Controlled Trials with Stochastic Potential Outcomes||||Deeb, Antoine || de Chaisemartin, Clément||||https://arxiv.org/pdf/1912.01052||||1912.01052||||In the literature studying randomized controlled trials (RCTs), it is often assumed that the potential outcomes of units participating in the experiment are deterministic. This assumption is unlikely to hold, as stochastic shocks may take place during the experiment. In this paper, we consider the case of an RCT with individual-level treatment assignment, and we allow for individual-level and cluster-level (e.g. village-level) shocks to affect the potential outcomes. We show that one can draw inference on two estimands: the ATE conditional on the realizations of the cluster-level shocks, using heteroskedasticity-robust standard errors; the ATE netted out of those shocks, using cluster-robust standard errors. By clustering, researchers can test if the treatment would still have had an effect, had the stochastic shocks that occurred during the experiment been different.||||@arxiv||||2019/12/02||||Clustering and External Validity in Randomized Controlled Trials...||||In the literature studying randomized controlled trials (RCTs), it is often assumed that the potential outcomes of units participating in the experiment are deterministic. This assumption is...||||https://arxiv.org/abs/1912.01052v1||||econ||||
29||||None||||Panel Data Quantile Regression for Treatment Effect Models||||arXiv.org||||2020/01/13||||Panel Data Quantile Regression for Treatment Effect Models||||Ishihara, Takuya||||https://arxiv.org/pdf/2001.04324||||2001.04324||||In this study, we explore the identification and estimation of the quantile treatment effects (QTE) using panel data. We generalize the change-in-changes (CIC) model proposed by Athey and Imbens (2006) and propose a tractable estimator of the QTE. The CIC model allows for the estimation of the potential outcomes distribution and captures the heterogeneous effects of the treatment on the outcomes. However, the CIC model has the following two problems: (1) there lacks a tractable estimator in the presence of covariates and (2) the CIC estimator does not work when the treatment is continuous. Our model allows for the presence of covariates and continuous treatment. We propose a two-step estimation method based on a quantile regression and minimum distance method. We then show the consistency and asymptotic normality of our estimator. Monte Carlo studies indicate that our estimator performs well in finite samples. We use our method to estimate the impact of an insurance program on quantiles of household production.||||@arxiv||||2020/01/13||||Panel Data Quantile Regression for Treatment Effect Models||||In this study, we explore the identification and estimation of the quantile treatment effects (QTE) using panel data. We generalize the change-in-changes (CIC) model proposed by Athey and Imbens...||||https://arxiv.org/abs/2001.04324v1||||econ||||
30||||None||||Dealing with Stochastic Volatility in Time Series Using the R Package stochvol||||arXiv.org||||2019/06/28||||Dealing with Stochastic Volatility in Time Series Using the R Package stochvol||||Kastner, Gregor||||https://arxiv.org/pdf/1906.12134||||1906.12134||||The R package stochvol provides a fully Bayesian implementation of heteroskedasticity modeling within the framework of stochastic volatility. It utilizes Markov chain Monte Carlo (MCMC) samplers to conduct inference by obtaining draws from the posterior distribution of parameters and latent variables which can then be used for predicting future volatilities. The package can straightforwardly be employed as a stand-alone tool; moreover, it allows for easy incorporation into other MCMC samplers. The main focus of this paper is to show the functionality of stochvol. In addition, it provides a brief mathematical description of the model, an overview of the sampling schemes used, and several illustrative examples using exchange rate data.||||@arxiv||||2019/06/28||||Dealing with Stochastic Volatility in Time Series Using the R...||||The R package stochvol provides a fully Bayesian implementation of heteroskedasticity modeling within the framework of stochastic volatility. It utilizes Markov chain Monte Carlo (MCMC) samplers...||||https://arxiv.org/abs/1906.12134v1||||econ||||
31||||None||||Does Obamacare Care? A Fuzzy Difference-in-discontinuities Approach||||arXiv.org||||2020/01/04||||Does Obamacare Care? A Fuzzy Difference-in-discontinuities Approach||||Galindo-Silva, Hector || Some, Nibene Habib || Tchuente, Guy||||https://arxiv.org/pdf/1812.06537||||1812.06537||||This paper explores the use of a fuzzy regression discontinuity design where multiple treatments are applied at the threshold. The identification results show that, under the very strong assumption that the change in the probability of treatment at the cutoff is equal across treatments, a difference-in-discontinuities estimator identifies the treatment effect of interest. The point estimates of the treatment effect using a simple fuzzy difference-in-discontinuities design are biased if the change in the probability of a treatment applying at the cutoff differs across treatments. Modifications of the fuzzy difference-in-discontinuities approach that rely on milder assumptions are also proposed. Our results suggest caution is needed when applying before-and-after methods in the presence of fuzzy discontinuities. Using data from the National Health Interview Survey, we apply this new identification strategy to evaluate the causal effect of the Affordable Care Act (ACA) on older Americans' health care access and utilization. Our results suggest that the ACA has (1) led to a 5% increase in the hospitalization rate of elderly Americans, (2) increased the probability of delaying care for cost reasons by 3.6%, and (3) exacerbated cost-related barriers to follow-up care and continuity of care: 7% more elderly individuals could not afford prescriptions, 7% more could not see a specialist, and 5.5% more could not afford a follow-up visit. Our results can be explained by an increase in the demand for health services without a corresponding adjustment in supply following the implementation of the ACA.||||@arxiv||||2018/12/16||||Does Obamacare Care? A Fuzzy Difference-in-discontinuities Approach||||This paper explores the use of a fuzzy regression discontinuity design where multiple treatments are applied at the threshold. The identification results show that, under the very strong...||||https://arxiv.org/abs/1812.06537v2||||econ||||
32||||None||||Innovation and Strategic Network Formation||||arXiv.org||||2020/02/09||||Innovation and Strategic Network Formation||||Dasaratha, Krishna||||https://arxiv.org/pdf/1911.06872||||1911.06872||||We study a model of innovation with a large number of firms that create new technologies by combining several discrete ideas. These ideas can be acquired by private investment or via social learning. Firms face a choice between secrecy, which protects existing intellectual property, and openness, which facilitates learning from others. Their decisions determine interaction rates between firms, and these interaction rates enter our model as link probabilities in a learning network. Higher interaction rates impose both positive and negative externalities on other firms, as there is more learning but also more competition. We show that the equilibrium learning network is at a critical threshold between sparse and dense networks. At equilibrium, the positive externality from interaction dominates: the innovation rate and even average firm profits would be dramatically higher if the network were denser. So there are large returns to increasing interaction rates above the critical threshold. Nevertheless, several natural types of interventions fail to move the equilibrium away from criticality. One policy solution is to introduce informational intermediaries, such as public innovators who do not have incentives to be secretive. These intermediaries can facilitate a high-innovation equilibrium by transmitting ideas from one private firm to another.||||@arxiv||||2019/11/15||||Innovation and Strategic Network Formation||||We study a model of innovation with a large number of firms that create new technologies by combining several discrete ideas. These ideas can be acquired by private investment or via social...||||https://arxiv.org/abs/1911.06872v3||||cs||||
33||||None||||Limitations of P-Values and $R^2$ for Stepwise Regression Building: A Fairness Demonstration in Health Policy Risk Adjustment||||arXiv.org||||2018/08/07||||Limitations of P-Values and $R^2$ for Stepwise Regression Building: A Fairness Demonstration in Health Policy Risk Adjustment||||Rose, Sherri || McGuire, Thomas G.||||https://arxiv.org/pdf/1803.05513||||1803.05513||||Stepwise regression building procedures are commonly used applied statistical tools, despite their well-known drawbacks. While many of their limitations have been widely discussed in the literature, other aspects of the use of individual statistical fit measures, especially in high-dimensional stepwise regression settings, have not. Giving primacy to individual fit, as is done with p-values and $R^2$, when group fit may be the larger concern, can lead to misguided decision making. One of the most consequential uses of stepwise regression is in health care, where these tools allocate hundreds of billions of dollars to health plans enrolling individuals with different predicted health care costs. The main goal of this "risk adjustment" system is to convey incentives to health plans such that they provide health care services fairly, a component of which is not to discriminate in access or care for persons or groups likely to be expensive. We address some specific limitations of p-values and $R^2$ for high-dimensional stepwise regression in this policy problem through an illustrated example by additionally considering a group-level fairness metric.||||@arxiv||||2018/03/14||||Limitations of P-Values and $R^2$ for Stepwise Regression...||||Stepwise regression building procedures are commonly used applied statistical tools, despite their well-known drawbacks. While many of their limitations have been widely discussed in the...||||https://arxiv.org/abs/1803.05513v2||||cs||||
34||||None||||Estimation and Inference for Policy Relevant Treatment Effects||||arXiv.org||||2020/02/09||||Estimation and Inference for Policy Relevant Treatment Effects||||Sasaki, Yuya || Ura, Takuya||||https://arxiv.org/pdf/1805.11503||||1805.11503||||Estimation of the policy relevant treatment effects (PRTE) involves estimation of multiple preliminary parameters, including propensity scores, conditional expectation functions of the outcome and covariates given the propensity score, and marginal treatment effects. These preliminary estimators can affect the asymptotic distribution of the PRTE estimator in complicated and intractable manners. In this light, we propose an orthogonal score for double debiased estimation of the PRTE, whereby the asymptotic distribution of the PRTE estimator is obtained without any influence of preliminary parameter estimators as far as they satisfy mild requirements of convergence rates. To our knowledge, our work is the first to develop limit distribution theories for inference about the PRTE.||||@arxiv||||2018/05/29||||Estimation and Inference for Policy Relevant Treatment Effects||||Estimation of the policy relevant treatment effects (PRTE) involves estimation of multiple preliminary parameters, including propensity scores, conditional expectation functions of the outcome and...||||https://arxiv.org/abs/1805.11503v3||||econ||||
35||||None||||Evaluating Conditional Cash Transfer Policies with Machine Learning Methods||||arXiv.org||||2018/03/16||||Evaluating Conditional Cash Transfer Policies with Machine Learning Methods||||Chen, Tzai-Shuen||||https://arxiv.org/pdf/1803.06401||||1803.06401||||This paper presents an out-of-sample prediction comparison between major machine learning models and the structural econometric model. Over the past decade, machine learning has established itself as a powerful tool in many prediction applications, but this approach is still not widely adopted in empirical economic studies. To evaluate the benefits of this approach, I use the most common machine learning algorithms, CART, C4.5, LASSO, random forest, and adaboost, to construct prediction models for a cash transfer experiment conducted by the Progresa program in Mexico, and I compare the prediction results with those of a previous structural econometric study. Two prediction tasks are performed in this paper: the out-of-sample forecast and the long-term within-sample simulation. For the out-of-sample forecast, both the mean absolute error and the root mean square error of the school attendance rates found by all machine learning models are smaller than those found by the structural model. Random forest and adaboost have the highest accuracy for the individual outcomes of all subgroups. For the long-term within-sample simulation, the structural model has better performance than do all of the machine learning models. The poor within-sample fitness of the machine learning model results from the inaccuracy of the income and pregnancy prediction models. The result shows that the machine learning model performs better than does the structural model when there are many data to learn; however, when the data are limited, the structural model offers a more sensible prediction. The findings of this paper show promise for adopting machine learning in economic policy analyses in the era of big data.||||@arxiv||||2018/03/16||||Evaluating Conditional Cash Transfer Policies with Machine Learning Methods||||This paper presents an out-of-sample prediction comparison between major machine learning models and the structural econometric model. Over the past decade, machine learning has established itself...||||https://arxiv.org/abs/1803.06401v1||||econ||||
36||||None||||Average Density Estimators: Efficiency and Bootstrap Consistency||||arXiv.org||||2019/04/19||||Average Density Estimators: Efficiency and Bootstrap Consistency||||Cattaneo, Matias D. || Jansson, Michael||||https://arxiv.org/pdf/1904.09372||||1904.09372||||This paper highlights a tension between semiparametric efficiency and bootstrap consistency in the context of a canonical semiparametric estimation problem. It is shown that although simple plug-in estimators suffer from bias problems preventing them from achieving semiparametric efficiency under minimal smoothness conditions, the nonparametric bootstrap automatically corrects for this bias and that, as a result, these seemingly inferior estimators achieve bootstrap consistency under minimal smoothness conditions. In contrast, "debiased" estimators that achieve semiparametric efficiency under minimal smoothness conditions do not achieve bootstrap consistency under those same conditions.||||@arxiv||||2019/04/19||||Average Density Estimators: Efficiency and Bootstrap Consistency||||This paper highlights a tension between semiparametric efficiency and bootstrap consistency in the context of a canonical semiparametric estimation problem. It is shown that although simple...||||https://arxiv.org/abs/1904.09372v1||||econ||||
37||||None||||Online Inference for Advertising Auctions||||arXiv.org||||2019/08/22||||Online Inference for Advertising Auctions||||Waisman, Caio || Nair, Harikesh S. || Carrion, Carlos || Xu, Nan||||https://arxiv.org/pdf/1908.08600||||1908.08600||||Advertisers that engage in real-time bidding (RTB) to display their ads commonly have two goals: learning their optimal bidding policy and estimating the expected effect of exposing users to their ads. Typical strategies to accomplish one of these goals tend to ignore the other, creating an apparent tension between the two. This paper exploits the economic structure of the bid optimization problem faced by advertisers to show that these two objectives can actually be perfectly aligned. By framing the advertiser's problem as a multi-armed bandit (MAB) problem, we propose a modified Thompson Sampling (TS) algorithm that concurrently learns the optimal bidding policy and estimates the expected effect of displaying the ad while minimizing economic losses from potential sub-optimal bidding. Simulations show that not only the proposed method successfully accomplishes the advertiser's goals, but also does so at a much lower cost than more conventional experimentation policies aimed at performing causal inference.||||@arxiv||||2019/08/22||||Online Inference for Advertising Auctions||||Advertisers that engage in real-time bidding (RTB) to display their ads commonly have two goals: learning their optimal bidding policy and estimating the expected effect of exposing users to their...||||https://arxiv.org/abs/1908.08600v1||||cs||||
38||||None||||Score Permutation Based Finite Sample Inference for Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) Models||||arXiv.org||||2018/07/23||||Score Permutation Based Finite Sample Inference for Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) Models||||Csáji, Balázs Csanád||||https://arxiv.org/pdf/1807.08390||||1807.08390||||A standard model of (conditional) heteroscedasticity, i.e., the phenomenon that the variance of a process changes over time, is the Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) model, which is especially important for economics and finance. GARCH models are typically estimated by the Quasi-Maximum Likelihood (QML) method, which works under mild statistical assumptions. Here, we suggest a finite sample approach, called ScoPe, to construct distribution-free confidence regions around the QML estimate, which have exact coverage probabilities, despite no additional assumptions about moments are made. ScoPe is inspired by the recently developed Sign-Perturbed Sums (SPS) method, which however cannot be applied in the GARCH case. ScoPe works by perturbing the score function using randomly permuted residuals. This produces alternative samples which lead to exact confidence regions. Experiments on simulated and stock market data are also presented, and ScoPe is compared with the asymptotic theory and bootstrap approaches.||||@arxiv||||2018/07/23||||Score Permutation Based Finite Sample Inference for Generalized...||||A standard model of (conditional) heteroscedasticity, i.e., the phenomenon that the variance of a process changes over time, is the Generalized AutoRegressive Conditional Heteroskedasticity...||||https://arxiv.org/abs/1807.08390v1||||cs||||
39||||None||||Constrained Information Design: Toolkit||||arXiv.org||||2018/11/08||||Constrained Information Design: Toolkit||||Doval, Laura || Skreta, Vasiliki||||https://arxiv.org/pdf/1811.03588||||1811.03588||||These notes show the tools in Le Treust and Tomala(2017) extend to the case of multiple inequality and equality constraints. This showcases the power of the results in that paper to analyze problems of information design subject to constraints. In fact, we show in Doval and Skreta (2018) that they can be used to provide an upper bound on the number of posteriors a designer with limited commitment uses in his optimal mechanism.||||@arxiv||||2018/11/08||||Constrained Information Design: Toolkit||||These notes show the tools in Le Treust and Tomala(2017) extend to the case of multiple inequality and equality constraints. This showcases the power of the results in that paper to analyze...||||https://arxiv.org/abs/1811.03588v1||||econ||||
40||||None||||Academic Engagement and Commercialization in an Institutional Transition Environment: Evidence from Shanghai Maritime University||||arXiv.org||||2019/01/23||||Academic Engagement and Commercialization in an Institutional Transition Environment: Evidence from Shanghai Maritime University||||Shi, Dongbo || Ge, Yeyanran||||https://arxiv.org/pdf/1901.07725||||1901.07725||||Does academic engagement accelerate or crowd out the commercialization of university knowledge? Research on this topic seldom considers the impact of the institutional environment, especially when a formal institution for encouraging the commercial activities of scholars has not yet been established. This study investigates this question in the context of China, which is in the institutional transition stage. Based on a survey of scholars from Shanghai Maritime University, we demonstrate that academic engagement has a positive impact on commercialization and that this impact is greater for risk-averse scholars than for other risk-seeking scholars. Our results suggest that in an institutional transition environment, the government should consider encouraging academic engagement to stimulate the commercialization activities of conservative scholars.||||@arxiv||||2019/01/23||||Academic Engagement and Commercialization in an Institutional...||||Does academic engagement accelerate or crowd out the commercialization of university knowledge? Research on this topic seldom considers the impact of the institutional environment, especially when...||||https://arxiv.org/abs/1901.07725v1||||econ||||
41||||None||||X-model: further development and possible modifications||||arXiv.org||||2019/07/22||||X-model: further development and possible modifications||||Kulakov, Sergei||||https://arxiv.org/pdf/1907.09206||||1907.09206||||Despite its critical importance, the famous X-model elaborated by Ziel and Steinert (2016) has neither bin been widely studied nor further developed. And yet, the possibilities to improve the model are as numerous as the fields it can be applied to. The present paper takes advantage of a technique proposed by Coulon et al. (2014) to enhance the X-model. Instead of using the wholesale supply and demand curves as inputs for the model, we rely on the transformed versions of these curves with a perfectly inelastic demand. As a result, computational requirements of our X-model reduce and its forecasting power increases substantially. Moreover, our X-model becomes more robust towards outliers present in the initial auction curves data.||||@arxiv||||2019/07/22||||X-model: further development and possible modifications||||Despite its critical importance, the famous X-model elaborated by Ziel and Steinert (2016) has neither bin been widely studied nor further developed. And yet, the possibilities to improve the...||||https://arxiv.org/abs/1907.09206v1||||econ||||
42||||None||||High-Dimensional Forecasting in the Presence of Unit Roots and Cointegration||||arXiv.org||||2019/11/24||||High-Dimensional Forecasting in the Presence of Unit Roots and Cointegration||||Smeekes, Stephan || Wijler, Etienne||||https://arxiv.org/pdf/1911.10552||||1911.10552||||We investigate how the possible presence of unit roots and cointegration affects forecasting with Big Data. As most macroeoconomic time series are very persistent and may contain unit roots, a proper handling of unit roots and cointegration is of paramount importance for macroeconomic forecasting. The high-dimensional nature of Big Data complicates the analysis of unit roots and cointegration in two ways. First, transformations to stationarity require performing many unit root tests, increasing room for errors in the classification. Second, modelling unit roots and cointegration directly is more difficult, as standard high-dimensional techniques such as factor models and penalized regression are not directly applicable to (co)integrated data and need to be adapted. We provide an overview of both issues and review methods proposed to address these issues. These methods are also illustrated with two empirical applications.||||@arxiv||||2019/11/24||||High-Dimensional Forecasting in the Presence of Unit Roots and...||||We investigate how the possible presence of unit roots and cointegration affects forecasting with Big Data. As most macroeoconomic time series are very persistent and may contain unit roots, a...||||https://arxiv.org/abs/1911.10552v1||||econ||||
43||||None||||The fair reward problem: the illusion of success and how to solve it||||arXiv.org||||2019/04/18||||The fair reward problem: the illusion of success and how to solve it||||Sornette, Didier || Wheatley, Spencer || Cauwels, Peter||||https://arxiv.org/pdf/1902.04940||||1902.04940||||Humanity has been fascinated by the pursuit of fortune since time immemorial, and many successful outcomes benefit from strokes of luck. But success is subject to complexity, uncertainty, and change - and at times becoming increasingly unequally distributed. This leads to tension and confusion over to what extent people actually get what they deserve (i.e., fairness/meritocracy). Moreover, in many fields, humans are over-confident and pervasively confuse luck for skill (I win, it's skill; I lose, it's bad luck). In some fields, there is too much risk taking; in others, not enough. Where success derives in large part from luck - and especially where bailouts skew the incentives (heads, I win; tails, you lose) - it follows that luck is rewarded too much. This incentivizes a culture of gambling, while downplaying the importance of productive effort. And, short term success is often rewarded, irrespective, and potentially at the detriment, of the long-term system fitness. However, much success is truly meritocratic, and the problem is to discern and reward based on merit. We call this the fair reward problem. To address this, we propose three different measures to assess merit: (i) raw outcome; (ii) risk adjusted outcome, and (iii) prospective. We emphasize the need, in many cases, for the deductive prospective approach, which considers the potential of a system to adapt and mutate in novel futures. This is formalized within an evolutionary system, comprised of five processes, inter alia handling the exploration-exploitation trade-off. Several human endeavors - including finance, politics, and science -are analyzed through these lenses, and concrete solutions are proposed to support a prosperous and meritocratic society.||||@arxiv||||2019/02/03||||The fair reward problem: the illusion of success and how to solve it||||Humanity has been fascinated by the pursuit of fortune since time immemorial, and many successful outcomes benefit from strokes of luck. But success is subject to complexity, uncertainty, and...||||https://arxiv.org/abs/1902.04940v2||||econ||||
44||||None||||Estimation of Conditional Average Treatment Effects with High-Dimensional Data||||arXiv.org||||2019/10/19||||Estimation of Conditional Average Treatment Effects with High-Dimensional Data||||Fan, Qingliang || Hsu, Yu-Chin || Lieli, Robert P. || Zhang, Yichong||||https://arxiv.org/pdf/1908.02399||||1908.02399||||Given the unconfoundedness assumption, we propose new nonparametric estimators for the reduced dimensional conditional average treatment effect (CATE) function. In the first stage, the nuisance functions necessary for identifying CATE are estimated by machine learning methods, allowing the number of covariates to be comparable to or larger than the sample size. Conditioning on a large number of variables (including their possible transformations) generally enhances the plausibility of the unconfoundedness assumption. The second stage consists of a low-dimensional kernel regression, reducing CATE to a function of the covariate(s) of interest. We consider two variants of the estimator depending on whether the nuisance functions are estimated over the full sample or over a hold-out sample. Building on Belloni at al. (2017) and Chernozhukov et al. (2018), we derive functional limit theory for the estimators and provide an easy-to-implement procedure for uniform inference based on the multiplier bootstrap. The empirical application revisits the effect of maternal smoking on a baby's birth weight as a function of the mother's age.||||@arxiv||||2019/08/06||||Estimation of Conditional Average Treatment Effects with...||||Given the unconfoundedness assumption, we propose new nonparametric estimators for the reduced dimensional conditional average treatment effect (CATE) function. In the first stage, the nuisance...||||https://arxiv.org/abs/1908.02399v3||||econ||||
45||||None||||Panel Data Analysis with Heterogeneous Dynamics||||arXiv.org||||2019/01/15||||Panel Data Analysis with Heterogeneous Dynamics||||Okui, Ryo || Yanagi, Takahide||||https://arxiv.org/pdf/1803.09452||||1803.09452||||This paper proposes a model-free approach to analyze panel data with heterogeneous dynamic structures across observational units. We first compute the sample mean, autocovariances, and autocorrelations for each unit, and then estimate the parameters of interest based on their empirical distributions. We then investigate the asymptotic properties of our estimators using double asymptotics and propose split-panel jackknife bias correction and inference based on the cross-sectional bootstrap. We illustrate the usefulness of our procedures by studying the deviation dynamics of the law of one price. Monte Carlo simulations confirm that the proposed bias correction is effective and yields valid inference in small samples.||||@arxiv||||2018/03/26||||Panel Data Analysis with Heterogeneous Dynamics||||This paper proposes a model-free approach to analyze panel data with heterogeneous dynamic structures across observational units. We first compute the sample mean, autocovariances, and...||||https://arxiv.org/abs/1803.09452v2||||econ||||
46||||None||||Regression Discontinuity Designs Using Covariates||||arXiv.org||||2018/09/11||||Regression Discontinuity Designs Using Covariates||||Calonico, Sebastian || Cattaneo, Matias D. || Farrell, Max H. || Titiunik, Rocio||||https://arxiv.org/pdf/1809.03904||||1809.03904||||We study regression discontinuity designs when covariates are included in the estimation. We examine local polynomial estimators that include discrete or continuous covariates in an additive separable way, but without imposing any parametric restrictions on the underlying population regression functions. We recommend a covariate-adjustment approach that retains consistency under intuitive conditions, and characterize the potential for estimation and inference improvements. We also present new covariate-adjusted mean squared error expansions and robust bias-corrected inference procedures, with heteroskedasticity-consistent and cluster-robust standard errors. An empirical illustration and an extensive simulation study is presented. All methods are implemented in \texttt{R} and \texttt{Stata} software packages.||||@arxiv||||2018/09/11||||Regression Discontinuity Designs Using Covariates||||We study regression discontinuity designs when covariates are included in the estimation. We examine local polynomial estimators that include discrete or continuous covariates in an additive...||||https://arxiv.org/abs/1809.03904v1||||econ||||
47||||None||||Calibrated Projection in MATLAB: Users' Manual||||arXiv.org||||2017/10/24||||Calibrated Projection in MATLAB: Users' Manual||||Kaido, Hiroaki || Molinari, Francesca || Stoye, Jörg || Thirkettle, Matthew||||https://arxiv.org/pdf/1710.09707||||1710.09707||||We present the calibrated-projection MATLAB package implementing the method to construct confidence intervals proposed by Kaido, Molinari and Stoye (2017). This manual provides details on how to use the package for inference on projections of partially identified parameters. It also explains how to use the MATLAB functions we developed to compute confidence intervals on solutions of nonlinear optimization problems with estimated constraints.||||@arxiv||||2017/10/24||||Calibrated Projection in MATLAB: Users' Manual||||We present the calibrated-projection MATLAB package implementing the method to construct confidence intervals proposed by Kaido, Molinari and Stoye (2017). This manual provides details on how to...||||https://arxiv.org/abs/1710.09707v1||||econ||||
48||||None||||The role of pawnshops in risk coping in early twentieth-century Japan||||arXiv.org||||2019/08/25||||The role of pawnshops in risk coping in early twentieth-century Japan||||Inoue, Tatsuki||||https://arxiv.org/pdf/1905.04419||||1905.04419||||This study examines the role of pawnshops as a risk-coping device in prewar Japan. Using data on pawnshop loans for more than 250 municipalities and exploiting the 1918-1920 influenza pandemic as a natural experiment, we find that the adverse health shock increased the total amount of loans from pawnshops. This is because those who regularly relied on pawnshops borrowed more money from them than usual to cope with the adverse health shock, and not because the number of people who used pawnshops increased.||||@arxiv||||2019/05/11||||The role of pawnshops in risk coping in early twentieth-century Japan||||This study examines the role of pawnshops as a risk-coping device in prewar Japan. Using data on pawnshop loans for more than 250 municipalities and exploiting the 1918-1920 influenza pandemic as...||||https://arxiv.org/abs/1905.04419v2||||econ||||
49||||None||||Revisiting the thermal and superthermal two-class distribution of incomes: A critical perspective||||arXiv.org||||2018/04/17||||Revisiting the thermal and superthermal two-class distribution of incomes: A critical perspective||||Schneider, Markus P. A.||||https://arxiv.org/pdf/1804.06341||||1804.06341||||This paper offers a two-pronged critique of the empirical investigation of the income distribution performed by physicists over the past decade. Their finding rely on the graphical analysis of the observed distribution of normalized incomes. Two central observations lead to the conclusion that the majority of incomes are exponentially distributed, but neither each individual piece of evidence nor their concurrent observation robustly proves that the thermal and superthermal mixture fits the observed distribution of incomes better than reasonable alternatives. A formal analysis using popular measures of fit shows that while an exponential distribution with a power-law tail provides a better fit of the IRS income data than the log-normal distribution (often assumed by economists), the thermal and superthermal mixture's fit can be improved upon further by adding a log-normal component. The economic implications of the thermal and superthermal distribution of incomes, and the expanded mixture are explored in the paper.||||@arxiv||||2018/04/17||||Revisiting the thermal and superthermal two-class distribution of...||||This paper offers a two-pronged critique of the empirical investigation of the income distribution performed by physicists over the past decade. Their finding rely on the graphical analysis of the...||||https://arxiv.org/abs/1804.06341v1||||econ||||
50||||None||||Communication, Distortion, and Randomness in Metric Voting||||arXiv.org||||2019/11/20||||Communication, Distortion, and Randomness in Metric Voting||||Kempe, David||||https://arxiv.org/pdf/1911.08129||||1911.08129||||In distortion-based analysis of social choice rules over metric spaces, one assumes that all voters and candidates are jointly embedded in a common metric space. Voters rank candidates by non-decreasing distance. The mechanism, receiving only this ordinal (comparison) information, should select a candidate approximately minimizing the sum of distances from all voters. It is known that while the Copeland rule and related rules guarantee distortion at most 5, many other standard voting rules, such as Plurality, Veto, or $k$-approval, have distortion growing unboundedly in the number $n$ of candidates.   Plurality, Veto, or $k$-approval with small $k$ require less communication from the voters than all deterministic social choice rules known to achieve constant distortion. This motivates our study of the tradeoff between the distortion and the amount of communication in deterministic social choice rules.   We show that any one-round deterministic voting mechanism in which each voter communicates only the candidates she ranks in a given set of $k$ positions must have distortion at least $\frac{2n-k}{k}$; we give a mechanism achieving an upper bound of $O(n/k)$, which matches the lower bound up to a constant. For more general communication-bounded voting mechanisms, in which each voter communicates $b$ bits of information about her ranking, we show a slightly weaker lower bound of $Ω(n/b)$ on the distortion.   For randomized mechanisms, it is known that Random Dictatorship achieves expected distortion strictly smaller than 3, almost matching a lower bound of $3-\frac{2}{n}$ for any randomized mechanism that only receives each voter's top choice. We close this gap, by giving a simple randomized social choice rule which only uses each voter's first choice, and achieves expected distortion $3-\frac{2}{n}$.||||@arxiv||||2019/11/19||||Communication, Distortion, and Randomness in Metric Voting||||In distortion-based analysis of social choice rules over metric spaces, one assumes that all voters and candidates are jointly embedded in a common metric space. Voters rank candidates by...||||https://arxiv.org/abs/1911.08129v2||||cs||||
51||||None||||Review of the Plan for Integrating Big Data Analytics Program for the Electronic Marketing System and Customer Relationship Management: A Case Study XYZ Institution||||arXiv.org||||2019/08/07||||Review of the Plan for Integrating Big Data Analytics Program for the Electronic Marketing System and Customer Relationship Management: A Case Study XYZ Institution||||Sudianto, Idha||||https://arxiv.org/pdf/1908.02430||||1908.02430||||This research aims to explore business processes and what the factors have major influence on electronic marketing and CRM systems? Which data needs to be analyzed and integrated in the system, and how to do that? How effective of integration the electronic marketing and CRM with big data enabled to support Marketing and Customer Relation operations. Research based on case studies at XYZ Organization: International Language Education Service in Surabaya. Research is studying secondary data which is supported by qualitative research methods. Using purposive sampling technique with observation and interviewing several respondents who need the system integration. The documentation of interview is coded to keep confidentiality of the informant. Method of extending participation, triangulation of data sources, discussions and the adequacy of the theory are uses to validate data. Miles and Huberman models is uses to do analysis the data interview. Results of the research are expected to become a holistic approach to fully integrate the Big Data Analytics program with electronic marketing and CRM systems.||||@arxiv||||2019/08/07||||Review of the Plan for Integrating Big Data Analytics Program for...||||This research aims to explore business processes and what the factors have major influence on electronic marketing and CRM systems? Which data needs to be analyzed and integrated in the system,...||||https://arxiv.org/abs/1908.02430v1||||econ||||
52||||None||||An Note on Why Geographically Weighted Regression Overcomes Multidimensional-Kernel-Based Varying-Coefficient Model||||arXiv.org||||2018/04/12||||An Note on Why Geographically Weighted Regression Overcomes Multidimensional-Kernel-Based Varying-Coefficient Model||||Yuan, Zihao||||https://arxiv.org/pdf/1803.01402||||1803.01402||||It is widely known that geographically weighted regression(GWR) is essentially same as varying-coefficient model. In the former research about varying-coefficient model, scholars tend to use multidimensional-kernel-based locally weighted estimation(MLWE) so that information of both distance and direction is considered. However, when we construct the local weight matrix of geographically weighted estimation, distance among the locations in the neighbor is the only factor controlling the value of entries of weight matrix. In other word, estimation of GWR is distance-kernel-based. Thus, in this paper, under stationary and limited dependent data with multidimensional subscripts, we analyze the local mean squared properties of without any assumption of the form of coefficient functions and compare it with MLWE. According to the theoretical and simulation results, geographically-weighted locally linear estimation(GWLE) is asymptotically more efficient than MLWE. Furthermore, a relationship between optimal bandwith selection and design of scale parameters is also obtained.||||@arxiv||||2018/03/04||||An Note on Why Geographically Weighted Regression Overcomes...||||It is widely known that geographically weighted regression(GWR) is essentially same as varying-coefficient model. In the former research about varying-coefficient model, scholars tend to use...||||https://arxiv.org/abs/1803.01402v2||||econ||||
53||||None||||Market Implementation of Multiple-Arrival Multiple-Deadline Differentiated Energy Services||||arXiv.org||||2019/06/07||||Market Implementation of Multiple-Arrival Multiple-Deadline Differentiated Energy Services||||Mo, Yanfang || Chen, Wei || Qiu, Li || Varaiya, Pravin||||https://arxiv.org/pdf/1906.02904||||1906.02904||||An increasing concern in power systems is on how to elicit flexibilities in demands for better supply/demand balance. To this end, several differentiated energy services have been put forward, wherein demands are discriminated by their different flexibility levels. Motivated by the duration-differentiated energy services, we have proposed energy services differentiated by durations, arrival times, and deadlines. The purpose of this paper is to study the market implementation of such multiple-arrival multiple-deadline differentiated energy services. To verify the economic feasibility, we establish that in a forward market, there exists an efficient competitive equilibrium which attains the maximum social welfare. In addition, we show that future information will influence current decisions on power delivery by studying a special kind of causal allocation policy. Finally, we propose two tractable integer programs, namely the optimal arbitrage and the minimum-cost allocation problems, which can be embedded in a two-level hierarchical real-time implementation of differentiated energy services.||||@arxiv||||2019/06/07||||Market Implementation of Multiple-Arrival Multiple-Deadline...||||An increasing concern in power systems is on how to elicit flexibilities in demands for better supply/demand balance. To this end, several differentiated energy services have been put forward,...||||https://arxiv.org/abs/1906.02904v1||||cs||||
54||||None||||How Smart Are `Water Smart Landscapes'?||||arXiv.org||||2018/03/13||||How Smart Are `Water Smart Landscapes'?||||Brelsford, Christa || Abbott, Joshua K.||||https://arxiv.org/pdf/1803.04593||||1803.04593||||Understanding the effectiveness of alternative approaches to water conservation is crucially important for ensuring the security and reliability of water services for urban residents. We analyze data from one of the longest-running "cash for grass" policies - the Southern Nevada Water Authority's Water Smart Landscapes program, where homeowners are paid to replace grass with xeric landscaping. We use a twelve year long panel dataset of monthly water consumption records for 300,000 households in Las Vegas, Nevada. Utilizing a panel difference-in-differences approach, we estimate the average water savings per square meter of turf removed. We find that participation in this program reduced the average treated household's consumption by 18 percent. We find no evidence that water savings degrade as the landscape ages, or that water savings per unit area are influenced by the value of the rebate. Depending on the assumed time horizon of benefits from turf removal, we find that the WSL program cost the water authority about $1.62 per thousand gallons of water saved, which compares favorably to alternative means of water conservation or supply augmentation.||||@arxiv||||2018/03/13||||How Smart Are `Water Smart Landscapes'?||||Understanding the effectiveness of alternative approaches to water conservation is crucially important for ensuring the security and reliability of water services for urban residents. We analyze...||||https://arxiv.org/abs/1803.04593v1||||econ||||
55||||None||||Synthetic Controls and Weighted Event Studies with Staggered Adoption||||arXiv.org||||2019/12/06||||Synthetic Controls and Weighted Event Studies with Staggered Adoption||||Ben-Michael, Eli || Feller, Avi || Rothstein, Jesse||||https://arxiv.org/pdf/1912.03290||||1912.03290||||Staggered adoption of policies by different units at different times creates promising opportunities for observational causal inference. The synthetic control method (SCM) is a recent addition to the evaluation toolkit but is designed to study a single treated unit and does not easily accommodate staggered adoption. In this paper, we generalize SCM to the staggered adoption setting. Current practice involves fitting SCM separately for each treated unit and then averaging. We show that the average of separate SCM fits does not necessarily achieve good balance for the average of the treated units, leading to possible bias in the estimated effect. We propose "partially pooled" SCM weights that instead minimize both average and state-specific imbalance, and show that the resulting estimator controls bias under a linear factor model. We also combine our partially pooled SCM weights with traditional fixed effects methods to obtain an augmented estimator that improves over both SCM weighting and fixed effects estimation alone. We assess the performance of the proposed method via extensive simulations and apply our results to the question of whether teacher collective bargaining leads to higher school spending, finding minimal impacts. We implement the proposed method in the augsynth R package.||||@arxiv||||2019/12/06||||Synthetic Controls and Weighted Event Studies with Staggered Adoption||||Staggered adoption of policies by different units at different times creates promising opportunities for observational causal inference. The synthetic control method (SCM) is a recent addition to...||||https://arxiv.org/abs/1912.03290v1||||econ||||
56||||None||||Verifying the existence of maximum likelihood estimates for generalized linear models||||arXiv.org||||2019/08/01||||Verifying the existence of maximum likelihood estimates for generalized linear models||||Correia, Sergio || Guimarães, Paulo || Zylkin, Thomas||||https://arxiv.org/pdf/1903.01633||||1903.01633||||A fundamental problem with nonlinear estimation models is that estimates are not guaranteed to exist. However, while non-existence is a well-studied issue for binary choice models, it presents significant challenges for other models as well and is not as well understood in more general settings. These challenges are only magnified for models that feature many fixed effects and other high-dimensional parameters. We address the current ambiguity surrounding this topic by studying the conditions that govern the existence of estimates for a wide class of generalized linear models (GLMs). We show that some, but not all, GLMs can still deliver consistent estimates of at least some of the linear parameters when these conditions fail to hold. We also demonstrate how to verify these conditions in the presence of high-dimensional fixed effects, as are often recommended in the international trade literature and in other common panel settings||||@arxiv||||2019/03/05||||Verifying the existence of maximum likelihood estimates for...||||A fundamental problem with nonlinear estimation models is that estimates are not guaranteed to exist. However, while non-existence is a well-studied issue for binary choice models, it presents...||||https://arxiv.org/abs/1903.01633v5||||econ||||
57||||None||||Macroeconomic Instability And Fiscal Decentralization: An Empirical Analysis||||arXiv.org||||2020/01/07||||Macroeconomic Instability And Fiscal Decentralization: An Empirical Analysis||||Jalil, Ahmad Zafarullah Abdul || Harun, Mukaramah || Mat, Siti Hadijah Che||||https://arxiv.org/pdf/2001.03486||||2001.03486||||The main objective of this paper is to fill a critical gap in the literature by analyzing the effects of decentralization on the macroeconomic stability. A survey of the voluminous literature on decentralization suggests that the question of the links between decentralization and macroeconomic stability has been relatively scantily analyzed. Even though there is still a lot of room for analysis as far as the effects of decentralization on other aspects of the economy are concerned, we believe that it is in this area that a more thorough analyses are mostly called for. Through this paper, we will try to shed more light on the issue notably by looking at other dimension of macroeconomic stability than the ones usually employed in previous studies as well as by examining other factors that might accentuate or diminish the effects of decentralization on macroeconomic stability. Our results found that decentralization appears to lead to a decrease in inflation rate. However, we do not find any correlation between decentralization with the level of fiscal deficit. Our results also show that the impact of decentralization on inflation is conditional on the level of perceived corruption and political institutions.||||@arxiv||||2020/01/07||||Macroeconomic Instability And Fiscal Decentralization: An...||||The main objective of this paper is to fill a critical gap in the literature by analyzing the effects of decentralization on the macroeconomic stability. A survey of the voluminous literature on...||||https://arxiv.org/abs/2001.03486v1||||econ||||
58||||None||||Identifying the Discount Factor in Dynamic Discrete Choice Models||||arXiv.org||||2019/09/16||||Identifying the Discount Factor in Dynamic Discrete Choice Models||||Abbring, Jaap H. || Daljord, Øystein||||https://arxiv.org/pdf/1808.10651||||1808.10651||||Empirical research often cites observed choice responses to variation that shifts expected discounted future utilities, but not current utilities, as an intuitive source of information on time preferences. We study the identification of dynamic discrete choice models under such economically motivated exclusion restrictions on primitive utilities. We show that each exclusion restriction leads to an easily interpretable moment condition with the discount factor as the only unknown parameter. The identified set of discount factors that solves this condition is finite, but not necessarily a singleton. Consequently, in contrast to common intuition, an exclusion restriction does not in general give point identification. Finally, we show that exclusion restrictions have nontrivial empirical content: The implied moment conditions impose restrictions on choices that are absent from the unconstrained model.||||@arxiv||||2018/08/31||||Identifying the Discount Factor in Dynamic Discrete Choice Models||||Empirical research often cites observed choice responses to variation that shifts expected discounted future utilities, but not current utilities, as an intuitive source of information on time...||||https://arxiv.org/abs/1808.10651v4||||econ||||
59||||None||||The effects of institutional quality on formal and informal borrowing across high-, middle-, and low-income countries||||arXiv.org||||2019/03/19||||The effects of institutional quality on formal and informal borrowing across high-, middle-, and low-income countries||||Khanh, Lan Chu||||https://arxiv.org/pdf/1903.07866||||1903.07866||||This paper examines the effects of institutional quality on financing choice of individual using a large dataset of 137,160 people from 131 countries. We classify borrowing activities into three categories, including formal, constructive informal, and underground borrowing. Although the result shows that better institutions aids the uses of formal borrowing, the impact of institutions on constructive informal and underground borrowing among three country sub-groups differs. Higher institutional quality improves constructive informal borrowing in middle-income countries but reduces the use of underground borrowing in high- and low-income countries.||||@arxiv||||2019/03/19||||The effects of institutional quality on formal and informal...||||This paper examines the effects of institutional quality on financing choice of individual using a large dataset of 137,160 people from 131 countries. We classify borrowing activities into three...||||https://arxiv.org/abs/1903.07866v1||||econ||||
60||||None||||Fast Mesh Refinement in Pseudospectral Optimal Control||||arXiv.org||||2019/04/29||||Fast Mesh Refinement in Pseudospectral Optimal Control||||Koeppen, N. || Ross, I. M. || Wilcox, L. C. || Proulx, R. J.||||https://arxiv.org/pdf/1904.12992||||1904.12992||||Mesh refinement in pseudospectral (PS) optimal control is embarrassingly easy --- simply increase the order $N$ of the Lagrange interpolating polynomial and the mathematics of convergence automates the distribution of the grid points. Unfortunately, as $N$ increases, the condition number of the resulting linear algebra increases as $N^2$; hence, spectral efficiency and accuracy are lost in practice. In this paper, we advance Birkhoff interpolation concepts over an arbitrary grid to generate well-conditioned PS optimal control discretizations. We show that the condition number increases only as $\sqrt{N}$ in general, but is independent of $N$ for the special case of one of the boundary points being fixed. Hence, spectral accuracy and efficiency are maintained as $N$ increases. The effectiveness of the resulting fast mesh refinement strategy is demonstrated by using \underline{polynomials of over a thousandth order} to solve a low-thrust, long-duration orbit transfer problem.||||@arxiv||||2019/04/29||||Fast Mesh Refinement in Pseudospectral Optimal Control||||Mesh refinement in pseudospectral (PS) optimal control is embarrassingly easy --- simply increase the order $N$ of the Lagrange interpolating polynomial and the mathematics of convergence...||||https://arxiv.org/abs/1904.12992v1||||cs||||
61||||None||||A Note on Equilibrium Uniqueness in Cournot Competition with Demand Uncertainty||||arXiv.org||||2019/06/09||||A Note on Equilibrium Uniqueness in Cournot Competition with Demand Uncertainty||||Leonardos, Stefanos || Melolidakis, Costis||||https://arxiv.org/pdf/1906.03558||||1906.03558||||We revisit the linear Cournot model with uncertain demand that is studied in Lagerlöf (2006)* and provide sufficient conditions for the uniqueness of an equilibrium that complement the existing results. We show that if the distribution of the demand intercept has the decreasing mean residual demand (DMRD) or the increasing generalized failure rate (IGFR) property, then uniqueness of equilibrium is guaranteed. The DMRD condition implies log-concavity of the expected profits per unit of output without additional assumptions on the existence or the shape of the density of the demand intercept and hence, answers in the affirmative the conjecture of Lagerlöf (2006) that such conditions may not be necessary.   *Johan Lagerlöf, Equilibrium uniqueness in a Cournot model with demand uncertainty. The B.E. Journal in Theoretical Economics, Vol. 6: Iss 1. (Topics), Article 19:1--6, 2006.||||@arxiv||||2019/06/09||||A Note on Equilibrium Uniqueness in Cournot Competition with...||||We revisit the linear Cournot model with uncertain demand that is studied in Lagerlöf (2006)* and provide sufficient conditions for the uniqueness of an equilibrium that complement the existing...||||https://arxiv.org/abs/1906.03558v1||||econ||||
62||||None||||Critical review of models, containing cultural levels beyond the organizational one||||arXiv.org||||2018/10/08||||Critical review of models, containing cultural levels beyond the organizational one||||Dimitrov, Kiril||||https://arxiv.org/pdf/1810.03605||||1810.03605||||The current article traces back the scientific interest to cultural levels across the organization at the University of National and World Economy, and especially in the series of Economic Alternatives - an official scientific magazine, issued by this Institution. Further, a wider and critical review of international achievements in this field is performed, revealing diverse analysis perspectives with respect to cultural levels. Also, a useful model of exploring and teaching the cultural levels beyond the organization is proposed.   Keywords: globalization, national culture, organization culture, cultural levels, cultural economics. JEL: M14, Z10.||||@arxiv||||2018/10/08||||Critical review of models, containing cultural levels beyond the...||||The current article traces back the scientific interest to cultural levels across the organization at the University of National and World Economy, and especially in the series of Economic...||||https://arxiv.org/abs/1810.03605v1||||econ||||
63||||None||||Efficient representation of supply and demand curves on day-ahead electricity markets||||arXiv.org||||2020/02/02||||Efficient representation of supply and demand curves on day-ahead electricity markets||||Soloviova, Mariia || Vargiolu, Tiziano||||https://arxiv.org/pdf/2002.00507||||2002.00507||||Our paper aims to model supply and demand curves of electricity day-ahead auction in a parsimonious way. Our main task is to build an appropriate algorithm to present the information about electricity prices and demands with far less parameters than the original one. We represent each curve using mesh-free interpolation techniques based on radial basis function approximation. We describe results of this method for the day-ahead IPEX spot price of Italy.||||@arxiv||||2020/02/02||||Efficient representation of supply and demand curves on day-ahead...||||Our paper aims to model supply and demand curves of electricity day-ahead auction in a parsimonious way. Our main task is to build an appropriate algorithm to present the information about...||||https://arxiv.org/abs/2002.00507v1||||econ||||
64||||None||||Placement of EV Charging Stations --- Balancing Benefits among Multiple Entities||||arXiv.org||||2018/01/07||||Placement of EV Charging Stations --- Balancing Benefits among Multiple Entities||||Luo, Chao || Huang, Yih-Fang || Gupta, Vijay||||https://arxiv.org/pdf/1801.02129||||1801.02129||||This paper studies the problem of multi-stage placement of electric vehicle (EV) charging stations with incremental EV penetration rates. A nested logit model is employed to analyze the charging preference of the individual consumer (EV owner), and predict the aggregated charging demand at the charging stations. The EV charging industry is modeled as an oligopoly where the entire market is dominated by a few charging service providers (oligopolists). At the beginning of each planning stage, an optimal placement policy for each service provider is obtained through analyzing strategic interactions in a Bayesian game. To derive the optimal placement policy, we consider both the transportation network graph and the electric power network graph. A simulation software --- The EV Virtual City 1.0 --- is developed using Java to investigate the interactions among the consumers (EV owner), the transportation network graph, the electric power network graph, and the charging stations. Through a series of experiments using the geographic and demographic data from the city of San Pedro District of Los Angeles, we show that the charging station placement is highly consistent with the heatmap of the traffic flow. In addition, we observe a spatial economic phenomenon that service providers prefer clustering instead of separation in the EV charging market.||||@arxiv||||2018/01/07||||Placement of EV Charging Stations --- Balancing Benefits among...||||This paper studies the problem of multi-stage placement of electric vehicle (EV) charging stations with incremental EV penetration rates. A nested logit model is employed to analyze the charging...||||https://arxiv.org/abs/1801.02129v1||||cs||||
65||||None||||A Principal-Agent approach to study Capacity Remuneration Mechanisms||||arXiv.org||||2019/11/28||||A Principal-Agent approach to study Capacity Remuneration Mechanisms||||Alasseur, Clémence || Farhat, Heythem || Saguan, Marcelo||||https://arxiv.org/pdf/1911.12623||||1911.12623||||We propose to study electricity capacity remuneration mechanism design through a Principal-Agent approach. The Principal represents the aggregation of electricity consumers (or a representative entity), subject to the physical risk of shortage, and the Agent represents the electricity capacity owners, who invest in capacity and produce electricity to satisfy consumers' demand, and are subject to financial risks. Following the methodology of Cvitanic et al. (2017), we propose an optimal contract, from consumers' perspective, which complements the revenue capacity owners achieved from the spot energy market, and incentivizes both parties to perform an optimal level of investments while sharing the physical and financial risks. Numerical results provide insights on the necessity of a capacity remuneration mechanism and also show how this is especially true when the level of uncertainties on demand or production side increases.||||@arxiv||||2019/11/28||||A Principal-Agent approach to study Capacity Remuneration Mechanisms||||We propose to study electricity capacity remuneration mechanism design through a Principal-Agent approach. The Principal represents the aggregation of electricity consumers (or a representative...||||https://arxiv.org/abs/1911.12623v1||||econ||||
66||||None||||The Implications of Pricing on Social Learning||||arXiv.org||||2019/05/09||||The Implications of Pricing on Social Learning||||Arieli, Itai || Koren, Moran || Smorodinsky, Rann||||https://arxiv.org/pdf/1905.03452||||1905.03452||||We study the implications of endogenous pricing for learning and welfare in the classic herding model . When prices are determined exogenously, it is known that learning occurs if and only if signals are unbounded. By contrast, we show that learning can occur when signals are bounded as long as non-conformism among consumers is scarce. More formally, learning happens if and only if signals exhibit the vanishing likelihood property introduced bellow. We discuss the implications of our results for potential market failure in the context of Schumpeterian growth with uncertainty over the value of innovations.||||@arxiv||||2019/05/09||||The Implications of Pricing on Social Learning||||We study the implications of endogenous pricing for learning and welfare in the classic herding model . When prices are determined exogenously, it is known that learning occurs if and only if...||||https://arxiv.org/abs/1905.03452v1||||cs||||
67||||None||||Katugampola Generalized Conformal Derivative Approach to Inada Conditions and Solow-Swan Economic Growth Model||||arXiv.org||||2019/06/29||||Katugampola Generalized Conformal Derivative Approach to Inada Conditions and Solow-Swan Economic Growth Model||||Fernández-Anaya, G. || Quezada-Téllez, L. A. || Nuñez-Zavala, B. || Brun-Battistini, D.||||https://arxiv.org/pdf/1907.00130||||1907.00130||||This article shows a new focus of mathematic analysis for the Solow-Swan economic growth model, using the generalized conformal derivative Katugampola (KGCD). For this, under the same Solow-Swan model assumptions, the Inada conditions are extended, which, for the new model shown here, depending on the order of the KGCD. This order plays an important role in the speed of convergence of the closed solutions obtained with this derivative for capital (k) and for per-capita production (y) in the cases without migration and with negative migration. Our approach to the model with the KGCD adds a new parameter to the Solow-Swan model, the order of the KGCD and not a new state variable. In addition, we propose several possible economic interpretations for that parameter.||||@arxiv||||2019/06/29||||Katugampola Generalized Conformal Derivative Approach to Inada...||||This article shows a new focus of mathematic analysis for the Solow-Swan economic growth model, using the generalized conformal derivative Katugampola (KGCD). For this, under the same Solow-Swan...||||https://arxiv.org/abs/1907.00130v1||||econ||||
68||||None||||Inducing Sparsity and Shrinkage in Time-Varying Parameter Models||||arXiv.org||||2019/12/16||||Inducing Sparsity and Shrinkage in Time-Varying Parameter Models||||Huber, Florian || Koop, Gary || Onorante, Luca||||https://arxiv.org/pdf/1905.10787||||1905.10787||||Time-varying parameter (TVP) models have the potential to be over-parameterized, particularly when the number of variables in the model is large. Global-local priors are increasingly used to induce shrinkage in such models. But the estimates produced by these priors can still have appreciable uncertainty. Sparsification has the potential to reduce this uncertainty and improve forecasts. In this paper, we develop computationally simple methods which both shrink and sparsify TVP models. In a simulated data exercise we show the benefits of our shrink-then-sparsify approach in a variety of sparse and dense TVP regressions. In a macroeconomic forecasting exercise, we find our approach to substantially improve forecast performance relative to shrinkage alone.||||@arxiv||||2019/05/26||||Inducing Sparsity and Shrinkage in Time-Varying Parameter Models||||Time-varying parameter (TVP) models have the potential to be over-parameterized, particularly when the number of variables in the model is large. Global-local priors are increasingly used to...||||https://arxiv.org/abs/1905.10787v2||||econ||||
69||||None||||Some Nontrivial Properties of a Formula for Compound Interest||||arXiv.org||||2018/09/27||||Some Nontrivial Properties of a Formula for Compound Interest||||Sonin, Isaac M. || Whitmeyer, Mark||||https://arxiv.org/pdf/1809.10566||||1809.10566||||We analyze the classical model of compound interest with a constant per-period payment and interest rate. We examine the outstanding balance function as well as the periodic payment function and show that the outstanding balance function is not generally concave in the interest rate, but instead may be initially convex on its domain and then concave.||||@arxiv||||2018/09/27||||Some Nontrivial Properties of a Formula for Compound Interest||||We analyze the classical model of compound interest with a constant per-period payment and interest rate. We examine the outstanding balance function as well as the periodic payment function and...||||https://arxiv.org/abs/1809.10566v1||||econ||||
70||||None||||Unforeseen Evidence||||arXiv.org||||2019/07/17||||Unforeseen Evidence||||Piermont, Evan||||https://arxiv.org/pdf/1907.07019||||1907.07019||||I propose a normative updating rule, extended Bayesianism, for the incorporation of probabilistic information arising from the process of becoming more aware. Extended Bayesianism generalizes standard Bayesian updating to allow the posterior to reside on richer probability space than the prior. I then provide an observable criterion on prior and posterior beliefs such that they were consistent with extended Bayesianism.||||@arxiv||||2019/07/16||||Unforeseen Evidence||||I propose a normative updating rule, extended Bayesianism, for the incorporation of probabilistic information arising from the process of becoming more aware. Extended Bayesianism generalizes...||||https://arxiv.org/abs/1907.07019v2||||cs||||
71||||None||||Exogenous Rewards for Promoting Cooperation in Scale-Free Networks||||arXiv.org||||2019/05/17||||Exogenous Rewards for Promoting Cooperation in Scale-Free Networks||||Cimpeanu, Theodor || Han, The Anh || Santos, Francisco C.||||https://arxiv.org/pdf/1905.04964||||1905.04964||||The design of mechanisms that encourage pro-social behaviours in populations of self-regarding agents is recognised as a major theoretical challenge within several areas of social, life and engineering sciences. When interference from external parties is considered, several heuristics have been identified as capable of engineering a desired collective behaviour at a minimal cost. However, these studies neglect the diverse nature of contexts and social structures that characterise real-world populations. Here we analyse the impact of diversity by means of scale-free interaction networks with high and low levels of clustering, and test various interference mechanisms using simulations of agents facing a cooperative dilemma. Our results show that interference on scale-free networks is not trivial and that distinct levels of clustering react differently to each interference mechanism. As such, we argue that no tailored response fits all scale-free networks and present which mechanisms are more efficient at fostering cooperation in both types of networks. Finally, we discuss the pitfalls of considering reckless interference mechanisms.||||@arxiv||||2019/05/13||||Exogenous Rewards for Promoting Cooperation in Scale-Free Networks||||The design of mechanisms that encourage pro-social behaviours in populations of self-regarding agents is recognised as a major theoretical challenge within several areas of social, life and...||||https://arxiv.org/abs/1905.04964v2||||cs||||
72||||None||||Strict strategy-proofness||||arXiv.org||||2018/07/31||||Strict strategy-proofness||||Escudé, Matteo || Sinander, Ludvig||||https://arxiv.org/pdf/1807.11864||||1807.11864||||A strictly strategy-proof mechanism is one that asks agents to use strictly dominant strategies. In the canonical one-dimensional mechanism design setting with private values, we show that strict strategy-proofness is equivalent to strict monotonicity plus the envelope formula, echoing a well-known characterisation of (weak) strategy-proofness. A consequence is that strategy-proofness can be made strict by an arbitrarily small modification, so that strictness is 'essentially for free'.||||@arxiv||||2018/07/31||||Strict strategy-proofness||||A strictly strategy-proof mechanism is one that asks agents to use strictly dominant strategies. In the canonical one-dimensional mechanism design setting with private values, we show that strict...||||https://arxiv.org/abs/1807.11864v1||||econ||||
73||||None||||Binscatter Regressions||||arXiv.org||||2019/02/25||||Binscatter Regressions||||Cattaneo, Matias D. || Crump, Richard K. || Farrell, Max H. || Feng, Yingjie||||https://arxiv.org/pdf/1902.09615||||1902.09615||||We introduce the \texttt{Stata} (and \texttt{R}) package \textsf{Binsreg}, which implements the binscatter methods developed in \citet*{Cattaneo-Crump-Farrell-Feng_2019_Binscatter}. The package includes the commands \texttt{binsreg}, \texttt{binsregtest}, and \texttt{binsregselect}. The first command (\texttt{binsreg}) implements binscatter for the regression function and its derivatives, offering several point estimation, confidence intervals and confidence bands procedures, with particular focus on constructing binned scatter plots. The second command (\texttt{binsregtest}) implements hypothesis testing procedures for parametric specification and for nonparametric shape restrictions of the unknown regression function. Finally, the third command (\texttt{binsregselect}) implements data-driven number of bins selectors for binscatter implementation using either quantile-spaced or evenly-spaced binning/partitioning. All the commands allow for covariate adjustment, smoothness restrictions, weighting and clustering, among other features. A companion \texttt{R} package with the same capabilities is also available.||||@arxiv||||2019/02/25||||Binscatter Regressions||||We introduce the \texttt{Stata} (and \texttt{R}) package \textsf{Binsreg}, which implements the binscatter methods developed in \citet*{Cattaneo-Crump-Farrell-Feng_2019_Binscatter}. The package...||||https://arxiv.org/abs/1902.09615v1||||econ||||
74||||None||||Time discounting under uncertainty||||arXiv.org||||2019/11/01||||Time discounting under uncertainty||||Bastianello, Lorenzo || Faro, José Heleno||||https://arxiv.org/pdf/1911.00370||||1911.00370||||We study intertemporal decision making under uncertainty. We give the first full characterization of discounted expected utility in a framework à la Savage. Despite the popularity of this model, no characterization is available in the literature. The concept of stationarity, introduced by Koopmans for deterministic discounted utility, plays a central role for both attitudes towards time and towards uncertainty. We show that a strong stationarity axiom characterizes discounted expected utility. When hedging considerations are taken into account, a weaker stationarity axiom generalizes discounted expected utility to Choquet discounted expected utility, allowing for non-neutral attitudes towards uncertainty.||||@arxiv||||2019/11/01||||Time discounting under uncertainty||||We study intertemporal decision making under uncertainty. We give the first full characterization of discounted expected utility in a framework à la Savage. Despite the popularity of this model,...||||https://arxiv.org/abs/1911.00370v1||||econ||||
75||||None||||Should We Adjust for the Test for Pre-trends in Difference-in-Difference Designs?||||arXiv.org||||2018/05/02||||Should We Adjust for the Test for Pre-trends in Difference-in-Difference Designs?||||Roth, Jonathan||||https://arxiv.org/pdf/1804.01208||||1804.01208||||The common practice in difference-in-difference (DiD) designs is to check for parallel trends prior to treatment assignment, yet typical estimation and inference does not account for the fact that this test has occurred. I analyze the properties of the traditional DiD estimator conditional on having passed (i.e. not rejected) the test for parallel pre-trends. When the DiD design is valid and the test for pre-trends confirms it, the typical DiD estimator is unbiased, but traditional standard errors are overly conservative. Additionally, there exists an alternative unbiased estimator that is more efficient than the traditional DiD estimator under parallel trends. However, when in population there is a non-zero pre-trend but we fail to reject the hypothesis of parallel pre-trends, the DiD estimator is generally biased relative to the population DiD coefficient. Moreover, if the trend is monotone, then under reasonable assumptions the bias from conditioning exacerbates the bias relative to the true treatment effect. I propose new estimation and inference procedures that account for the test for parallel trends, and compare their performance to that of the traditional estimator in a Monte Carlo simulation.||||@arxiv||||2018/04/04||||Should We Adjust for the Test for Pre-trends in...||||The common practice in difference-in-difference (DiD) designs is to check for parallel trends prior to treatment assignment, yet typical estimation and inference does not account for the fact that...||||https://arxiv.org/abs/1804.01208v2||||econ||||
76||||None||||A Simple and Efficient Estimation of the Average Treatment Effect in the Presence of Unmeasured Confounders||||arXiv.org||||2018/07/16||||A Simple and Efficient Estimation of the Average Treatment Effect in the Presence of Unmeasured Confounders||||Ai, Chunrong || Huang, Lukang || Zhang, Zheng||||https://arxiv.org/pdf/1807.05678||||1807.05678||||Wang and Tchetgen Tchetgen (2017) studied identification and estimation of the average treatment effect when some confounders are unmeasured. Under their identification condition, they showed that the semiparametric efficient influence function depends on five unknown functionals. They proposed to parameterize all functionals and estimate the average treatment effect from the efficient influence function by replacing the unknown functionals with estimated functionals. They established that their estimator is consistent when certain functionals are correctly specified and attains the semiparametric efficiency bound when all functionals are correctly specified. In applications, it is likely that those functionals could all be misspecified. Consequently their estimator could be inconsistent or consistent but not efficient. This paper presents an alternative estimator that does not require parameterization of any of the functionals. We establish that the proposed estimator is always consistent and always attains the semiparametric efficiency bound. A simple and intuitive estimator of the asymptotic variance is presented, and a small scale simulation study reveals that the proposed estimation outperforms the existing alternatives in finite samples.||||@arxiv||||2018/07/16||||A Simple and Efficient Estimation of the Average Treatment Effect...||||Wang and Tchetgen Tchetgen (2017) studied identification and estimation of the average treatment effect when some confounders are unmeasured. Under their identification condition, they showed that...||||https://arxiv.org/abs/1807.05678v1||||econ||||
77||||None||||probitfe and logitfe: Bias corrections for probit and logit models with two-way fixed effects||||arXiv.org||||2017/02/26||||probitfe and logitfe: Bias corrections for probit and logit models with two-way fixed effects||||Cruz-Gonzalez, Mario || Fernandez-Val, Ivan || Weidner, Martin||||https://arxiv.org/pdf/1610.07714||||1610.07714||||We present the Stata commands probitfe and logitfe, which estimate probit and logit panel data models with individual and/or time unobserved effects. Fixed effect panel data methods that estimate the unobserved effects can be severely biased because of the incidental parameter problem (Neyman and Scott, 1948). We tackle this problem by using the analytical and jackknife bias corrections derived in Fernandez-Val and Weidner (2016) for panels where the two dimensions ($N$ and $T$) are moderately large. We illustrate the commands with an empirical application to international trade and a Monte Carlo simulation calibrated to this application.||||@arxiv||||2016/10/25||||probitfe and logitfe: Bias corrections for probit and logit models...||||We present the Stata commands probitfe and logitfe, which estimate probit and logit panel data models with individual and/or time unobserved effects. Fixed effect panel data methods that estimate...||||https://arxiv.org/abs/1610.07714v2||||econ||||
78||||None||||Partial Identification of Expectations with Interval Data||||arXiv.org||||2018/02/28||||Partial Identification of Expectations with Interval Data||||Asher, Sam || Novosad, Paul || Rafkin, Charlie||||https://arxiv.org/pdf/1802.10490||||1802.10490||||A conditional expectation function (CEF) can at best be partially identified when the conditioning variable is interval censored. When the number of bins is small, existing methods often yield minimally informative bounds. We propose three innovations that make meaningful inference possible in interval data contexts. First, we prove novel nonparametric bounds for contexts where the distribution of the censored variable is known. Second, we show that a class of measures that describe the conditional mean across a fixed interval of the conditioning space can often be bounded tightly even when the CEF itself cannot. Third, we show that a constraint on CEF curvature can either tighten bounds or can substitute for the monotonicity assumption often made in interval data applications. We derive analytical bounds that use the first two innovations, and develop a numerical method to calculate bounds under the third. We show the performance of the method in simulations and then present two applications. First, we resolve a known problem in the estimation of mortality as a function of education: because individuals with high school or less are a smaller and thus more negatively selected group over time, estimates of their mortality change are likely to be biased. Our method makes it possible to hold education rank bins constant over time, revealing that current estimates of rising mortality for less educated women are biased upward in some cases by a factor of three. Second, we apply the method to the estimation of intergenerational mobility, where researchers frequently use coarsely measured education data in the many contexts where matched parent-child income data are unavailable. Conventional measures like the rank-rank correlation may be uninformative once interval censoring is taken into account; CEF interval-based measures of mobility are bounded tightly.||||@arxiv||||2018/02/28||||Partial Identification of Expectations with Interval Data||||A conditional expectation function (CEF) can at best be partially identified when the conditioning variable is interval censored. When the number of bins is small, existing methods often yield...||||https://arxiv.org/abs/1802.10490v1||||econ||||
79||||None||||How on Earth: Flourishing in a Not-for-Profit World by 2050||||arXiv.org||||2019/02/04||||How on Earth: Flourishing in a Not-for-Profit World by 2050||||Hinton, Jennifer || Maclurcan, Donnie||||https://arxiv.org/pdf/1902.01398||||1902.01398||||In this book, we outline a model of a non-capitalist market economy based on not-for-profit forms of business. This work presents both a critique of the current economic system and a vision of a more socially, economically, and ecologically sustainable economy. The point of departure is the purpose and profit-orientation embedded in the legal forms used by businesses (e.g., for-profit or not-for-profit) and the ramifications of this for global sustainability challenges such as environmental pollution, resource use, climate change, and economic inequality. We document the rapid rise of not-for-profit forms of business in the global economy and offer a conceptual framework and an analytical lens through which to view these relatively new economic actors and their potential for transforming the economy. The book explores how a market consisting of only or mostly not-for-profit forms of business might lead to better financial circulation, economic equality, social well-being, and environmental regeneration as compared to for-profit markets.||||@arxiv||||2019/02/04||||How on Earth: Flourishing in a Not-for-Profit World by 2050||||In this book, we outline a model of a non-capitalist market economy based on not-for-profit forms of business. This work presents both a critique of the current economic system and a vision of a...||||https://arxiv.org/abs/1902.01398v1||||econ||||
80||||None||||Does Random Consideration Explain Behavior when Choice is Hard? Evidence from a Large-scale Experiment||||arXiv.org||||2019/06/17||||Does Random Consideration Explain Behavior when Choice is Hard? Evidence from a Large-scale Experiment||||Aguiar, Victor H. || Boccardi, Maria Jose || Kashaev, Nail || Kim, Jeongbin||||https://arxiv.org/pdf/1812.09619||||1812.09619||||We study population behavior when choice is hard because considering alternatives is costly. To simplify their choice problem, individuals may pay attention to only a subset of available alternatives. We design and implement a novel online experiment that exogenously varies choice sets and consideration costs for a large sample of individuals. We provide a theoretical and statistical framework that allows us to test random consideration at the population level. Within this framework, we compare competing models of random consideration. We find that the standard random utility model fails to explain the population behavior. However, our results suggest that a model of random consideration with logit attention and heterogeneous preferences provides a good explanation for the population behavior. Finally, we find that the random consideration rule that subjects use is different for different consideration costs while preferences are not. We observe that the higher the consideration cost the further behavior is from the full-consideration benchmark, which supports the hypothesis that hard choices have a substantial negative impact on welfare via limited consideration.||||@arxiv||||2018/12/22||||Does Random Consideration Explain Behavior when Choice is Hard?...||||We study population behavior when choice is hard because considering alternatives is costly. To simplify their choice problem, individuals may pay attention to only a subset of available...||||https://arxiv.org/abs/1812.09619v2||||econ||||
81||||None||||Granger causality on horizontal sum of Boolean algebras||||arXiv.org||||2018/10/03||||Granger causality on horizontal sum of Boolean algebras||||Bohdalová, M. || Kalina, M. || Nánásiová, O.||||https://arxiv.org/pdf/1810.01654||||1810.01654||||The intention of this paper is to discuss the mathematical model of causality introduced by C.W.J. Granger in 1969. The Granger's model of causality has become well-known and often used in various econometric models describing causal systems, e.g., between commodity prices and exchange rates.   Our paper presents a new mathematical model of causality between two measured objects. We have slightly modified the well-known Kolmogorovian probability model. In particular, we use the horizontal sum of set $σ$-algebras instead of their direct product.||||@arxiv||||2018/10/03||||Granger causality on horizontal sum of Boolean algebras||||The intention of this paper is to discuss the mathematical model of causality introduced by C.W.J. Granger in 1969. The Granger's model of causality has become well-known and often used in various...||||https://arxiv.org/abs/1810.01654v1||||econ||||
82||||None||||Compactification of Extensive Forms and Belief in the Opponents' Future Rationality||||arXiv.org||||2019/05/02||||Compactification of Extensive Forms and Belief in the Opponents' Future Rationality||||Liu, Shuige||||https://arxiv.org/pdf/1905.00355||||1905.00355||||We introduce an operation, called compactification, to reduce an extensive form to a compact one where each decision node in the game tree can be assigned to more than one player. Motivated by Thompson (1952)'s interchange of decision nodes, we attempt to capture the notion of a faithful representation of the chronological order of the moves in a dynamic game which plays a vital role in fields like epistemic game theory. The compactification process preserves perfect recall and the unambiguity of the order among information sets. We specify an algorithm, called leaves-to-root process, which compactifies at least as many information sets as any other compactification process. The compact extensive form provides an approach to avoid problems in dynamic game theory due to the vague definition of the chronological order of the moves, for example, belief in the opponents' future rationality (Perea (2014))'s sensitivity to the specific extensive form representation. We show that any strategy which can rationally be chosen under common belief in future rationality in a minimal compact game if and only if it satisfies this property in every extensive form game which is related to it via some compactification process.||||@arxiv||||2019/05/01||||Compactification of Extensive Forms and Belief in the...||||We introduce an operation, called compactification, to reduce an extensive form to a compact one where each decision node in the game tree can be assigned to more than one player. Motivated by...||||https://arxiv.org/abs/1905.00355v2||||econ||||
83||||None||||From Disequilibrium Markets to Equilibrium||||arXiv.org||||2019/12/20||||From Disequilibrium Markets to Equilibrium||||Lax, Christian || Trimborn, Torsten||||https://arxiv.org/pdf/1912.09679||||1912.09679||||The modeling of financial markets as disequilibrium models by ordinary differential equations has become a popular modeling tool. One famous example of such a model is the Beja-Goldman model(The Journal of Finance, 1980) which we consider in this paper. We study the passage from disequilibrium dynamics to equilibrium. Mathematically, this limit corresponds to an asymptotic limit also known as a Tikhonov-Fenichel reduction. Furthermore, we analyze the stability of the reduced equilibrium model and discuss the economic implications. We conduct several numerical examples to visualize and support our analysis.||||@arxiv||||2019/12/20||||From Disequilibrium Markets to Equilibrium||||The modeling of financial markets as disequilibrium models by ordinary differential equations has become a popular modeling tool. One famous example of such a model is the Beja-Goldman model(The...||||https://arxiv.org/abs/1912.09679v1||||econ||||
84||||None||||Design-based Analysis in Difference-In-Differences Settings with Staggered Adoption||||arXiv.org||||2018/09/01||||Design-based Analysis in Difference-In-Differences Settings with Staggered Adoption||||Athey, Susan || Imbens, Guido||||https://arxiv.org/pdf/1808.05293||||1808.05293||||In this paper we study estimation of and inference for average treatment effects in a setting with panel data. We focus on the setting where units, e.g., individuals, firms, or states, adopt the policy or treatment of interest at a particular point in time, and then remain exposed to this treatment at all times afterwards. We take a design perspective where we investigate the properties of estimators and procedures given assumptions on the assignment process. We show that under random assignment of the adoption date the standard Difference-In-Differences estimator is is an unbiased estimator of a particular weighted average causal effect. We characterize the proeperties of this estimand, and show that the standard variance estimator is conservative.||||@arxiv||||2018/08/15||||Design-based Analysis in Difference-In-Differences Settings with...||||In this paper we study estimation of and inference for average treatment effects in a setting with panel data. We focus on the setting where units, e.g., individuals, firms, or states, adopt the...||||https://arxiv.org/abs/1808.05293v3||||cs||||
86||||None||||Orthogonal Statistical Learning||||arXiv.org||||2019/03/11||||Orthogonal Statistical Learning||||Foster, Dylan J. || Syrgkanis, Vasilis||||https://arxiv.org/pdf/1901.09036||||1901.09036||||We provide excess risk guarantees for statistical learning in a setting where the population risk with respect to which we evaluate the target model depends on an unknown model that must be to be estimated from data (a "nuisance model"). We analyze a two-stage sample splitting meta-algorithm that takes as input two arbitrary estimation algorithms: one for the target model and one for the nuisance model. We show that if the population risk satisfies a condition called Neyman orthogonality, the impact of the nuisance estimation error on the excess risk bound achieved by the meta-algorithm is of second order. Our theorem is agnostic to the particular algorithms used for the target and nuisance and only makes an assumption on their individual performance. This enables the use of a plethora of existing results from statistical learning and machine learning literature to give new guarantees for learning with a nuisance component. Moreover, by focusing on excess risk rather than parameter estimation, we can give guarantees under weaker assumptions than in previous works and accommodate the case where the target parameter belongs to a complex nonparametric class. We characterize conditions on the metric entropy such that oracle rates---rates of the same order as if we knew the nuisance model---are achieved. We also analyze the rates achieved by specific estimation algorithms such as variance-penalized empirical risk minimization, neural network estimation and sparse high-dimensional linear model estimation. We highlight the applicability of our results in four settings of central importance in the literature: 1) heterogeneous treatment effect estimation, 2) offline policy optimization, 3) domain adaptation, and 4) learning with missing data.||||@arxiv||||2019/01/25||||Orthogonal Statistical Learning||||We provide excess risk guarantees for statistical learning in a setting where the population risk with respect to which we evaluate the target model depends on an unknown model that must be to be...||||https://arxiv.org/abs/1901.09036v2||||cs||||
87||||None||||Sparse Approximate Factor Estimation for High-Dimensional Covariance Matrices||||arXiv.org||||2019/06/13||||Sparse Approximate Factor Estimation for High-Dimensional Covariance Matrices||||Daniele, Maurizio || Pohlmeier, Winfried || Zagidullina, Aygul||||https://arxiv.org/pdf/1906.05545||||1906.05545||||We propose a novel estimation approach for the covariance matrix based on the $l_1$-regularized approximate factor model. Our sparse approximate factor (SAF) covariance estimator allows for the existence of weak factors and hence relaxes the pervasiveness assumption generally adopted for the standard approximate factor model. We prove consistency of the covariance matrix estimator under the Frobenius norm as well as the consistency of the factor loadings and the factors.   Our Monte Carlo simulations reveal that the SAF covariance estimator has superior properties in finite samples for low and high dimensions and different designs of the covariance matrix. Moreover, in an out-of-sample portfolio forecasting application the estimator uniformly outperforms alternative portfolio strategies based on alternative covariance estimation approaches and modeling strategies including the $1/N$-strategy.||||@arxiv||||2019/06/13||||Sparse Approximate Factor Estimation for High-Dimensional...||||We propose a novel estimation approach for the covariance matrix based on the $l_1$-regularized approximate factor model. Our sparse approximate factor (SAF) covariance estimator allows for the...||||https://arxiv.org/abs/1906.05545v1||||econ||||
88||||None||||Economic inequality and Islamic Charity: An exploratory agent-based modeling approach||||arXiv.org||||2018/04/24||||Economic inequality and Islamic Charity: An exploratory agent-based modeling approach||||Sabzian, Hossein || Aliahmadi, Alireza || Azar, Adel || Mirzaee, Madjid||||https://arxiv.org/pdf/1804.09284||||1804.09284||||Economic inequality is one of the pivotal issues for most of economic and social policy makers across the world to insure the sustainable economic growth and justice. In the mainstream school of economics, namely neoclassical theories, economic issues are dealt with in a mechanistic manner. Such a mainstream framework is majorly focused on investigating a socio-economic system based on an axiomatic scheme where reductionism approach plays a vital role. The major limitations of such theories include unbounded rationality of economic agents, reducing the economic aggregates to a set of predictable factors and lack of attention to adaptability and the evolutionary nature of economic agents. In tackling deficiencies of conventional economic models, in the past two decades, some new approaches have been recruited. One of those novel approaches is the Complex adaptive systems (CAS) framework which has shown a very promising performance in action. In contrast to mainstream school, under this framework, the economic phenomena are studied in an organic manner where the economic agents are supposed to be both boundedly rational and adaptive. According to it, the economic aggregates emerge out of the ways agents of a system decide and interact. As a powerful way of modeling CASs, Agent-based models (ABMs) has found a growing application among academicians and practitioners. ABMs show that how simple behavioral rules of agents and local interactions among them at micro-scale can generate surprisingly complex patterns at macro-scale. In this paper, ABMs have been used to show (1) how an economic inequality emerges in a system and to explain (2) how sadaqah as an Islamic charity rule can majorly help alleviating the inequality and how resource allocation strategies taken by charity entities can accelerate this alleviation.||||@arxiv||||2018/04/24||||Economic inequality and Islamic Charity: An exploratory...||||Economic inequality is one of the pivotal issues for most of economic and social policy makers across the world to insure the sustainable economic growth and justice. In the mainstream school of...||||https://arxiv.org/abs/1804.09284v1||||econ||||
89||||None||||Dual Regression||||arXiv.org||||2018/09/23||||Dual Regression||||Spady, Richard || Stouli, Sami||||https://arxiv.org/pdf/1210.6958||||1210.6958||||We propose dual regression as an alternative to the quantile regression process for the global estimation of conditional distribution functions under minimal assumptions. Dual regression provides all the interpretational power of the quantile regression process while avoiding the need for repairing the intersecting conditional quantile surfaces that quantile regression often produces in practice. Our approach introduces a mathematical programming characterization of conditional distribution functions which, in its simplest form, is the dual program of a simultaneous estimator for linear location-scale models. We apply our general characterization to the specification and estimation of a flexible class of conditional distribution functions, and present asymptotic theory for the corresponding empirical dual regression process.||||@arxiv||||2012/10/25||||Dual Regression||||We propose dual regression as an alternative to the quantile regression process for the global estimation of conditional distribution functions under minimal assumptions. Dual regression provides...||||https://arxiv.org/abs/1210.6958v4||||econ||||
90||||None||||Testing nonparametric shape restrictions||||arXiv.org||||2019/09/04||||Testing nonparametric shape restrictions||||Komarova, Tatiana || Hidalgo, Javier||||https://arxiv.org/pdf/1909.01675||||1909.01675||||We describe and examine a test for shape constraints, such as monotonicity, convexity (or both simultaneously), U-shape, S-shape and others, in a nonparametric framework using partial sums empirical processes. We show that, after a suitable transformation, its asymptotic distribution is a functional of the standard Brownian motion, so that critical values are available. However, due to the possible poor approximation of the asymptotic critical values to the finite sample ones, we also describe a valid bootstrap algorithm.||||@arxiv||||2019/09/04||||Testing nonparametric shape restrictions||||We describe and examine a test for shape constraints, such as monotonicity, convexity (or both simultaneously), U-shape, S-shape and others, in a nonparametric framework using partial sums...||||https://arxiv.org/abs/1909.01675v1||||econ||||
91||||None||||Heterogeneous Employment Effects of Job Search Programmes: A Machine Learning Approach||||arXiv.org||||2018/05/12||||Heterogeneous Employment Effects of Job Search Programmes: A Machine Learning Approach||||Knaus, Michael || Lechner, Michael || Strittmatter, Anthony||||https://arxiv.org/pdf/1709.10279||||1709.10279||||We systematically investigate the effect heterogeneity of job search programmes for unemployed workers. To investigate possibly heterogeneous employment effects, we combine non-experimental causal empirical models with Lasso-type estimators. The empirical analyses are based on rich administrative data from Swiss social security records. We find considerable heterogeneities only during the first six months after the start of training. Consistent with previous results of the literature, unemployed persons with fewer employment opportunities profit more from participating in these programmes. Furthermore, we also document heterogeneous employment effects by residence status. Finally, we show the potential of easy-to-implement programme participation rules for improving average employment effects of these active labour market programmes.||||@arxiv||||2017/09/29||||Heterogeneous Employment Effects of Job Search Programmes: A...||||We systematically investigate the effect heterogeneity of job search programmes for unemployed workers. To investigate possibly heterogeneous employment effects, we combine non-experimental causal...||||https://arxiv.org/abs/1709.10279v2||||econ||||
92||||None||||Why Markets are Inefficient: A Gambling "Theory" of Financial Markets For Practitioners and Theorists||||arXiv.org||||2018/01/06||||Why Markets are Inefficient: A Gambling "Theory" of Financial Markets For Practitioners and Theorists||||Moffitt, Steven D.||||https://arxiv.org/pdf/1801.01948||||1801.01948||||The purpose of this article is to propose a new "theory," the Strategic Analysis of Financial Markets (SAFM) theory, that explains the operation of financial markets using the analytical perspective of an enlightened gambler. The gambler understands that all opportunities for superior performance arise from suboptimal decisions by humans, but understands also that knowledge of human decision making alone is not enough to understand market behavior --- one must still model how those decisions lead to market prices. Thus are there three parts to the model: gambling theory, human decision making, and strategic problem solving. A new theory is necessary because at this writing in 2017, there is no theory of financial markets acceptable to both practitioners and theorists. Theorists' efficient market theory, for example, cannot explain bubbles and crashes nor the exceptional returns of famous investors and speculators such as Warren Buffett and George Soros. At the same time, a new theory must be sufficiently quantitative, explain market "anomalies" and provide predictions in order to satisfy theorists. It is hoped that the SAFM framework will meet these requirements.||||@arxiv||||2018/01/06||||Why Markets are Inefficient: A Gambling "Theory" of...||||The purpose of this article is to propose a new "theory," the Strategic Analysis of Financial Markets (SAFM) theory, that explains the operation of financial markets using the analytical...||||https://arxiv.org/abs/1801.01948v1||||econ||||
93||||None||||Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments||||arXiv.org||||2019/09/03||||Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments||||Chernozhukov, Victor || Demirer, Mert || Duflo, Esther || Fernández-Val, Iván||||https://arxiv.org/pdf/1712.04802||||1712.04802||||We propose strategies to estimate and make inference on key features of heterogeneous effects in randomized experiments. These key features include best linear predictors of the effects using machine learning proxies, average effects sorted by impact groups, and average characteristics of most and least impacted units. The approach is valid in high dimensional settings, where the effects are proxied by machine learning methods. We post-process these proxies into the estimates of the key features. Our approach is generic, it can be used in conjunction with penalized methods, deep and shallow neural networks, canonical and new random forests, boosted trees, and ensemble methods. It does not rely on strong assumptions. In particular, we don't require conditions for consistency of the machine learning methods. Estimation and inference relies on repeated data splitting to avoid overfitting and achieve validity. For inference, we take medians of p-values and medians of confidence intervals, resulting from many different data splits, and then adjust their nominal level to guarantee uniform validity. This variational inference method is shown to be uniformly valid and quantifies the uncertainty coming from both parameter estimation and data splitting. We illustrate the use of the approach with two randomized experiments in development on the effects of microcredit and nudges to stimulate immunization demand.||||@arxiv||||2017/12/13||||Generic Machine Learning Inference on Heterogenous Treatment...||||We propose strategies to estimate and make inference on key features of heterogeneous effects in randomized experiments. These key features include best linear predictors of the effects using...||||https://arxiv.org/abs/1712.04802v4||||econ||||
94||||None||||Simulation Modelling of Inequality in Cancer Service Access||||arXiv.org||||2018/07/09||||Simulation Modelling of Inequality in Cancer Service Access||||Chan, Ka C. || Williams, Ruth F. G. || Lenard, Christopher T. || Mills, Terence M.||||https://arxiv.org/pdf/1807.03048||||1807.03048||||This paper applies economic concepts from measuring income inequality to an exercise in assessing spatial inequality in cancer service access in regional areas. We propose a mathematical model for accessing chemotherapy among local government areas (LGAs). Our model incorporates a distance factor. With a simulation we report results for a single inequality measure: the Lorenz curve is depicted for our illustrative data. We develop this approach in order to move incrementally towards its application to actual data and real-world health service regions. We seek to develop the exercises that can lead policy makers to relevant policy information on the most useful data collections to be collected and modeling for cancer service access in regional areas.||||@arxiv||||2018/07/09||||Simulation Modelling of Inequality in Cancer Service Access||||This paper applies economic concepts from measuring income inequality to an exercise in assessing spatial inequality in cancer service access in regional areas. We propose a mathematical model for...||||https://arxiv.org/abs/1807.03048v1||||econ||||
95||||None||||Optimal Control of Prevention and Treatment in a Basic Macroeconomic-Epidemiological Model||||arXiv.org||||2019/10/08||||Optimal Control of Prevention and Treatment in a Basic Macroeconomic-Epidemiological Model||||La Torre, Davide || Malik, Tufail || Marsiglio, Simone||||https://arxiv.org/pdf/1910.03383||||1910.03383||||We analyze the optimal control of disease prevention and treatment in a basic SIS model. We develop a simple macroeconomic setup in which the social planner determines how to optimally intervene, through income taxation, in order to minimize the social cost, inclusive of infection and economic costs, of the spread of an epidemic disease. The disease lowers economic production and thus income by reducing the size of the labor force employed in productive activities, tightening thus the economy's overall resources constraint. We consider a framework in which the planner uses the collected tax revenue to intervene in either prevention (aimed at reducing the rate of infection) or treatment (aimed at increasing the speed of recovery). Both optimal prevention and treatment policies allow the economy to achieve a disease-free equilibrium in the long run but their associated costs are substantially different along the transitional dynamic path. By quantifying the social costs associated with prevention and treatment we determine which policy is most cost-effective under different circumstances, showing that prevention (treatment) is desirable whenever the infectivity rate is low (high).||||@arxiv||||2019/10/08||||Optimal Control of Prevention and Treatment in a Basic...||||We analyze the optimal control of disease prevention and treatment in a basic SIS model. We develop a simple macroeconomic setup in which the social planner determines how to optimally intervene,...||||https://arxiv.org/abs/1910.03383v1||||econ||||
96||||None||||Indirect transactions and requirements||||arXiv.org||||2019/12/28||||Indirect transactions and requirements||||Coskun, Husna Betul||||https://arxiv.org/pdf/1911.11569||||1911.11569||||The indirect transactions between sectors of an economic system has been a long-standing open problem. There have been numerous attempts to define and mathematically formulate this concept in various other scientific fields in literature as well. The existing indirect effects formulations, however, cannot quantify the indirect transactions between any two sectors of an economic system. Consequently, although the direct and total requirement matrices are formulated and used for economic system analysis, the indirect requirements matrix has never been formulated before. Based on the system decomposition theory, the indirect transactions and the corresponding indirect requirements matrix are introduced in the present article for the first time. This novel concept of the indirect transactions is also compared with some existing indirect effect formulations, and the theoretical advancement brought by the proposed methodology is discussed. It is shown theoretically and through illustrative examples that the proposed indirect transactions accurately describe and quantify the indirect interactions and relationships, unlike the current indirect effects formulations. The indirect requirements matrices for the US economy using aggregated input-output tables for multiple years are also presented and briefly analyzed.||||@arxiv||||2019/11/23||||Indirect transactions and requirements||||The indirect transactions between sectors of an economic system has been a long-standing open problem. There have been numerous attempts to define and mathematically formulate this concept in...||||https://arxiv.org/abs/1911.11569v3||||econ||||
97||||None||||A tail dependence-based MST and their topological indicators in modelling systemic risk in the European insurance sector||||arXiv.org||||2020/01/18||||A tail dependence-based MST and their topological indicators in modelling systemic risk in the European insurance sector||||Denkowska, Anna || Wanat, Stanisław||||https://arxiv.org/pdf/2001.06567||||2001.06567||||In the present work we analyze the dynamics of indirect connections between insurance companies that result from market price channels. In our analysis we assume that the stock quotations of insurance companies reflect market sentiments which constitute a very important systemic risk factor. Interlinkages between insurers and their dynamics have a direct impact on systemic risk contagion in the insurance sector. We propose herein a new hybrid approach to the analysis of interlinkages dynamics based on combining the copula-DCC-GARCH model and Minimum Spanning Trees (MST). Using the copula-DCC-GARCH model we determine the correlation coefficients in the distribution tails. Then, for each analysed period we construct MST based on these coefficients. The dynamics is analysed by means of time series of selected topological indicators of the MSTs. Our empirical results show the usefulness of the proposed approach to the analysis of systemic risk in the insurance sector. The times series obtained from the proposed hybrid approach reflect the phenomena occurring on the market. The analysed MST topological indicators can be considered as systemic risk predictors.||||@arxiv||||2020/01/18||||A tail dependence-based MST and their topological indicators in...||||In the present work we analyze the dynamics of indirect connections between insurance companies that result from market price channels. In our analysis we assume that the stock quotations of...||||https://arxiv.org/abs/2001.06567v1||||econ||||
98||||None||||Predicting Economic Recessions Using Machine Learning Algorithms||||arXiv.org||||2017/01/03||||Predicting Economic Recessions Using Machine Learning Algorithms||||Nyman, Rickard || Ormerod, Paul||||https://arxiv.org/pdf/1701.01428||||1701.01428||||Even at the beginning of 2008, the economic recession of 2008/09 was not being predicted. The failure to predict recessions is a persistent theme in economic forecasting. The Survey of Professional Forecasters (SPF) provides data on predictions made for the growth of total output, GDP, in the United States for one, two, three and four quarters ahead since the end of the 1960s. Over a three quarters ahead horizon, the mean prediction made for GDP growth has never been negative over this period. The correlation between the mean SPF three quarters ahead forecast and the data is very low, and over the most recent 25 years is not significantly different from zero.   Here, we show that the machine learning technique of random forests has the potential to give early warning of recessions. We use a small set of explanatory variables from financial markets which would have been available to a forecaster at the time of making the forecast. We train the algorithm over the 1970Q2-1990Q1 period, and make predictions one, three and six quarters ahead. We then re-train over 1970Q2-1990Q2 and make a further set of predictions, and so on. We did not attempt any optimisation of predictions, using only the default input parameters to the algorithm we downloaded in the package R.   We compare the predictions made from 1990 to the present with the actual data. One quarter ahead, the algorithm is not able to improve on the SPF predictions. Three and six quarters ahead, the correlations between actual and predicted are low, but they are very significantly different from zero. Although the timing is slightly wrong, a serious downturn in the first half of 2009 could have been predicted six quarters ahead in late 2007. The algorithm never predicts a recession when one did not occur.   We obtain even stronger results with random forest machine learning techniques in the case of the United Kingdom.||||@arxiv||||2017/01/03||||Predicting Economic Recessions Using Machine Learning Algorithms||||Even at the beginning of 2008, the economic recession of 2008/09 was not being predicted. The failure to predict recessions is a persistent theme in economic forecasting. The Survey of...||||https://arxiv.org/abs/1701.01428v1||||q-fin||||
99||||None||||Transboundary Pollution Externalities: Think Globally, Act Locally?||||arXiv.org||||2019/10/10||||Transboundary Pollution Externalities: Think Globally, Act Locally?||||La Torre, Davide || Liuzzi, Danilo || Marsiglio, Simone||||https://arxiv.org/pdf/1910.04469||||1910.04469||||We analyze the implications of transboundary pollution externalities on environmental policymaking in a spatial and finite time horizon setting. We focus on a simple regional optimal pollution control problem in order to compare the global and local solutions in which, respectively, the transboundary externality is and is not taken into account in the determination of the optimal policy by individual local policymakers. We show that the local solution is suboptimal and as such a global approach to environmental problems is effectively needed. Our conclusions hold true in different frameworks, including situations in which the spatial domain is either bounded or unbounded, and situations in which macroeconomic-environmental feedback effects are taken into account. We also show that if every local economy implements an environmental policy stringent enough, then the global average level of pollution will fall. If this is the case, over the long run the entire global economy will be able to achieve a completely pollution-free status.||||@arxiv||||2019/10/10||||Transboundary Pollution Externalities: Think Globally, Act Locally?||||We analyze the implications of transboundary pollution externalities on environmental policymaking in a spatial and finite time horizon setting. We focus on a simple regional optimal pollution...||||https://arxiv.org/abs/1910.04469v1||||econ||||
100||||None||||Stratification Trees for Adaptive Randomization in Randomized Controlled Trials||||arXiv.org||||2020/01/12||||Stratification Trees for Adaptive Randomization in Randomized Controlled Trials||||Tabord-Meehan, Max||||https://arxiv.org/pdf/1806.05127||||1806.05127||||This paper proposes an adaptive randomization procedure for two-stage randomized controlled trials. The method uses data from a first-wave experiment in order to determine how to stratify in a second wave of the experiment, where the objective is to minimize the variance of an estimator for the average treatment effect (ATE). We consider selection from a class of stratified randomization procedures which we call stratification trees: these are procedures whose strata can be represented as decision trees, with differing treatment assignment probabilities across strata. By using the first wave to estimate a stratification tree, we simultaneously select which covariates to use for stratification, how to stratify over these covariates, as well as the assignment probabilities within these strata. Our main result shows that using this randomization procedure with an appropriate estimator results in an asymptotic variance which is minimal in the class of stratification trees. Moreover, the results we present are able to accommodate a large class of assignment mechanisms within strata, including stratified block randomization. In a simulation study, we find that our method, paired with an appropriate cross-validation procedure ,can improve on ad-hoc choices of stratification. We conclude by applying our method to the study in Karlan and Wood (2017), where we estimate stratification trees using the first wave of their experiment.||||@arxiv||||2018/06/13||||Stratification Trees for Adaptive Randomization in Randomized...||||This paper proposes an adaptive randomization procedure for two-stage randomized controlled trials. The method uses data from a first-wave experiment in order to determine how to stratify in a...||||https://arxiv.org/abs/1806.05127v4||||econ||||
101||||None||||A Profit Optimization Approach Based on the Use of Pumped-Hydro Energy Storage Unit and Dynamic Pricing||||arXiv.org||||2018/06/07||||A Profit Optimization Approach Based on the Use of Pumped-Hydro Energy Storage Unit and Dynamic Pricing||||Taşcikaraoğlu, Akın || Erdinç, Ozan||||https://arxiv.org/pdf/1806.05211||||1806.05211||||In this study, an optimization problem is proposed in order to obtain the maximum economic benefit from wind farms with variable and intermittent energy generation in the day ahead and balancing electricity markets. This method, which is based on the use of pumped-hydro energy storage unit and wind farm together, increases the profit from the power plant by taking advantage of the price changes in the markets and at the same time supports the power system by supplying a portion of the peak load demand in the system to which the plant is connected. With the objective of examining the effectiveness of the proposed method, detailed simulation studies are carried out by making use of actual wind and price data, and the results are compared to those obtained for the various cases in which the storage unit is not available and/or the proposed price-based energy management method is not applied. As a consequence, it is demonstrated that the pumped-hydro energy storage units are the storage systems capable of being used effectively for high-power levels and that the proposed optimization problem is quite successful in the cost-effective implementation of these systems.||||@arxiv||||2018/06/07||||A Profit Optimization Approach Based on the Use of Pumped-Hydro...||||In this study, an optimization problem is proposed in order to obtain the maximum economic benefit from wind farms with variable and intermittent energy generation in the day ahead and balancing...||||https://arxiv.org/abs/1806.05211v1||||econ||||
102||||None||||Interdistrict School Choice: A Theory of Student Assignment||||arXiv.org||||2019/01/06||||Interdistrict School Choice: A Theory of Student Assignment||||Hafalir, Isa E. || Kojima, Fuhito || Yenmez, M. Bumin||||https://arxiv.org/pdf/1812.11297||||1812.11297||||Interdistrict school choice programs-where a student can be assigned to a school outside of her district-are widespread in the US, yet the market-design literature has not considered such programs. We introduce a model of interdistrict school choice and present two mechanisms that produce stable or efficient assignments. We consider three categories of policy goals on assignments and identify when the mechanisms can achieve them. By introducing a novel framework of interdistrict school choice, we provide a new avenue of research in market design.||||@arxiv||||2018/12/29||||Interdistrict School Choice: A Theory of Student Assignment||||Interdistrict school choice programs-where a student can be assigned to a school outside of her district-are widespread in the US, yet the market-design literature has not considered such...||||https://arxiv.org/abs/1812.11297v2||||econ||||
103||||None||||Injectivity and the Law of Demand||||arXiv.org||||2019/08/15||||Injectivity and the Law of Demand||||Allen, Roy||||https://arxiv.org/pdf/1908.05714||||1908.05714||||Establishing that a demand mapping is injective is core first step for a variety of methodologies. When a version of the law of demand holds, global injectivity can be checked by seeing whether the demand mapping is constant over any line segments. When we add the assumption of differentiability, we obtain necessary and sufficient conditions for injectivity that generalize classical \cite{gale1965jacobian} conditions for quasi-definite Jacobians.||||@arxiv||||2019/08/15||||Injectivity and the Law of Demand||||Establishing that a demand mapping is injective is core first step for a variety of methodologies. When a version of the law of demand holds, global injectivity can be checked by seeing whether...||||https://arxiv.org/abs/1908.05714v1||||econ||||
104||||None||||Conditions for the uniqueness of the Gately point for cooperative games||||arXiv.org||||2019/01/06||||Conditions for the uniqueness of the Gately point for cooperative games||||Staudacher, Jochen || Anwander, Johannes||||https://arxiv.org/pdf/1901.01485||||1901.01485||||We are studying the Gately point, an established solution concept for cooperative games. We point out that there are superadditive games for which the Gately point is not unique, i.e. in general the concept is rather set-valued than an actual point. We derive conditions under which the Gately point is guaranteed to be a unique imputation and provide a geometric interpretation. The Gately point can be understood as the intersection of a line defined by two points with the set of imputations. Our uniqueness conditions guarantee that these two points do not coincide. We provide demonstrative interpretations for negative propensities to disrupt. We briefly show that our uniqueness conditions for the Gately point include quasibalanced games and discuss the relation of the Gately point to the $τ$-value in this context. Finally, we point out relations to cost games and the ACA method and end upon a few remarks on the implementation of the Gately point and an upcoming software package for cooperative game theory.||||@arxiv||||2019/01/06||||Conditions for the uniqueness of the Gately point for cooperative games||||We are studying the Gately point, an established solution concept for cooperative games. We point out that there are superadditive games for which the Gately point is not unique, i.e. in general...||||https://arxiv.org/abs/1901.01485v1||||econ||||
105||||None||||Anticipated impacts of Brexit scenarios on UK food prices and implications for policies on poverty and health: a structured expert judgement approach||||arXiv.org||||2020/01/22||||Anticipated impacts of Brexit scenarios on UK food prices and implications for policies on poverty and health: a structured expert judgement approach||||Barons, Martine J || Aspinall, Willy||||https://arxiv.org/pdf/1904.03053||||1904.03053||||Food insecurity is associated with increased risk for several health conditions and with poor chronic disease management. Key determinants for household food insecurity are income and food costs. Whereas short-term household incomes are likely to remain static, increased food prices would be a significant driver of food insecurity. To investigate food price drivers for household food security and its health consequences in the UK under scenarios of Deal and No deal for Brexit . To estimate the 5\% and 95\% quantiles of the projected price distributions. Structured expert judgement elicitation, a well-established method for quantifying uncertainty, using experts. In July 2018, each expert estimated the median, 5\% and 95\% quantiles of changes in price for ten food categories under Brexit Deal and No-deal to June 2020 assuming Brexit had taken place on 29th March 2019. These were aggregated based on the accuracy and informativeness of the experts on calibration questions. Ten specialists in food procurement, retail, agriculture, economics, statistics and household food security. Results: when combined in proportions used to calculate Consumer Prices Index food basket costs, median food price change for Brexit with a Deal is expected to be +6.1\% [90\% credible interval:-3\%, +17\%] and with No deal +22.5\% [+1\%, +52\%]. The number of households experiencing food insecurity and its severity are likely to increase because of expected sizeable increases in median food prices after Brexit. Higher increases are more likely than lower rises and towards the upper limits, these would entail severe impacts. Research showing a low food budget leads to increasingly poor diet suggests that demand for health services in both the short and longer term is likely to increase due to the effects of food insecurity on the incidence and management of diet-sensitive conditions.||||@arxiv||||2019/04/05||||Anticipated impacts of Brexit scenarios on UK food prices and...||||Food insecurity is associated with increased risk for several health conditions and with poor chronic disease management. Key determinants for household food insecurity are income and food costs....||||https://arxiv.org/abs/1904.03053v3||||econ||||
106||||None||||Testing for Common Breaks in a Multiple Equations System||||arXiv.org||||2018/01/11||||Testing for Common Breaks in a Multiple Equations System||||Oka, Tatsushi || Perron, Pierre||||https://arxiv.org/pdf/1606.00092||||1606.00092||||The issue addressed in this paper is that of testing for common breaks across or within equations of a multivariate system. Our framework is very general and allows integrated regressors and trends as well as stationary regressors. The null hypothesis is that breaks in different parameters occur at common locations and are separated by some positive fraction of the sample size unless they occur across different equations. Under the alternative hypothesis, the break dates across parameters are not the same and also need not be separated by a positive fraction of the sample size whether within or across equations. The test considered is the quasi-likelihood ratio test assuming normal errors, though as usual the limit distribution of the test remains valid with non-normal errors. Of independent interest, we provide results about the rate of convergence of the estimates when searching over all possible partitions subject only to the requirement that each regime contains at least as many observations as some positive fraction of the sample size, allowing break dates not separated by a positive fraction of the sample size across equations. Simulations show that the test has good finite sample properties. We also provide an application to issues related to level shifts and persistence for various measures of inflation to illustrate its usefulness.||||@arxiv||||2016/06/01||||Testing for Common Breaks in a Multiple Equations System||||The issue addressed in this paper is that of testing for common breaks across or within equations of a multivariate system. Our framework is very general and allows integrated regressors and...||||https://arxiv.org/abs/1606.00092v2||||econ||||
107||||None||||Rearranging Edgeworth-Cornish-Fisher Expansions||||arXiv.org||||2013/05/31||||Rearranging Edgeworth-Cornish-Fisher Expansions||||Chernozhukov, Victor || Fernandez-Val, Ivan || Galichon, Alfred||||https://arxiv.org/pdf/0708.1627||||0708.1627||||This paper applies a regularization procedure called increasing rearrangement to monotonize Edgeworth and Cornish-Fisher expansions and any other related approximations of distribution and quantile functions of sample statistics. Besides satisfying the logical monotonicity, required of distribution and quantile functions, the procedure often delivers strikingly better approximations to the distribution and quantile functions of the sample mean than the original Edgeworth-Cornish-Fisher expansions.||||@arxiv||||2007/08/12||||Rearranging Edgeworth-Cornish-Fisher Expansions||||This paper applies a regularization procedure called increasing rearrangement to monotonize Edgeworth and Cornish-Fisher expansions and any other related approximations of distribution and...||||https://arxiv.org/abs/0708.1627v2||||econ||||
108||||None||||Understanding consumer demand for new transport technologies and services, and implications for the future of mobility||||arXiv.org||||2019/04/11||||Understanding consumer demand for new transport technologies and services, and implications for the future of mobility||||Vij, Akshay||||https://arxiv.org/pdf/1904.05554||||1904.05554||||The transport sector is witnessing unprecedented levels of disruption. Privately owned cars that operate on internal combustion engines have been the dominant modes of passenger transport for much of the last century. However, recent advances in transport technologies and services, such as the development of autonomous vehicles, the emergence of shared mobility services, and the commercialization of alternative fuel vehicle technologies, promise to revolutionise how humans travel. The implications are profound: some have predicted the end of private car dependent Western societies, others have portended greater suburbanization than has ever been observed before. If transport systems are to fulfil current and future needs of different subpopulations, and satisfy short and long-term societal objectives, it is imperative that we comprehend the many factors that shape individual behaviour. This chapter introduces the technologies and services most likely to disrupt prevailing practices in the transport sector. We review past studies that have examined current and future demand for these new technologies and services, and their likely short and long-term impacts on extant mobility patterns. We conclude with a summary of what these new technologies and services might mean for the future of mobility.||||@arxiv||||2019/04/11||||Understanding consumer demand for new transport technologies and...||||The transport sector is witnessing unprecedented levels of disruption. Privately owned cars that operate on internal combustion engines have been the dominant modes of passenger transport for much...||||https://arxiv.org/abs/1904.05554v1||||econ||||
109||||None||||Multilevel evolutionary developmental optimization (MEDO): A theoretical framework for understanding preferences and selection dynamics||||arXiv.org||||2019/11/10||||Multilevel evolutionary developmental optimization (MEDO): A theoretical framework for understanding preferences and selection dynamics||||Safron, Adam||||https://arxiv.org/pdf/1910.13443||||1910.13443||||What is motivation and how does it work? Where do goals come from and how do they vary within and between species and individuals? Why do we prefer some things over others? MEDO is a theoretical framework for understanding these questions in abstract terms, as well as for generating and evaluating specific hypotheses that seek to explain goal-oriented behavior. MEDO views preferences as selective pressures influencing the likelihood of particular outcomes. With respect to biological organisms, these patterns must compete and cooperate in shaping system evolution. To the extent that shaping processes are themselves altered by experience, this enables feedback relationships where histories of reward and punishment can impact future motivation. In this way, various biases can undergo either amplification or attenuation, resulting in preferences and behavioral orientations of varying degrees of inter-temporal and inter-situational stability. MEDO specifically models all shaping dynamics in terms of natural selection operating on multiple levels--genetic, neural, and cultural--and even considers aspects of development to themselves be evolutionary processes. Thus, MEDO reflects a kind of generalized Darwinism, in that it assumes that natural selection provides a common principle for understanding the emergence of complexity within all dynamical systems in which replication, variation, and selection occur. However, MEDO combines this evolutionary perspective with economic decision theory, which describes both the preferences underlying individual choices, as well as the preferences underlying choices made by engineers in designing optimized systems. In this way, MEDO uses economic decision theory to describe goal-oriented behaviors as well as the interacting evolutionary optimization processes from which they emerge. (Please note: this manuscript was written and finalized in 2012.)||||@arxiv||||2019/10/27||||Multilevel evolutionary developmental optimization (MEDO): A...||||What is motivation and how does it work? Where do goals come from and how do they vary within and between species and individuals? Why do we prefer some things over others? MEDO is a theoretical...||||https://arxiv.org/abs/1910.13443v2||||econ||||
110||||None||||Convex Relaxation Based Locational Marginal Prices||||arXiv.org||||2019/10/23||||Convex Relaxation Based Locational Marginal Prices||||Winnicki, Anna || Ndrio, Mariola || Bose, Subhonmesh||||https://arxiv.org/pdf/1910.10673||||1910.10673||||We propose and analyze semidefinite relaxation based locational marginal prices (RLMPs) for real and reactive power in electricity markets. Our analysis reveals that when the non-convex economic dispatch problem has zero duality gap, the RLMPs exhibit properties similar to locational marginal prices with linearized power flow equations. Otherwise, they behave similar to convex hull prices. Restricted to radial distribution networks, RLMPs reduce to second-order cone relaxation based distribution locational marginal prices. We illustrate our theoretical results on numerical examples.||||@arxiv||||2019/10/23||||Convex Relaxation Based Locational Marginal Prices||||We propose and analyze semidefinite relaxation based locational marginal prices (RLMPs) for real and reactive power in electricity markets. Our analysis reveals that when the non-convex economic...||||https://arxiv.org/abs/1910.10673v1||||cs||||
111||||None||||Transformation Models in High-Dimensions||||arXiv.org||||2017/12/20||||Transformation Models in High-Dimensions||||Klaassen, Sven || Kueck, Jannis || Spindler, Martin||||https://arxiv.org/pdf/1712.07364||||1712.07364||||Transformation models are a very important tool for applied statisticians and econometricians. In many applications, the dependent variable is transformed so that homogeneity or normal distribution of the error holds. In this paper, we analyze transformation models in a high-dimensional setting, where the set of potential covariates is large. We propose an estimator for the transformation parameter and we show that it is asymptotically normally distributed using an orthogonalized moment condition where the nuisance functions depend on the target parameter. In a simulation study, we show that the proposed estimator works well in small samples. A common practice in labor economics is to transform wage with the log-function. In this study, we test if this transformation holds in CPS data from the United States.||||@arxiv||||2017/12/20||||Transformation Models in High-Dimensions||||Transformation models are a very important tool for applied statisticians and econometricians. In many applications, the dependent variable is transformed so that homogeneity or normal...||||https://arxiv.org/abs/1712.07364v1||||econ||||
112||||None||||Optimal mechanism for the sale of a durable good||||arXiv.org||||2020/01/21||||Optimal mechanism for the sale of a durable good||||Doval, Laura || Skreta, Vasiliki||||https://arxiv.org/pdf/1904.07456||||1904.07456||||We show that posted prices are the optimal mechanism to sell a durable good to a privately informed buyer when the seller has limited commitment in an infinite horizon setting. We provide a methodology for mechanism design with limited commitment and transferable utility. Whereas in the case of commitment, subject to the buyer's truthtelling and participation constraints, the seller's problem is a decision problem, in the case of limited commitment, the seller's problem corresponds to an intrapersonal game, where different "incarnations" of the seller represent the different beliefs he may have about the buyer's valuation.||||@arxiv||||2019/04/16||||Optimal mechanism for the sale of a durable good||||We show that posted prices are the optimal mechanism to sell a durable good to a privately informed buyer when the seller has limited commitment in an infinite horizon setting. We provide a...||||https://arxiv.org/abs/1904.07456v4||||econ||||
113||||None||||Analysis of Regression Discontinuity Designs with Multiple Cutoffs or Multiple Scores||||arXiv.org||||2019/12/16||||Analysis of Regression Discontinuity Designs with Multiple Cutoffs or Multiple Scores||||Cattaneo, Matias D. || Titiunik, Rocio || Vazquez-Bare, Gonzalo||||https://arxiv.org/pdf/1912.07346||||1912.07346||||We introduce the \texttt{Stata} (and \texttt{R}) package \texttt{rdmulti}, which includes three commands (\texttt{rdmc}, \texttt{rdmcplot}, \texttt{rdms}) for analyzing Regression Discontinuity (RD) designs with multiple cutoffs or multiple scores. The command \texttt{rdmc} applies to non-cummulative and cummulative multi-cutoff RD settings. It calculates pooled and cutoff-specific RD treatment effects, and provides robust bias-corrected inference procedures. Post estimation and inference is allowed. The command \texttt{rdmcplot} offers RD plots for multi-cutoff settings. Finally, the command \texttt{rdms} concerns multi-score settings, covering in particular cumulative cutoffs and two running variables contexts. It also calculates pooled and cutoff-specific RD treatment effects, provides robust bias-corrected inference procedures, and allows for post-estimation estimation and inference. These commands employ the \texttt{Stata} (and \texttt{R}) package \texttt{rdrobust} for plotting, estimation, and inference. Companion \texttt{R} functions with the same syntax and capabilities are provided.||||@arxiv||||2019/12/16||||Analysis of Regression Discontinuity Designs with Multiple Cutoffs...||||We introduce the \texttt{Stata} (and \texttt{R}) package \texttt{rdmulti}, which includes three commands (\texttt{rdmc}, \texttt{rdmcplot}, \texttt{rdms}) for analyzing Regression Discontinuity...||||https://arxiv.org/abs/1912.07346v1||||econ||||
114||||None||||Perfect bidder collusion through bribe and request||||arXiv.org||||2019/12/08||||Perfect bidder collusion through bribe and request||||Lu, Jingfeng || Lu, Zongwei || Riis, Christian||||https://arxiv.org/pdf/1912.03607||||1912.03607||||We study collusion in a second price auction with two bidders in a dynamic environment. One bidder can make a take-it-or-leave-it collusion proposal, which consists of both an offer and a request of bribes, to the opponent. We show there always exists a robust equilibrium in which the collusion success probability is one. In the equilibrium, the interim expected payoff of the collusion initiator Pareto dominates the counterpart in any robust equilibria of the single-option model (Esö and Schummer (2004)) and any other separating equilibria in our model.||||@arxiv||||2019/12/08||||Perfect bidder collusion through bribe and request||||We study collusion in a second price auction with two bidders in a dynamic environment. One bidder can make a take-it-or-leave-it collusion proposal, which consists of both an offer and a request...||||https://arxiv.org/abs/1912.03607v1||||econ||||
115||||None||||How the network properties of shareholders vary with investor type and country||||arXiv.org||||2019/09/26||||How the network properties of shareholders vary with investor type and country||||Yao, Qing || Evans, Tim || Christensen, Kim||||https://arxiv.org/pdf/1812.06694||||1812.06694||||We construct two examples of shareholder networks in which shareholders are connected if they have shares in the same company. We do this for the shareholders in Turkish companies and we compare this against the network formed from the shareholdings in Dutch companies. We analyse the properties of these two networks in terms of the different types of shareholder. We create a suitable randomised version of these networks to enable us to find significant features in our networks. For that we find the roles played by different types of shareholder in these networks, and also show how these roles differ in the two countries we study.||||@arxiv||||2018/12/17||||How the network properties of shareholders vary with investor type...||||We construct two examples of shareholder networks in which shareholders are connected if they have shares in the same company. We do this for the shareholders in Turkish companies and we compare...||||https://arxiv.org/abs/1812.06694v2||||econ||||
116||||None||||An Analysis Framework for Metric Voting based on LP Duality||||arXiv.org||||2019/12/14||||An Analysis Framework for Metric Voting based on LP Duality||||Kempe, David||||https://arxiv.org/pdf/1911.07162||||1911.07162||||Distortion-based analysis has established itself as a fruitful framework for comparing voting mechanisms. m voters and n candidates are jointly embedded in an (unknown) metric space, and the voters submit rankings of candidates by non-decreasing distance from themselves. Based on the submitted rankings, the social choice rule chooses a winning candidate; the quality of the winner is the sum of the (unknown) distances to the voters. The rule's choice will in general be suboptimal, and the worst-case ratio between the cost of its chosen candidate and the optimal candidate is called the rule's distortion. It was shown in prior work that every deterministic rule has distortion at least 3, while the Copeland rule and related rules guarantee worst-case distortion at most 5; a very recent result gave a rule with distortion $2+\sqrt{5} \approx 4.236$.   We provide a framework based on LP-duality and flow interpretations of the dual which provides a simpler and more unified way for proving upper bounds on the distortion of social choice rules. We illustrate the utility of this approach with three examples. First, we give a fairly simple proof of a strong generalization of the upper bound of 5 on the distortion of Copeland, to social choice rules with short paths from the winning candidate to the optimal candidate in generalized weak preference graphs. A special case of this result recovers the recent $2+\sqrt{5}$ guarantee. Second, using this generalized bound, we show that the Ranked Pairs and Schulze rules have distortion $Θ(\sqrt(n))$. Finally, our framework naturally suggests a combinatorial rule that is a strong candidate for achieving distortion 3, which had also been proposed in recent work. We prove that the distortion bound of 3 would follow from any of three combinatorial conjectures we formulate.||||@arxiv||||2019/11/17||||An Analysis Framework for Metric Voting based on LP Duality||||Distortion-based analysis has established itself as a fruitful framework for comparing voting mechanisms. m voters and n candidates are jointly embedded in an (unknown) metric space, and the...||||https://arxiv.org/abs/1911.07162v3||||cs||||
117||||None||||Confidence Collapse in a Multi-Household, Self-Reflexive DSGE Model||||arXiv.org||||2019/07/17||||Confidence Collapse in a Multi-Household, Self-Reflexive DSGE Model||||Morelli, Federico Guglielmo || Benzaquen, Michael || Tarzia, Marco || Bouchaud, Jean-Philippe||||https://arxiv.org/pdf/1907.07425||||1907.07425||||We investigate a multi-household DSGE model in which past aggregate consumption impacts the confidence, and therefore consumption propensity, of individual households. We find that such a minimal setup is extremely rich, and leads to a variety of realistic output dynamics: high output with no crises; high output with increased volatility and deep, short lived recessions; alternation of high and low output states where relatively mild drop in economic conditions can lead to a temporary confidence collapse and steep decline in economic activity. The crisis probability depends exponentially on the parameters of the model, which means that markets cannot efficiently price the associated risk premium. We conclude by stressing that within our framework, {\it narratives} become an important monetary policy tool, that can help steering the economy back on track.||||@arxiv||||2019/07/17||||Confidence Collapse in a Multi-Household, Self-Reflexive DSGE Model||||We investigate a multi-household DSGE model in which past aggregate consumption impacts the confidence, and therefore consumption propensity, of individual households. We find that such a minimal...||||https://arxiv.org/abs/1907.07425v1||||econ||||
118||||None||||Hybrid threats as an exogenous economic shock||||arXiv.org||||2019/12/17||||Hybrid threats as an exogenous economic shock||||Nozharov, Shteryo||||https://arxiv.org/pdf/1912.08916||||1912.08916||||The aim of this study is to contribute to the theory of exogenous economic shocks and their equivalents in an attempt to explain business cycle fluctuations, which still do not have a clear explanation. To this end the author has developed an econometric model based on a regression analysis. Another objective is to tackle the issue of hybrid threats, which have not yet been subjected to a cross-disciplinary research. These were reviewed in terms of their economic characteristics in order to complement research in the fields of defence and security.||||@arxiv||||2019/12/17||||Hybrid threats as an exogenous economic shock||||The aim of this study is to contribute to the theory of exogenous economic shocks and their equivalents in an attempt to explain business cycle fluctuations, which still do not have a clear...||||https://arxiv.org/abs/1912.08916v1||||econ||||
119||||None||||Constrained Pseudo-market Equilibrium||||arXiv.org||||2019/09/12||||Constrained Pseudo-market Equilibrium||||Echenique, Federico || Miralles, Antonio || Zhang, Jun||||https://arxiv.org/pdf/1909.05986||||1909.05986||||We propose a market solution to the problem of resource allocation subject to quantitative constraints, such as those imposed by considerations of diversity or geographical distribution. Constraints are "priced," and agents are charged to the extent that their purchases affect the value (at equilibrium prices) of the relevant constraints. The result is a constrained-efficient market equilibrium outcome. The outcome is fair whenever the constraints do not single out individual agents, which happens, for example with geographical distribution constraints. In economies with endowments, moreover, our equilibrium outcomes are constrained efficient and approximately individually rational.||||@arxiv||||2019/09/12||||Constrained Pseudo-market Equilibrium||||We propose a market solution to the problem of resource allocation subject to quantitative constraints, such as those imposed by considerations of diversity or geographical distribution....||||https://arxiv.org/abs/1909.05986v1||||econ||||
120||||None||||The transmission of uncertainty shocks on income inequality: State-level evidence from the United States||||arXiv.org||||2018/06/21||||The transmission of uncertainty shocks on income inequality: State-level evidence from the United States||||Fischer, Manfred M. || Huber, Florian || Pfarrhofer, Michael||||https://arxiv.org/pdf/1806.08278||||1806.08278||||In this paper, we explore the relationship between state-level household income inequality and macroeconomic uncertainty in the United States. Using a novel large-scale macroeconometric model, we shed light on regional disparities of inequality responses to a national uncertainty shock. The results suggest that income inequality decreases in most states, with a pronounced degree of heterogeneity in terms of shapes and magnitudes of the dynamic responses. By contrast, some few states, mostly located in the West and South census region, display increasing levels of income inequality over time. We find that this directional pattern in responses is mainly driven by the income composition and labor market fundamentals. In addition, forecast error variance decompositions allow for a quantitative assessment of the importance of uncertainty shocks in explaining income inequality. The findings highlight that volatility shocks account for a considerable fraction of forecast error variance for most states considered. Finally, a regression-based analysis sheds light on the driving forces behind differences in state-specific inequality responses.||||@arxiv||||2018/06/21||||The transmission of uncertainty shocks on income inequality:...||||In this paper, we explore the relationship between state-level household income inequality and macroeconomic uncertainty in the United States. Using a novel large-scale macroeconometric model, we...||||https://arxiv.org/abs/1806.08278v1||||econ||||
121||||None||||Censored Quantile Instrumental Variable Estimation with Stata||||arXiv.org||||2019/09/24||||Censored Quantile Instrumental Variable Estimation with Stata||||Chernozhukov, Victor || Fernández-Val, Iván || Han, Sukjin || Kowalski, Amanda||||https://arxiv.org/pdf/1801.05305||||1801.05305||||Many applications involve a censored dependent variable and an endogenous independent variable. Chernozhukov et al. (2015) introduced a censored quantile instrumental variable estimator (CQIV) for use in those applications, which has been applied by Kowalski (2016), among others. In this article, we introduce a Stata command, cqiv, that simplifes application of the CQIV estimator in Stata. We summarize the CQIV estimator and algorithm, we describe the use of the cqiv command, and we provide empirical examples.||||@arxiv||||2018/01/13||||Censored Quantile Instrumental Variable Estimation with Stata||||Many applications involve a censored dependent variable and an endogenous independent variable. Chernozhukov et al. (2015) introduced a censored quantile instrumental variable estimator (CQIV) for...||||https://arxiv.org/abs/1801.05305v3||||econ||||
122||||None||||A Random Attention Model||||arXiv.org||||2019/08/29||||A Random Attention Model||||Cattaneo, Matias D. || Ma, Xinwei || Masatlioglu, Yusufcan || Suleymanov, Elchin||||https://arxiv.org/pdf/1712.03448||||1712.03448||||This paper illustrates how one can deduce preference from observed choices when attention is not only limited but also random. In contrast to earlier approaches, we introduce a Random Attention Model (RAM) where we abstain from any particular attention formation, and instead consider a large class of nonparametric random attention rules. Our model imposes one intuitive condition, termed Monotonic Attention, which captures the idea that each consideration set competes for the decision-maker's attention. We then develop revealed preference theory within RAM and obtain precise testable implications for observable choice probabilities. Based on these theoretical findings, we propose econometric methods for identification, estimation, and inference of the decision maker's preferences. To illustrate the applicability of our results and their concrete empirical content in specific settings, we also develop revealed preference theory and accompanying econometric methods under additional nonparametric assumptions on the consideration set for binary choice problems. Finally, we provide general purpose software implementation of our estimation and inference results, and showcase their performance using simulations.||||@arxiv||||2017/12/09||||A Random Attention Model||||This paper illustrates how one can deduce preference from observed choices when attention is not only limited but also random. In contrast to earlier approaches, we introduce a Random Attention...||||https://arxiv.org/abs/1712.03448v3||||econ||||
123||||None||||Inference for Impulse Responses under Model Uncertainty||||arXiv.org||||2019/10/07||||Inference for Impulse Responses under Model Uncertainty||||Lieb, Lenard || Smeekes, Stephan||||https://arxiv.org/pdf/1709.09583||||1709.09583||||In many macroeconomic applications, confidence intervals for impulse responses are constructed by estimating VAR models in levels - ignoring cointegration rank uncertainty. We investigate the consequences of ignoring this uncertainty. We adapt several methods for handling model uncertainty and highlight their shortcomings. We propose a new method - Weighted-Inference-by-Model-Plausibility (WIMP) - that takes rank uncertainty into account in a data-driven way. In simulations the WIMP outperforms all other methods considered, delivering intervals that are robust to rank uncertainty, yet not overly conservative. We also study potential ramifications of rank uncertainty on applied macroeconomic analysis by re-assessing the effects of fiscal policy shocks.||||@arxiv||||2017/09/27||||Inference for Impulse Responses under Model Uncertainty||||In many macroeconomic applications, confidence intervals for impulse responses are constructed by estimating VAR models in levels - ignoring cointegration rank uncertainty. We investigate the...||||https://arxiv.org/abs/1709.09583v3||||econ||||
124||||None||||Partial Identification and inference in nonparametric one-to-one matching models||||arXiv.org||||2019/10/05||||Partial Identification and inference in nonparametric one-to-one matching models||||Gualdani, Cristina || Sinha, Shruti||||https://arxiv.org/pdf/1902.05610||||1902.05610||||When the analyst has data on one large market, we study partial identification of the preference parameters in models of one-to-one matching with transfers without imposing parametric distributional restrictions on the agents' unobserved characteristics. We provide a tractable characterisation of the sharp identified set and discuss inference, under various classes of nonparametric distributional assumptions on the agents' unobserved characteristics. We use our methodology to test if the variations in marriage matching patterns observed over time in the U.S. are caused by changes in the agents' preferences for education assortativeness or by a shift in the proportion of educated women.||||@arxiv||||2019/02/14||||Partial Identification and inference in nonparametric one-to-one...||||When the analyst has data on one large market, we study partial identification of the preference parameters in models of one-to-one matching with transfers without imposing parametric...||||https://arxiv.org/abs/1902.05610v3||||econ||||
125||||None||||Subgeometrically ergodic autoregressions||||arXiv.org||||2019/04/16||||Subgeometrically ergodic autoregressions||||Meitz, Mika || Saikkonen, Pentti||||https://arxiv.org/pdf/1904.07089||||1904.07089||||In this paper we discuss how the notion of subgeometric ergodicity in Markov chain theory can be exploited to study the stability of nonlinear time series models. Subgeometric ergodicity means that the transition probability measures converge to the stationary measure at a rate slower than geometric. Specifically, we consider higher-order nonlinear autoregressions that may exhibit rather arbitrary behavior for moderate values of the observed series and that behave in a near unit root manner for large values of the observed series. Generalizing existing first-order results, we show that these autoregressions are, under appropriate conditions, subgeometrically ergodic. As useful implications we also obtain stationarity and $β$-mixing with subgeometrically decaying mixing coefficients.||||@arxiv||||2019/04/15||||Subgeometrically ergodic autoregressions||||In this paper we discuss how the notion of subgeometric ergodicity in Markov chain theory can be exploited to study the stability of nonlinear time series models. Subgeometric ergodicity means...||||https://arxiv.org/abs/1904.07089v2||||econ||||
126||||None||||An Online Algorithm for Learning Buyer Behavior under Realistic Pricing Restrictions||||arXiv.org||||2018/03/06||||An Online Algorithm for Learning Buyer Behavior under Realistic Pricing Restrictions||||Saharoy, Debjyoti || Tulabandhula, Theja||||https://arxiv.org/pdf/1803.01968||||1803.01968||||We propose a new efficient online algorithm to learn the parameters governing the purchasing behavior of a utility maximizing buyer, who responds to prices, in a repeated interaction setting. The key feature of our algorithm is that it can learn even non-linear buyer utility while working with arbitrary price constraints that the seller may impose. This overcomes a major shortcoming of previous approaches, which use unrealistic prices to learn these parameters making them unsuitable in practice.||||@arxiv||||2018/03/06||||An Online Algorithm for Learning Buyer Behavior under Realistic...||||We propose a new efficient online algorithm to learn the parameters governing the purchasing behavior of a utility maximizing buyer, who responds to prices, in a repeated interaction setting. The...||||https://arxiv.org/abs/1803.01968v1||||cs||||
127||||None||||The artefact of the Natural Resources Curse||||arXiv.org||||2019/11/21||||The artefact of the Natural Resources Curse||||Mapon, Matata Ponyo || Tsasa, Jean-Paul K.||||https://arxiv.org/pdf/1911.09681||||1911.09681||||This paper reexamines the validity of the natural resource curse hypothesis, using the database of mineral exporting countries. Our findings are as follows: (i) Resource-rich countries (RRCs) do not necessarily exhibit poor political, economic and social performance; (ii) RRCs that perform poorly have a low diversified exports portfolio; (iii) In contrast, RRCs with a low diversified exports portfolio do not necessarily perform poorly. Then, we develop a model of strategic interaction from a Bayesian game setup to study the role of leadership and governance in the management of natural resources. We show that an improvement in the leadership-governance binomial helps to discipline the behavior of lobby groups (theorem 1) and generate a Pareto improvement in the management of natural resources (theorem 2). Evidence from the World Bank Group's CPIA data confirms the later finding. Our results remain valid after some robustness checks.||||@arxiv||||2019/11/21||||The artefact of the Natural Resources Curse||||This paper reexamines the validity of the natural resource curse hypothesis, using the database of mineral exporting countries. Our findings are as follows: (i) Resource-rich countries (RRCs) do...||||https://arxiv.org/abs/1911.09681v1||||econ||||
128||||None||||Dealing with cross-country heterogeneity in panel VARs using finite mixture models||||arXiv.org||||2019/03/12||||Dealing with cross-country heterogeneity in panel VARs using finite mixture models||||Huber, Florian || Pfarrhofer, Michael||||https://arxiv.org/pdf/1804.01554||||1804.01554||||In this paper, we provide a parsimonious means to estimate panel VARs with stochastic volatility. We assume that coefficients associated with domestic lagged endogenous variables arise from a Gaussian mixture model. Shrinkage on the cluster size is introduced through suitable priors on the component weights and cluster-relevant quantities are identified through novel shrinkage priors. To assess whether dynamic interdependencies between economies are needed, we moreover impose shrinkage priors on the coefficients related to other countries' endogenous variables. Finally, our model controls for static interdependencies by assuming that the reduced form shocks of the model feature a factor stochastic volatility structure. We assess the merits of the proposed approach by using synthetic data as well as a real data application. In the empirical application, we forecast Eurozone unemployment rates and show that our proposed approach works well in terms of predictions.||||@arxiv||||2018/04/04||||Dealing with cross-country heterogeneity in panel VARs using...||||In this paper, we provide a parsimonious means to estimate panel VARs with stochastic volatility. We assume that coefficients associated with domestic lagged endogenous variables arise from a...||||https://arxiv.org/abs/1804.01554v2||||econ||||
129||||None||||Credit Cycles, Securitization, and Credit Default Swaps||||arXiv.org||||2019/01/01||||Credit Cycles, Securitization, and Credit Default Swaps||||Peña, Juan Ignacio||||https://arxiv.org/pdf/1901.00177||||1901.00177||||We present a limits-to-arbitrage model to study the impact of securitization, leverage and credit risk protection on the cyclicity of bank credit. In a stable bank credit situation, no cycles of credit expansion or contraction appear. Unlevered securitization together with mis-pricing of securitized assets increases lending cyclicality, favoring credit booms and busts. Leverage changes the state of affairs with respect to the simple securitization. First, the volume of real activity and banking profits increases. Second, banks sell securities when markets decline. This selling puts further pressure on falling prices. The mis-pricing of credit risk protection or securitized assets influences the real economy. Trading in these contracts reduces the amount of funding available to entrepreneurs, particularly to high-credit-risk borrowers. This trading decreases the liquidity of the securitized assets, and especially those based on investments with high credit risk.||||@arxiv||||2019/01/01||||Credit Cycles, Securitization, and Credit Default Swaps||||We present a limits-to-arbitrage model to study the impact of securitization, leverage and credit risk protection on the cyclicity of bank credit. In a stable bank credit situation, no cycles of...||||https://arxiv.org/abs/1901.00177v1||||econ||||
130||||None||||Strategically Simple Mechanisms||||arXiv.org||||2018/12/03||||Strategically Simple Mechanisms||||Borgers, Tilman || Li, Jiangtao||||https://arxiv.org/pdf/1812.00849||||1812.00849||||We define and investigate a property of mechanisms that we call "strategic simplicity," and that is meant to capture the idea that, in strategically simple mechanisms, strategic choices require limited strategic sophistication. We define a mechanism to be strategically simple if choices can be based on first-order beliefs about the other agents' preferences and first-order certainty about the other agents' rationality alone, and there is no need for agents to form higher-order beliefs, because such beliefs are irrelevant to the optimal strategies. All dominant strategy mechanisms are strategically simple. But many more mechanisms are strategically simple. In particular, strategically simple mechanisms may be more flexible than dominant strategy mechanisms in the bilateral trade problem and the voting problem.||||@arxiv||||2018/12/03||||Strategically Simple Mechanisms||||We define and investigate a property of mechanisms that we call "strategic simplicity," and that is meant to capture the idea that, in strategically simple mechanisms, strategic choices require...||||https://arxiv.org/abs/1812.00849v1||||econ||||
131||||None||||Microfoundations of Discounting||||arXiv.org||||2020/01/08||||Microfoundations of Discounting||||Adamou, Alexander T. I. || Berman, Yonatan || Mavroyiannis, Diomides P. || Peters, Ole B.||||https://arxiv.org/pdf/1910.02137||||1910.02137||||An important question in economics is how people choose between different payments in the future. The classical normative model predicts that a decision maker discounts a later payment relative to an earlier one by an exponential function of the time between them. Descriptive models use non-exponential functions to fit observed behavioral phenomena, such as preference reversal. Here we propose a model of discounting, consistent with standard axioms of choice, in which decision makers maximize the growth rate of their wealth. Four specifications of the model produce four forms of discounting -- no discounting, exponential, hyperbolic, and a hybrid of exponential and hyperbolic -- two of which predict preference reversal. Our model requires no assumption of behavioral bias or payment risk.||||@arxiv||||2019/10/04||||Microfoundations of Discounting||||An important question in economics is how people choose between different payments in the future. The classical normative model predicts that a decision maker discounts a later payment relative to...||||https://arxiv.org/abs/1910.02137v3||||econ||||
132||||None||||Computing a Data Dividend||||arXiv.org||||2019/06/27||||Computing a Data Dividend||||Bax, Eric||||https://arxiv.org/pdf/1905.01805||||1905.01805||||Quality data is a fundamental contributor to success in statistics and machine learning. If a statistical assessment or machine learning leads to decisions that create value, data contributors may want a share of that value. This paper presents methods to assess the value of individual data samples, and of sets of samples, to apportion value among different data contributors. We use Shapley values for individual samples and Owen values for combined samples, and show that these values can be computed in polynomial time in spite of their definitions having numbers of terms that are exponential in the number of samples.||||@arxiv||||2019/05/06||||Computing a Data Dividend||||Quality data is a fundamental contributor to success in statistics and machine learning. If a statistical assessment or machine learning leads to decisions that create value, data contributors may...||||https://arxiv.org/abs/1905.01805v2||||cs||||
133||||None||||Identifying the occurrence or non occurrence of cognitive bias in situations resembling the Monty Hall problem||||arXiv.org||||2018/02/25||||Identifying the occurrence or non occurrence of cognitive bias in situations resembling the Monty Hall problem||||Borhani, Fatemeh || Green, Edward J.||||https://arxiv.org/pdf/1802.08935||||1802.08935||||People reason heuristically in situations resembling inferential puzzles such as Bertrand's box paradox and the Monty Hall problem. The practical significance of that fact for economic decision making is uncertain because a departure from sound reasoning may, but does not necessarily, result in a "cognitively biased" outcome different from what sound reasoning would have produced. Criteria are derived here, applicable to both experimental and non-experimental situations, for heuristic reasoning in an inferential-puzzle situations to result, or not to result, in cognitively bias. In some situations, neither of these criteria is satisfied, and whether or not agents' posterior probability assessments or choices are cognitively biased cannot be determined.||||@arxiv||||2018/02/25||||Identifying the occurrence or non occurrence of cognitive bias in...||||People reason heuristically in situations resembling inferential puzzles such as Bertrand's box paradox and the Monty Hall problem. The practical significance of that fact for economic decision...||||https://arxiv.org/abs/1802.08935v1||||econ||||
134||||None||||Aggregating Google Trends: Multivariate Testing and Analysis||||arXiv.org||||2018/03/25||||Aggregating Google Trends: Multivariate Testing and Analysis||||France, Stephen L. || Shi, Yuying||||https://arxiv.org/pdf/1712.03152||||1712.03152||||Web search data are a valuable source of business and economic information. Previous studies have utilized Google Trends web search data for economic forecasting. We expand this work by providing algorithms to combine and aggregate search volume data, so that the resulting data is both consistent over time and consistent between data series. We give a brand equity example, where Google Trends is used to analyze shopping data for 100 top ranked brands and these data are used to nowcast economic variables. We describe the importance of out of sample prediction and show how principal component analysis (PCA) can be used to improve the signal to noise ratio and prevent overfitting in nowcasting models. We give a finance example, where exploratory data analysis and classification is used to analyze the relationship between Google Trends searches and stock prices.||||@arxiv||||2017/12/08||||Aggregating Google Trends: Multivariate Testing and Analysis||||Web search data are a valuable source of business and economic information. Previous studies have utilized Google Trends web search data for economic forecasting. We expand this work by providing...||||https://arxiv.org/abs/1712.03152v2||||econ||||
135||||None||||A Contribution to Theory of Factor Income Distribution, Cambridge Capital Controversy and Equity Premium Puzzle||||arXiv.org||||2019/12/07||||A Contribution to Theory of Factor Income Distribution, Cambridge Capital Controversy and Equity Premium Puzzle||||Liu, Xiaofeng||||https://arxiv.org/pdf/1911.12490||||1911.12490||||Under very general conditions, we construct a micro-macro model for closed economy with a large number of heterogeneous agents. By introducing both financial capital (i.e. valued capital---- equities of firms) and physical capital (i.e. capital goods), our framework gives a logically consistent, complete factor income distribution theory with micro-foundation. The model shows factor incomes obey different distribution rules at the micro and macro levels, while marginal distribution theory and no-arbitrage princi-ple are unified into a common framework. Our efforts solve the main problems of Cambridge capital controversy, and reasonably explain the equity premium puzzle. Strong empirical evidences support our results.||||@arxiv||||2019/11/28||||A Contribution to Theory of Factor Income Distribution, Cambridge...||||Under very general conditions, we construct a micro-macro model for closed economy with a large number of heterogeneous agents. By introducing both financial capital (i.e. valued capital----...||||https://arxiv.org/abs/1911.12490v2||||econ||||
136||||None||||Coordination Event Detection and Initiator Identification in Time Series Data||||arXiv.org||||2019/11/23||||Coordination Event Detection and Initiator Identification in Time Series Data||||Amornbunchornvej, Chainarong || Brugere, Ivan || Strandburg-Peshkin, Ariana || Farine, Damien || Crofoot, Margaret C. || Berger-Wolf, Tanya Y.||||https://arxiv.org/pdf/1603.01570||||1603.01570||||Behavior initiation is a form of leadership and is an important aspect of social organization that affects the processes of group formation, dynamics, and decision-making in human societies and other social animal species. In this work, we formalize the "Coordination Initiator Inference Problem" and propose a simple yet powerful framework for extracting periods of coordinated activity and determining individuals who initiated this coordination, based solely on the activity of individuals within a group during those periods. The proposed approach, given arbitrary individual time series, automatically (1) identifies times of coordinated group activity, (2) determines the identities of initiators of those activities, and (3) classifies the likely mechanism by which the group coordination occurred, all of which are novel computational tasks. We demonstrate our framework on both simulated and real-world data: trajectories tracking of animals as well as stock market data. Our method is competitive with existing global leadership inference methods but provides the first approaches for local leadership and coordination mechanism classification. Our results are consistent with ground-truthed biological data and the framework finds many known events in financial data which are not otherwise reflected in the aggregate NASDAQ index. Our method is easily generalizable to any coordinated time-series data from interacting entities.||||@arxiv||||2016/03/04||||Coordination Event Detection and Initiator Identification in Time...||||Behavior initiation is a form of leadership and is an important aspect of social organization that affects the processes of group formation, dynamics, and decision-making in human societies and...||||https://arxiv.org/abs/1603.01570v2||||cs||||
137||||None||||Valid Post-Selection and Post-Regularization Inference: An Elementary, General Approach||||arXiv.org||||2015/08/18||||Valid Post-Selection and Post-Regularization Inference: An Elementary, General Approach||||Chernozhukov, Victor || Hansen, Christian || Spindler, Martin||||https://arxiv.org/pdf/1501.03430||||1501.03430||||Here we present an expository, general analysis of valid post-selection or post-regularization inference about a low-dimensional target parameter, $α$, in the presence of a very high-dimensional nuisance parameter, $η$, which is estimated using modern selection or regularization methods. Our analysis relies on high-level, easy-to-interpret conditions that allow one to clearly see the structures needed for achieving valid post-regularization inference. Simple, readily verifiable sufficient conditions are provided for a class of affine-quadratic models. We focus our discussion on estimation and inference procedures based on using the empirical analog of theoretical equations $$M(α, η)=0$$ which identify $α$. Within this structure, we show that setting up such equations in a manner such that the orthogonality/immunization condition $$\partial_ηM(α, η) = 0$$ at the true parameter values is satisfied, coupled with plausible conditions on the smoothness of $M$ and the quality of the estimator $\hat η$, guarantees that inference on for the main parameter $α$ based on testing or point estimation methods discussed below will be regular despite selection or regularization biases occurring in estimation of $η$. In particular, the estimator of $α$ will often be uniformly consistent at the root-$n$ rate and uniformly asymptotically normal even though estimators $\hat η$ will generally not be asymptotically linear and regular. The uniformity holds over large classes of models that do not impose highly implausible "beta-min" conditions. We also show that inference can be carried out by inverting tests formed from Neyman's $C(α)$ (orthogonal score) statistics.||||@arxiv||||2015/01/14||||Valid Post-Selection and Post-Regularization Inference: An...||||Here we present an expository, general analysis of valid post-selection or post-regularization inference about a low-dimensional target parameter, $α$, in the presence of a very...||||https://arxiv.org/abs/1501.03430v3||||econ||||
138||||None||||Macroeconomics and FinTech: Uncovering Latent Macroeconomic Effects on Peer-to-Peer Lending||||arXiv.org||||2017/10/31||||Macroeconomics and FinTech: Uncovering Latent Macroeconomic Effects on Peer-to-Peer Lending||||Foo, Jessica || Lim, Lek-Heng || Wong, Ken Sze-Wai||||https://arxiv.org/pdf/1710.11283||||1710.11283||||Peer-to-peer (P2P) lending is a fast growing financial technology (FinTech) trend that is displacing traditional retail banking. Studies on P2P lending have focused on predicting individual interest rates or default probabilities. However, the relationship between aggregated P2P interest rates and the general economy will be of interest to investors and borrowers as the P2P credit market matures. We show that the variation in P2P interest rates across grade types are determined by three macroeconomic latent factors formed by Canonical Correlation Analysis (CCA) - macro default, investor uncertainty, and the fundamental value of the market. However, the variation in P2P interest rates across term types cannot be explained by the general economy.||||@arxiv||||2017/10/31||||Macroeconomics and FinTech: Uncovering Latent Macroeconomic...||||Peer-to-peer (P2P) lending is a fast growing financial technology (FinTech) trend that is displacing traditional retail banking. Studies on P2P lending have focused on predicting individual...||||https://arxiv.org/abs/1710.11283v1||||econ||||
139||||None||||Confidentiality and linked data||||arXiv.org||||2019/07/15||||Confidentiality and linked data||||Ritchie, Felix || Smith, Jim||||https://arxiv.org/pdf/1907.06465||||1907.06465||||Data providers such as government statistical agencies perform a balancing act: maximising information published to inform decision-making and research, while simultaneously protecting privacy. The emergence of identified administrative datasets with the potential for sharing (and thus linking) offers huge potential benefits but significant additional risks. This article introduces the principles and methods of linking data across different sources and points in time, focusing on potential areas of risk. We then consider confidentiality risk, focusing in particular on the "intruder" problem central to the area, and looking at both risks from data producer outputs and from the release of micro-data for further analysis. Finally, we briefly consider potential solutions to micro-data release, both the statistical solutions considered in other contributed articles and non-statistical solutions.||||@arxiv||||2019/07/15||||Confidentiality and linked data||||Data providers such as government statistical agencies perform a balancing act: maximising information published to inform decision-making and research, while simultaneously protecting privacy....||||https://arxiv.org/abs/1907.06465v1||||cs||||
140||||None||||An Interacting Agent Model of Economic Crisis||||arXiv.org||||2020/01/30||||An Interacting Agent Model of Economic Crisis||||Ikeda, Yuichi||||https://arxiv.org/pdf/2001.11843||||2001.11843||||Most national economies are linked by international trade. Consequently, economic globalization forms a massive and complex economic network with strong links, that is, interactions arising from increasing trade. Various interesting collective motions are expected to emerge from strong economic interactions in a global economy under trade liberalization. Among the various economic collective motions, economic crises are our most intriguing problem. In our previous studies, we have revealed that the Kuramoto's coupled limit-cycle oscillator model and the Ising-like spin model on networks are invaluable tools for characterizing the economic crises. In this study, we develop a mathematical theory to describe an interacting agent model that derives the Kuramoto model and the Ising-like spin model by using appropriate approximations. Our interacting agent model suggests phase synchronization and spin ordering during economic crises. We confirm the emergence of the phase synchronization and spin ordering during economic crises by analyzing various economic time series data. We also develop a network reconstruction model based on entropy maximization that considers the sparsity of the network. Here network reconstruction means estimating a network's adjacency matrix from a node's local information. The interbank network is reconstructed using the developed model, and a comparison is made of the reconstructed network with the actual data. We successfully reproduce the interbank network and the known stylized facts. In addition, the exogenous shock acting on an industry community in a supply chain network and financial sector are estimated. Estimation of exogenous shocks acting on communities of in the real economy in the supply chain network provide evidence of the channels of distress propagating from the financial sector to the real economy through the supply chain network.||||@arxiv||||2020/01/30||||An Interacting Agent Model of Economic Crisis||||Most national economies are linked by international trade. Consequently, economic globalization forms a massive and complex economic network with strong links, that is, interactions arising from...||||https://arxiv.org/abs/2001.11843v1||||econ||||
141||||None||||Fair Division with Bounded Sharing||||arXiv.org||||2019/12/01||||Fair Division with Bounded Sharing||||Segal-Halevi, Erel||||https://arxiv.org/pdf/1912.00459||||1912.00459||||A set of objects is to be divided fairly among agents with different tastes, modeled by additive value functions. If the objects cannot be shared, so that each of them must be entirely allocated to a single agent, then fair division may not exist. How many objects must be shared between two or more agents in order to attain a fair division? The paper studies various notions of fairness, such as proportionality, envy-freeness and equitability. It also studies consensus division, in which each agent assigns the same value to all bundles --- a notion that is useful in truthful fair division mechanisms. It proves upper bounds on the number of required sharings. However, it shows that finding the minimum number of sharings is, in general, NP-hard even for generic instances. Many problems remain open.||||@arxiv||||2019/12/01||||Fair Division with Bounded Sharing||||A set of objects is to be divided fairly among agents with different tastes, modeled by additive value functions. If the objects cannot be shared, so that each of them must be entirely allocated...||||https://arxiv.org/abs/1912.00459v1||||cs||||
142||||None||||Cancer Risk Messages: Public Health and Economic Welfare||||arXiv.org||||2018/07/10||||Cancer Risk Messages: Public Health and Economic Welfare||||Williams, Ruth F. G. || Chan, Ka C. || Lenard, Christopher T. || Mills, Terence M.||||https://arxiv.org/pdf/1807.03045||||1807.03045||||Statements for public health purposes such as "1 in 2 will get cancer by age 85" have appeared in public spaces. The meaning drawn from such statements affects economic welfare, not just public health. Both markets and government use risk information on all kinds of risks, useful information can, in turn, improve economic welfare, however inaccuracy can lower it. We adapt the contingency table approach so that a quoted risk is cross-classified with the states of nature. We show that bureaucratic objective functions regarding the accuracy of a reported cancer risk can then be stated.||||@arxiv||||2018/07/09||||Cancer Risk Messages: Public Health and Economic Welfare||||Statements for public health purposes such as "1 in 2 will get cancer by age 85" have appeared in public spaces. The meaning drawn from such statements affects economic welfare, not just public...||||https://arxiv.org/abs/1807.03045v2||||econ||||
143||||None||||Lasso under Multi-way Clustering: Estimation and Post-selection Inference||||arXiv.org||||2019/08/21||||Lasso under Multi-way Clustering: Estimation and Post-selection Inference||||Chiang, Harold D. || Sasaki, Yuya||||https://arxiv.org/pdf/1905.02107||||1905.02107||||This paper studies high-dimensional regression models with lasso when data is sampled under multi-way clustering. First, we establish convergence rates for the lasso and post-lasso estimators. Second, we propose a novel inference method based on a post-double-selection procedure and show its asymptotic validity. Our procedure can be easily implemented with existing statistical packages. Simulation results demonstrate that the proposed procedure works well in finite sample. We illustrate the proposed method with a couple of empirical applications to development and growth economics.||||@arxiv||||2019/05/06||||Lasso under Multi-way Clustering: Estimation and Post-selection Inference||||This paper studies high-dimensional regression models with lasso when data is sampled under multi-way clustering. First, we establish convergence rates for the lasso and post-lasso estimators....||||https://arxiv.org/abs/1905.02107v3||||econ||||
144||||None||||Plug-in Regularized Estimation of High-Dimensional Parameters in Nonlinear Semiparametric Models||||arXiv.org||||2019/10/20||||Plug-in Regularized Estimation of High-Dimensional Parameters in Nonlinear Semiparametric Models||||Chernozhukov, Victor || Nekipelov, Denis || Semenova, Vira || Syrgkanis, Vasilis||||https://arxiv.org/pdf/1806.04823||||1806.04823||||We propose an l1-regularized M-estimator for a high-dimensional sparse parameter that is identified by a class of semiparametric conditional moment restrictions (CMR). We estimate the nonparametric nuisance parameter by modern machine learning methods. Plugging the first-stage estimate into the CMR, we construct the M-estimator loss function for the target parameter so that its gradient is insensitive (formally, Neyman-orthogonal) with respect to the first-stage regularization bias. As a result, the estimator achieves oracle convergence rate \sqrt{k \log p/n}, where oracle knows the true first stage and solves only a parametric problem. We apply our results to conditional moment models with missing data, games of incomplete information and treatment effects in regression models with non-linear link functions.||||@arxiv||||2018/06/13||||Plug-in Regularized Estimation of High-Dimensional Parameters in...||||We propose an l1-regularized M-estimator for a high-dimensional sparse parameter that is identified by a class of semiparametric conditional moment restrictions (CMR). We estimate the...||||https://arxiv.org/abs/1806.04823v4||||cs||||
145||||None||||Specification Testing in Nonparametric Instrumental Quantile Regression||||arXiv.org||||2019/09/23||||Specification Testing in Nonparametric Instrumental Quantile Regression||||Breunig, Christoph||||https://arxiv.org/pdf/1909.10129||||1909.10129||||There are many environments in econometrics which require nonseparable modeling of a structural disturbance. In a nonseparable model with endogenous regressors, key conditions are validity of instrumental variables and monotonicity of the model in a scalar unobservable variable. Under these conditions the nonseparable model is equivalent to an instrumental quantile regression model. A failure of the key conditions, however, makes instrumental quantile regression potentially inconsistent. This paper develops a methodology for testing the hypothesis whether the instrumental quantile regression model is correctly specified. Our test statistic is asymptotically normally distributed under correct specification and consistent against any alternative model. In addition, test statistics to justify the model simplification are established. Finite sample properties are examined in a Monte Carlo study and an empirical illustration is provided.||||@arxiv||||2019/09/23||||Specification Testing in Nonparametric Instrumental Quantile Regression||||There are many environments in econometrics which require nonseparable modeling of a structural disturbance. In a nonseparable model with endogenous regressors, key conditions are validity of...||||https://arxiv.org/abs/1909.10129v1||||econ||||
146||||None||||Rethinking travel behavior modeling representations through embeddings||||arXiv.org||||2019/08/31||||Rethinking travel behavior modeling representations through embeddings||||Pereira, Francisco C.||||https://arxiv.org/pdf/1909.00154||||1909.00154||||This paper introduces the concept of travel behavior embeddings, a method for re-representing discrete variables that are typically used in travel demand modeling, such as mode, trip purpose, education level, family type or occupation. This re-representation process essentially maps those variables into a latent space called the \emph{embedding space}. The benefit of this is that such spaces allow for richer nuances than the typical transformations used in categorical variables (e.g. dummy encoding, contrasted encoding, principal components analysis). While the usage of latent variable representations is not new per se in travel demand modeling, the idea presented here brings several innovations: it is an entirely data driven algorithm; it is informative and consistent, since the latent space can be visualized and interpreted based on distances between different categories; it preserves interpretability of coefficients, despite being based on Neural Network principles; and it is transferrable, in that embeddings learned from one dataset can be reused for other ones, as long as travel behavior keeps consistent between the datasets.   The idea is strongly inspired on natural language processing techniques, namely the word2vec algorithm. Such algorithm is behind recent developments such as in automatic translation or next word prediction. Our method is demonstrated using a model choice model, and shows improvements of up to 60\% with respect to initial likelihood, and up to 20% with respect to likelihood of the corresponding traditional model (i.e. using dummy variables) in out-of-sample evaluation. We provide a new Python package, called PyTre (PYthon TRavel Embeddings), that others can straightforwardly use to replicate our results or improve their own models. Our experiments are themselves based on an open dataset (swissmetro).||||@arxiv||||2019/08/31||||Rethinking travel behavior modeling representations through embeddings||||This paper introduces the concept of travel behavior embeddings, a method for re-representing discrete variables that are typically used in travel demand modeling, such as mode, trip purpose,...||||https://arxiv.org/abs/1909.00154v1||||cs||||
147||||None||||Linkages and systemic risk in the European insurance sector: Some new evidence based on dynamic spanning trees||||arXiv.org||||2019/08/21||||Linkages and systemic risk in the European insurance sector: Some new evidence based on dynamic spanning trees||||Denkowska, Anna || Wanat, Stanisław||||https://arxiv.org/pdf/1908.01142||||1908.01142||||This paper is part of the research on the interlinkages between insurers and their contribution to systemic risk on the insurance market. Its main purpose is to present the results of the analysis of linkage dynamics and systemic risk in the European insurance sector which are obtained using correlation networks. These networks are based on dynamic dependence structures modelled using a copula. Then, we determine minimum spanning trees (MST). Finally, the linkage dynamics is described by means of selected topological network measures.||||@arxiv||||2019/08/03||||Linkages and systemic risk in the European insurance sector: Some...||||This paper is part of the research on the interlinkages between insurers and their contribution to systemic risk on the insurance market. Its main purpose is to present the results of the analysis...||||https://arxiv.org/abs/1908.01142v2||||econ||||
148||||None||||Estimating Average Treatment Effects: Supplementary Analyses and Remaining Challenges||||arXiv.org||||2017/02/04||||Estimating Average Treatment Effects: Supplementary Analyses and Remaining Challenges||||Athey, Susan || Imbens, Guido || Pham, Thai || Wager, Stefan||||https://arxiv.org/pdf/1702.01250||||1702.01250||||There is a large literature on semiparametric estimation of average treatment effects under unconfounded treatment assignment in settings with a fixed number of covariates. More recently attention has focused on settings with a large number of covariates. In this paper we extend lessons from the earlier literature to this new setting. We propose that in addition to reporting point estimates and standard errors, researchers report results from a number of supplementary analyses to assist in assessing the credibility of their estimates.||||@arxiv||||2017/02/04||||Estimating Average Treatment Effects: Supplementary Analyses and...||||There is a large literature on semiparametric estimation of average treatment effects under unconfounded treatment assignment in settings with a fixed number of covariates. More recently attention...||||https://arxiv.org/abs/1702.01250v1||||econ||||
149||||None||||Most productive scale size of China's regional R&D value chain: A mixed structure network||||arXiv.org||||2019/10/09||||Most productive scale size of China's regional R&D value chain: A mixed structure network||||Assani, Saeed || Jiang, Jianlin || Assani, Ahmad || Yang, Feng||||https://arxiv.org/pdf/1910.03805||||1910.03805||||This paper offers new mathematical models to measure the most productive scale size (MPSS) of production systems with mixed structure networks (mixed of series and parallel). In the first property, we deal with a general multi-stage network which can be transformed, using dummy processes, into a series of parallel networks. In the second property, we consider a direct network combined with series and parallel structure. In this paper, we propose new models to measure the overall MPSS of the production systems and their internal processes. MPSS decomposition is discussed and examined. As a real-life application, this study measures the efficiency and MPSS of research and development (R&D) activities of Chinese provinces within an R&D value chain network. In the R&D value chain, profitability and marketability stages are connected in series, where the profitability stage is composed of operation and R&D efforts connected in parallel. The MPSS network model provides not only the MPSS measurement but also values that indicate the appropriate degree of intermediate measures for the two stages. Improvement strategy is given for each region based on the gap between the current and the appropriate level of intermediate measures. Our findings show that the marketability efficiency values of Chinese R&D regions were low, and no regions are operated under the MPSS. As a result, most Chinese regions performed inefficiently regarding both profitability and marketability. This finding provides initial evidence that the generally lower profitability and marketability efficiency of Chinese regions is a severe problem that may be due to wasted resources on production and R&D.||||@arxiv||||2019/10/09||||Most productive scale size of China's regional R&D value...||||This paper offers new mathematical models to measure the most productive scale size (MPSS) of production systems with mixed structure networks (mixed of series and parallel). In the first...||||https://arxiv.org/abs/1910.03805v1||||econ||||
150||||None||||Nowcasting Recessions using the SVM Machine Learning Algorithm||||arXiv.org||||2019/06/27||||Nowcasting Recessions using the SVM Machine Learning Algorithm||||James, Alexander || Abu-Mostafa, Yaser S. || Qiao, Xiao||||https://arxiv.org/pdf/1903.03202||||1903.03202||||We introduce a novel application of Support Vector Machines (SVM), an important Machine Learning algorithm, to determine the beginning and end of recessions in real time. Nowcasting, "forecasting" a condition about the present time because the full information about it is not available until later, is key for recessions, which are only determined months after the fact. We show that SVM has excellent predictive performance for this task, and we provide implementation details to facilitate its use in similar problems in economics and finance.||||@arxiv||||2019/02/17||||Nowcasting Recessions using the SVM Machine Learning Algorithm||||We introduce a novel application of Support Vector Machines (SVM), an important Machine Learning algorithm, to determine the beginning and end of recessions in real time. Nowcasting, "forecasting"...||||https://arxiv.org/abs/1903.03202v2||||cs||||
151||||None||||Incentivising Participation in Liquid Democracy with Breadth-First Delegation||||arXiv.org||||2019/02/25||||Incentivising Participation in Liquid Democracy with Breadth-First Delegation||||Kotsialou, Grammateia || Riley, Luke||||https://arxiv.org/pdf/1811.03710||||1811.03710||||Liquid democracy allows members of an electorate to either directly vote over alternatives, or delegate their voting rights to someone they trust. Most of the liquid democracy literature and implementations allow each voter to nominate only one delegate per election. However, if that delegate abstains, the voting rights assigned to her are left unused. To minimise the number of unused delegations, it has been suggested that each voter should declare a personal ranking over voters she trusts. In this paper, we show that even if personal rankings over voters are declared, the standard delegation method of liquid democracy remains problematic. More specifically, we show that when personal rankings over voters are declared, it could be undesirable to receive delegated voting rights, which is contrary to what liquid democracy fundamentally relies on. To solve this issue, we propose a new method to delegate voting rights in an election, called breadth-first delegation. Additionally, the proposed method prioritises assigning voting rights to individuals closely connected to the voters who delegate.||||@arxiv||||2018/11/08||||Incentivising Participation in Liquid Democracy with Breadth-First...||||Liquid democracy allows members of an electorate to either directly vote over alternatives, or delegate their voting rights to someone they trust. Most of the liquid democracy literature and...||||https://arxiv.org/abs/1811.03710v2||||cs||||
152||||None||||Deviations from Zipf's law contain more information than Zipf's law itself||||arXiv.org||||2019/11/12||||Deviations from Zipf's law contain more information than Zipf's law itself||||De Marzo, Giordano || Gabrielli, Andrea || Zaccaria, Andrea || Pietronero, Luciano||||https://arxiv.org/pdf/1911.04844||||1911.04844||||Rank size plots of very different systems are usually fitted with Zipf's law, however, one often observes strong deviations at large sizes. We show that these deviations contain essential and general information on the evolution and the intrinsic cutoffs of the system. In particular, if the first ranks show deviations from Zipf's law, the empirical maximum represents the intrinsic upper cutoff of the physical system. Moreover, pure Zipf's law is always present whenever the underlying power-law size distribution is undersampled.||||@arxiv||||2019/11/12||||Deviations from Zipf's law contain more information than...||||Rank size plots of very different systems are usually fitted with Zipf's law, however, one often observes strong deviations at large sizes. We show that these deviations contain essential and...||||https://arxiv.org/abs/1911.04844v1||||econ||||
153||||None||||Methodological provisions for conducting empirical research of the availability and implementation of the consumers socially responsible intentions||||arXiv.org||||2019/01/01||||Methodological provisions for conducting empirical research of the availability and implementation of the consumers socially responsible intentions||||Potrashkova, Lyudmyla || Raiko, Diana || Tseitlin, Leonid || Savchenko, Olga || Nagy, Szabolcs||||https://arxiv.org/pdf/1901.00191||||1901.00191||||Social responsibility of consumers is one of the main conditions for the recoupment of enterprises expenses associated with the implementation of social and ethical marketing tasks. Therefore, the enterprises, which plan to act on terms of social and ethical marketing, should monitor the social responsibility of consumers in the relevant markets. At the same time, special attention should be paid to the analysis of factors that prevent consumers from implementing their socially responsible intentions in the regions with a low level of social activity of consumers. The purpose of the article is to develop methodological guidelines that determine the tasks and directions of conducting empirical studies aimed at assessing the gap between the socially responsible intentions of consumers and the actual implementation of these intentions, as well as to identify the causes of this gap. An empirical survey of the sampled consumers in Kharkiv was carried out in terms of the proposed methodological provisions. It revealed a rather high level of respondents' willingness to support socially responsible enterprises and a rather low level of implementation of these intentions due to the lack of consumers awareness. To test the proposed methodological guidelines, an empirical study of the consumers social responsibility was conducted in 2017 on a sample of students and professors of the Semen Kuznets Kharkiv National University of Economics (120 people). Questioning of the respondents was carried out using the Google Forms. The finding allowed to make conclusion for existence of a high level of respondents' willingness to support socially responsible and socially active enterprises. However, the study also revealed the existence of a significant gap between the intentions and actions of consumers, caused by the lack of awareness.||||@arxiv||||2019/01/01||||Methodological provisions for conducting empirical research of the...||||Social responsibility of consumers is one of the main conditions for the recoupment of enterprises expenses associated with the implementation of social and ethical marketing tasks. Therefore, the...||||https://arxiv.org/abs/1901.00191v1||||econ||||
154||||None||||Non-Asymptotic Inference in Instrumental Variables Estimation||||arXiv.org||||2018/09/10||||Non-Asymptotic Inference in Instrumental Variables Estimation||||Horowitz, Joel L.||||https://arxiv.org/pdf/1809.03600||||1809.03600||||This paper presents a simple method for carrying out inference in a wide variety of possibly nonlinear IV models under weak assumptions. The method is non-asymptotic in the sense that it provides a finite sample bound on the difference between the true and nominal probabilities of rejecting a correct null hypothesis. The method is a non-Studentized version of the Anderson-Rubin test but is motivated and analyzed differently. In contrast to the conventional Anderson-Rubin test, the method proposed here does not require restrictive distributional assumptions, linearity of the estimated model, or simultaneous equations. Nor does it require knowledge of whether the instruments are strong or weak. It does not require testing or estimating the strength of the instruments. The method can be applied to quantile IV models that may be nonlinear and can be used to test a parametric IV model against a nonparametric alternative. The results presented here hold in finite samples, regardless of the strength of the instruments.||||@arxiv||||2018/09/10||||Non-Asymptotic Inference in Instrumental Variables Estimation||||This paper presents a simple method for carrying out inference in a wide variety of possibly nonlinear IV models under weak assumptions. The method is non-asymptotic in the sense that it provides...||||https://arxiv.org/abs/1809.03600v1||||econ||||
155||||None||||Implications of macroeconomic volatility in the Euro area||||arXiv.org||||2018/06/28||||Implications of macroeconomic volatility in the Euro area||||Hauzenberger, Niko || Böck, Maximilian || Pfarrhofer, Michael || Stelzer, Anna || Zens, Gregor||||https://arxiv.org/pdf/1801.02925||||1801.02925||||In this paper we estimate a Bayesian vector autoregressive model with factor stochastic volatility in the error term to assess the effects of an uncertainty shock in the Euro area. This allows us to treat macroeconomic uncertainty as a latent quantity during estimation. Only a limited number of contributions to the literature estimate uncertainty and its macroeconomic consequences jointly, and most are based on single country models. We analyze the special case of a shock restricted to the Euro area, where member states are highly related by construction. We find significant results of a decrease in real activity for all countries over a period of roughly a year following an uncertainty shock. Moreover, equity prices, short-term interest rates and exports tend to decline, while unemployment levels increase. Dynamic responses across countries differ slightly in magnitude and duration, with Ireland, Slovakia and Greece exhibiting different reactions for some macroeconomic fundamentals.||||@arxiv||||2018/01/09||||Implications of macroeconomic volatility in the Euro area||||In this paper we estimate a Bayesian vector autoregressive model with factor stochastic volatility in the error term to assess the effects of an uncertainty shock in the Euro area. This allows us...||||https://arxiv.org/abs/1801.02925v2||||econ||||
156||||None||||Econometric Modeling of Regional Electricity Spot Prices in the Australian Market||||arXiv.org||||2018/04/23||||Econometric Modeling of Regional Electricity Spot Prices in the Australian Market||||Smith, Michael Stanley || Shively, Thomas S.||||https://arxiv.org/pdf/1804.08218||||1804.08218||||Wholesale electricity markets are increasingly integrated via high voltage interconnectors, and inter-regional trade in electricity is growing. To model this, we consider a spatial equilibrium model of price formation, where constraints on inter-regional flows result in three distinct equilibria in prices. We use this to motivate an econometric model for the distribution of observed electricity spot prices that captures many of their unique empirical characteristics. The econometric model features supply and inter-regional trade cost functions, which are estimated using Bayesian monotonic regression smoothing methodology. A copula multivariate time series model is employed to capture additional dependence -- both cross-sectional and serial-- in regional prices. The marginal distributions are nonparametric, with means given by the regression means. The model has the advantage of preserving the heavy right-hand tail in the predictive densities of price. We fit the model to half-hourly spot price data in the five interconnected regions of the Australian national electricity market. The fitted model is then used to measure how both supply and price shocks in one region are transmitted to the distribution of prices in all regions in subsequent periods. Finally, to validate our econometric model, we show that prices forecast using the proposed model compare favorably with those from some benchmark alternatives.||||@arxiv||||2018/04/23||||Econometric Modeling of Regional Electricity Spot Prices in the...||||Wholesale electricity markets are increasingly integrated via high voltage interconnectors, and inter-regional trade in electricity is growing. To model this, we consider a spatial equilibrium...||||https://arxiv.org/abs/1804.08218v1||||econ||||
157||||None||||Estimating and decomposing most productive scale size in parallel DEA networks with shared inputs: A case of China's Five-Year Plans||||arXiv.org||||2019/10/10||||Estimating and decomposing most productive scale size in parallel DEA networks with shared inputs: A case of China's Five-Year Plans||||Assani, Saeed || Jiang, Jianlin || Assani, Ahmad || Yang, Feng||||https://arxiv.org/pdf/1910.03421||||1910.03421||||Attaining the optimal scale size of production systems is an issue frequently found in the priority questions on management agendas of various types of organizations. Determining the most productive scale size (MPSS) allows the decision makers not only to know the best scale size that their systems can achieve but also to tell the decision makers how to move the inefficient systems onto the MPSS region. This paper investigates the MPSS concept for production systems consisting of multiple subsystems connected in parallel. First, we propose a relational model where the MPSS of the whole system and the internal subsystems are measured in a single DEA implementation. Then, it is proved that the MPSS of the system can be decomposed as the weighted sum of the MPSS of the individual subsystems. The main result is that the system is overall MPSS if and only if it is MPSS in each subsystem. MPSS decomposition allows the decision makers to target the non-MPSS subsystems so that the necessary improvements can be readily suggested. An application of China's Five-Year Plans (FYPs) with shared inputs is used to show the applicability of the proposed model for estimating and decomposing MPSS in parallel network DEA. Industry and Agriculture sectors are selected as two parallel subsystems in the FYPs. Interesting findings have been noticed. Using the same amount of resources, the Industry sector had a better economic scale than the Agriculture sector. Furthermore, the last two FYPs, 11th and 12th, were the perfect two FYPs among the others.||||@arxiv||||2019/10/08||||Estimating and decomposing most productive scale size in parallel...||||Attaining the optimal scale size of production systems is an issue frequently found in the priority questions on management agendas of various types of organizations. Determining the most...||||https://arxiv.org/abs/1910.03421v3||||econ||||
158||||None||||Price competition with uncertain quality and cost||||arXiv.org||||2019/04/10||||Price competition with uncertain quality and cost||||Heinsalu, Sander||||https://arxiv.org/pdf/1903.03987||||1903.03987||||Consumers in many markets are uncertain about firms' qualities and costs, so buy based on both the price and the quality inferred from it. Optimal pricing depends on consumer heterogeneity only when firms with higher quality have higher costs, regardless of whether costs and qualities are private or public. If better quality firms have lower costs, then good quality is sold cheaper than bad under private costs and qualities, but not under public. However, if higher quality is costlier, then price weakly increases in quality under both informational environments.||||@arxiv||||2019/03/10||||Price competition with uncertain quality and cost||||Consumers in many markets are uncertain about firms' qualities and costs, so buy based on both the price and the quality inferred from it. Optimal pricing depends on consumer heterogeneity only...||||https://arxiv.org/abs/1903.03987v2||||econ||||
159||||None||||Estimation of Peer Effects in Endogenous Social Networks: Control Function Approach||||arXiv.org||||2019/07/30||||Estimation of Peer Effects in Endogenous Social Networks: Control Function Approach||||Johnsson, Ida || Moon, Hyungsik Roger||||https://arxiv.org/pdf/1709.10024||||1709.10024||||We propose a method of estimating the linear-in-means model of peer effects in which the peer group, defined by a social network, is endogenous in the outcome equation for peer effects. Endogeneity is due to unobservable individual characteristics that influence both link formation in the network and the outcome of interest. We propose two estimators of the peer effect equation that control for the endogeneity of the social connections using a control function approach. We leave the functional form of the control function unspecified and treat it as unknown. To estimate the model, we use a sieve semiparametric approach, and we establish asymptotics of the semiparametric estimator.||||@arxiv||||2017/09/28||||Estimation of Peer Effects in Endogenous Social Networks: Control...||||We propose a method of estimating the linear-in-means model of peer effects in which the peer group, defined by a social network, is endogenous in the outcome equation for peer effects....||||https://arxiv.org/abs/1709.10024v3||||econ||||
160||||None||||Optimal Search and Awareness Expansion||||arXiv.org||||2019/11/18||||Optimal Search and Awareness Expansion||||Greminger, Rafael P.||||https://arxiv.org/pdf/1911.07773||||1911.07773||||This paper introduces a search problem where a consumer has to first become aware of an alternative, before being able to search it. Initially, the consumer is aware of only a few alternatives. During search, the consumer sequentially decides between searching alternatives he is already aware of and expanding awareness to discover more products. I show that the optimal policy for this search problem is fully characterized by simple reservation values. Moreover, I prove that the purchase outcome of a consumer optimally solving the search problem is equivalent to the consumer simply choosing the product offering the largest value on a predetermined index.||||@arxiv||||2019/11/18||||Optimal Search and Awareness Expansion||||This paper introduces a search problem where a consumer has to first become aware of an alternative, before being able to search it. Initially, the consumer is aware of only a few alternatives....||||https://arxiv.org/abs/1911.07773v1||||econ||||
161||||None||||Digital Economy And Society. A Cross Country Comparison Of Hungary And Ukraine||||arXiv.org||||2019/01/02||||Digital Economy And Society. A Cross Country Comparison Of Hungary And Ukraine||||Nagy, Szabolcs||||https://arxiv.org/pdf/1901.00283||||1901.00283||||We live in the Digital Age in which both economy and society have been transforming significantly. The Internet and the connected digital devices are inseparable parts of our daily life and the engine of the economic growth. In this paper, first I analyzed the status of digital economy and society in Hungary, then compared it with Ukraine and made conclusions regarding the future development tendencies. Using secondary data provided by the European Commission I investigated the five components of the Digital Economy and Society Index of Hungary. I performed cross country analysis to find out the significant differences between Ukraine and Hungary in terms of access to the Internet and device use including smartphones, computers and tablets. Based on my findings, I concluded that Hungary is more developed in terms of the significant parameters of the digital economy and society than Ukraine, but even Hungary is an emerging digital nation. Considering the high growth rate of Internet, tablet and smartphone penetration in both countries, I expect faster progress in the development of the digital economy and society in Hungary and Ukraine.||||@arxiv||||2019/01/02||||Digital Economy And Society. A Cross Country Comparison Of Hungary...||||We live in the Digital Age in which both economy and society have been transforming significantly. The Internet and the connected digital devices are inseparable parts of our daily life and the...||||https://arxiv.org/abs/1901.00283v1||||econ||||
162||||None||||Modelling Cooperation in a Dynamic Healthcare System||||arXiv.org||||2019/09/06||||Modelling Cooperation in a Dynamic Healthcare System||||Alalawi, Zainab || Zeng, Yifeng || Han, The Anh || Elragig, Aiman||||https://arxiv.org/pdf/1909.03070||||1909.03070||||Our research is concerned with studying behavioural changes within a dynamic system, i.e. health care, and their effects on the decision-making process. Evolutionary Game theory is applied to investigate the most probable strategy(ies) adopted by individuals in a finite population based on the interactions among them with an eye to modelling behaviour using the following metrics: cost of investment, cost of management, cost of treatment, reputation benefit for the provider(s), and the gained health benefit for the patient.||||@arxiv||||2019/09/06||||Modelling Cooperation in a Dynamic Healthcare System||||Our research is concerned with studying behavioural changes within a dynamic system, i.e. health care, and their effects on the decision-making process. Evolutionary Game theory is applied to...||||https://arxiv.org/abs/1909.03070v1||||cs||||
163||||None||||nprobust: Nonparametric Kernel-Based Estimation and Robust Bias-Corrected Inference||||arXiv.org||||2019/06/01||||nprobust: Nonparametric Kernel-Based Estimation and Robust Bias-Corrected Inference||||Calonico, Sebastian || Cattaneo, Matias D. || Farrell, Max H.||||https://arxiv.org/pdf/1906.00198||||1906.00198||||Nonparametric kernel density and local polynomial regression estimators are very popular in Statistics, Economics, and many other disciplines. They are routinely employed in applied work, either as part of the main empirical analysis or as a preliminary ingredient entering some other estimation or inference procedure. This article describes the main methodological and numerical features of the software package nprobust, which offers an array of estimation and inference procedures for nonparametric kernel-based density and local polynomial regression methods, implemented in both the R and Stata statistical platforms. The package includes not only classical bandwidth selection, estimation, and inference methods (Wand and Jones, 1995; Fan and Gijbels, 1996), but also other recent developments in the statistics and econometrics literatures such as robust bias-corrected inference and coverage error optimal bandwidth selection (Calonico, Cattaneo and Farrell, 2018, 2019). Furthermore, this article also proposes a simple way of estimating optimal bandwidths in practice that always delivers the optimal mean square error convergence rate regardless of the specific evaluation point, that is, no matter whether it is implemented at a boundary or interior point. Numerical performance is illustrated using an empirical application and simulated data, where a detailed numerical comparison with other R packages is given.||||@arxiv||||2019/06/01||||nprobust: Nonparametric Kernel-Based Estimation and Robust...||||Nonparametric kernel density and local polynomial regression estimators are very popular in Statistics, Economics, and many other disciplines. They are routinely employed in applied work, either...||||https://arxiv.org/abs/1906.00198v1||||econ||||
164||||None||||House Price Modeling with Digital Census||||arXiv.org||||2018/08/29||||House Price Modeling with Digital Census||||Zhu, Enwei || Sobolevsky, Stanislav||||https://arxiv.org/pdf/1809.03834||||1809.03834||||Urban house prices are strongly associated with local socioeconomic factors. In literature, house price modeling is based on socioeconomic variables from traditional census, which is not real-time, dynamic and comprehensive. Inspired by the emerging concept of "digital census" - using large-scale digital records of human activities to measure urban population dynamics and socioeconomic conditions, we introduce three typical datasets, namely 311 complaints, crime complaints and taxi trips, into house price modeling. Based on the individual housing sales data in New York City, we provide comprehensive evidence that these digital census datasets can substantially improve the modeling performances on both house price levels and changes, regardless whether traditional census is included or not. Hence, digital census can serve as both effective alternatives and complements to traditional census for house price modeling.||||@arxiv||||2018/08/29||||House Price Modeling with Digital Census||||Urban house prices are strongly associated with local socioeconomic factors. In literature, house price modeling is based on socioeconomic variables from traditional census, which is not...||||https://arxiv.org/abs/1809.03834v1||||econ||||
165||||None||||Whos Ditching the Bus?||||arXiv.org||||2020/01/29||||Whos Ditching the Bus?||||Berrebi, Simon J. || Watkins, Kari E.||||https://arxiv.org/pdf/2001.02200||||2001.02200||||This paper uses stop-level passenger count data in four cities to understand the nation-wide bus ridership decline between 2012 and 2018. The local characteristics associated with ridership change are evaluated in Portland, Miami, Minneapolis/St-Paul, and Atlanta. Poisson models explain ridership as a cross-section and the change thereof as a panel. While controlling for change in frequency, jobs, and population, the correlation with local socio-demographic characteristics are investigated using data from the American Community Survey. The effect of changing neighborhood demographics on bus ridership are modeled using Longitudinal Employer-Household Dynamics data. At a point in time, neighborhoods with high proportions of non-white, carless, and most significantly, high-school-educated residents are the most likely to have high ridership. Over time, white neighborhoods are losing the most ridership across all four cities. Places with high concentrations of residents with college education and without access to a car also lose ridership at a faster rate in two of the cities. The sign and significance of these results remain consistent even when controlling for intra-urban migration. Although bus ridership is declining across neighborhood characteristics, these results suggest that the underlying cause must be primarily affecting the travel behavior of white bus riders. Shifts in neighborhood socio-demographics, however, were found to be modest and unlikely to be causing the nation-wide ridership crisis. Only in Miami is the proportion of white residents near the most frequent parts of the bus network increasing. There, demographic shifts may be contributing to the overall decline in bus ridership.||||@arxiv||||2020/01/07||||Whos Ditching the Bus?||||This paper uses stop-level passenger count data in four cities to understand the nation-wide bus ridership decline between 2012 and 2018. The local characteristics associated with ridership change...||||https://arxiv.org/abs/2001.02200v2||||cs||||
166||||None||||Nonparametric Tests for Treatment Effect Heterogeneity with Duration Outcomes||||arXiv.org||||2017/09/30||||Nonparametric Tests for Treatment Effect Heterogeneity with Duration Outcomes||||Sant'Anna, Pedro H. C.||||https://arxiv.org/pdf/1612.02090||||1612.02090||||This article proposes different tests for treatment effect heterogeneity when the outcome of interest, typically a duration variable, may be right-censored. The proposed tests study whether a policy 1) has zero distributional (average) effect for all subpopulations defined by covariate values, and 2) has homogeneous average effect across different subpopulations. The proposed tests are based on two-step Kaplan-Meier integrals, and do not rely on parametric distributional assumptions, shape restrictions, nor on restricting the potential treatment effect heterogeneity across different subpopulations. Our framework is suitable not only to exogenous treatment allocation, but can also account for treatment noncompliance, an important feature in many applications. The proposed tests are consistent against fixed alternatives, and can detect nonparametric alternatives converging to the null at the parametric $n^{-1/2}$-rate, $n$ being the sample size. Critical values are computed with the assistance of a multiplier bootstrap. The finite sample properties of the proposed tests are examined by means of a Monte Carlo study, and an application about the effect of labor market programs on unemployment duration. Open-source software is available for implementing all proposed tests.||||@arxiv||||2016/12/07||||Nonparametric Tests for Treatment Effect Heterogeneity with...||||This article proposes different tests for treatment effect heterogeneity when the outcome of interest, typically a duration variable, may be right-censored. The proposed tests study whether a...||||https://arxiv.org/abs/1612.02090v3||||econ||||
167||||None||||Finite Sample Inference for the Maximum Score Estimand||||arXiv.org||||2019/03/04||||Finite Sample Inference for the Maximum Score Estimand||||Rosen, Adam M. || Ura, Takuya||||https://arxiv.org/pdf/1903.01511||||1903.01511||||We provide a finite sample inference method for the structural parameters of a semiparametric binary response model under a conditional median restriction originally studied by Manski (1975, 1985). Our inference method is valid for any sample size and irrespective of whether the structural parameters are point identified or partially identified, for example due to the lack of a continuously distributed covariate with large support. Our inference approach exploits distributional properties of observable outcomes conditional on the observed sequence of exogenous variables. Moment inequalities conditional on this size n sequence of exogenous covariates are constructed, and the test statistic is a monotone function of violations of sample moment inequalities. The critical value used for inference is provided by the appropriate quantile of a known function of n independent Rademacher random variables. We investigate power properties of the underlying test and provide simulation studies to support the theoretical findings.||||@arxiv||||2019/03/04||||Finite Sample Inference for the Maximum Score Estimand||||We provide a finite sample inference method for the structural parameters of a semiparametric binary response model under a conditional median restriction originally studied by Manski (1975,...||||https://arxiv.org/abs/1903.01511v1||||econ||||
168||||None||||Deep learning, deep change? Mapping the development of the Artificial Intelligence General Purpose Technology||||arXiv.org||||2018/08/20||||Deep learning, deep change? Mapping the development of the Artificial Intelligence General Purpose Technology||||Klinger, J. || Mateos-Garcia, J. || Stathoulopoulos, K.||||https://arxiv.org/pdf/1808.06355||||1808.06355||||General Purpose Technologies (GPTs) that can be applied in many industries are an important driver of economic growth and national and regional competitiveness. In spite of this, the geography of their development and diffusion has not received significant attention in the literature. We address this with an analysis of Deep Learning (DL), a core technique in Artificial Intelligence (AI) increasingly being recognized as the latest GPT. We identify DL papers in a novel dataset from ArXiv, a popular preprints website, and use CrunchBase, a technology business directory to measure industrial capabilities related to it. After showing that DL conforms with the definition of a GPT, having experienced rapid growth and diffusion into new fields where it has generated an impact, we describe changes in its geography. Our analysis shows China's rise in AI rankings and relative decline in several European countries. We also find that initial volatility in the geography of DL has been followed by consolidation, suggesting that the window of opportunity for new entrants might be closing down as new DL research hubs become dominant. Finally, we study the regional drivers of DL clustering. We find that competitive DL clusters tend to be based in regions combining research and industrial activities related to it. This could be because GPT developers and adopters located close to each other can collaborate and share knowledge more easily, thus overcoming coordination failures in GPT deployment. Our analysis also reveals a Chinese comparative advantage in DL after we control for other explanatory factors, perhaps underscoring the importance of access to data and supportive policies for the successful development of this complex, `omni-use' technology.||||@arxiv||||2018/08/20||||Deep learning, deep change? Mapping the development of the...||||General Purpose Technologies (GPTs) that can be applied in many industries are an important driver of economic growth and national and regional competitiveness. In spite of this, the geography of...||||https://arxiv.org/abs/1808.06355v1||||cs||||
169||||None||||Deep Learning for Causal Inference||||arXiv.org||||2018/03/01||||Deep Learning for Causal Inference||||Ramachandra, Vikas||||https://arxiv.org/pdf/1803.00149||||1803.00149||||In this paper, we propose deep learning techniques for econometrics, specifically for causal inference and for estimating individual as well as average treatment effects. The contribution of this paper is twofold: 1. For generalized neighbor matching to estimate individual and average treatment effects, we analyze the use of autoencoders for dimensionality reduction while maintaining the local neighborhood structure among the data points in the embedding space. This deep learning based technique is shown to perform better than simple k nearest neighbor matching for estimating treatment effects, especially when the data points have several features/covariates but reside in a low dimensional manifold in high dimensional space. We also observe better performance than manifold learning methods for neighbor matching. 2. Propensity score matching is one specific and popular way to perform matching in order to estimate average and individual treatment effects. We propose the use of deep neural networks (DNNs) for propensity score matching, and present a network called PropensityNet for this. This is a generalization of the logistic regression technique traditionally used to estimate propensity scores and we show empirically that DNNs perform better than logistic regression at propensity score matching. Code for both methods will be made available shortly on Github at: https://github.com/vikas84bf||||@arxiv||||2018/03/01||||Deep Learning for Causal Inference||||In this paper, we propose deep learning techniques for econometrics, specifically for causal inference and for estimating individual as well as average treatment effects. The contribution of this...||||https://arxiv.org/abs/1803.00149v1||||cs||||
170||||None||||Segregation with Social Linkages: Evaluating Schelling's Model with Networked Individuals||||arXiv.org||||2020/01/09||||Segregation with Social Linkages: Evaluating Schelling's Model with Networked Individuals||||Cerqueti, Roy || De Benedictis, Luca || Sciabolazza, Valerio Leone||||https://arxiv.org/pdf/2001.02959||||2001.02959||||This paper generalizes the original Schelling (1969, 1971a,b, 2006) model of racial and residential segregation to a context of variable externalities due to social linkages. In a setting in which individuals' utility function is a convex combination of a heuristic function a la Schelling, of the distance to friends, and of the cost of moving, the prediction of the original model gets attenuated: the segregation equilibria are not the unique solutions. While the cost of distance has a monotonic pro-status-quo effect, equivalent to that of models of migration and gravity models, if friends and neighbours are formed following independent processes the location of friends in space generates an externality that reinforces the initial configuration if the distance to friends is minimal, and if the degree of each agent is high. The effect on segregation equilibria crucially depends on the role played by network externalities.||||@arxiv||||2020/01/09||||Segregation with Social Linkages: Evaluating Schelling's Model...||||This paper generalizes the original Schelling (1969, 1971a,b, 2006) model of racial and residential segregation to a context of variable externalities due to social linkages. In a setting in which...||||https://arxiv.org/abs/2001.02959v1||||econ||||
171||||None||||Fair and Efficient Division among Families||||arXiv.org||||2018/11/22||||Fair and Efficient Division among Families||||Bade, Sophie || Segal-Halevi, Erel||||https://arxiv.org/pdf/1811.06684||||1811.06684||||Fair division theory mostly involves individual consumption. But resources are often allocated to groups, such as families or countries, whose members consume the same bundle but have different preferences. Do fair and efficient allocations exist in such an "economy of families"? We adapt three common notions of fairness: fair-share, no-envy and egalitarian-equivalence, to an economy of families. The stronger adaptation --- individual fairness --- requires that each individual in each family perceives the division as fair; the weaker one --- family fairness --- requires that the family as a whole, treated as a single agent with (typically) incomplete preferences, perceives the division as fair. Individual-fair-share, family-no-envy and family-egalitarian-equivalence are compatible with efficiency under broad conditions. The same holds for individual-no-envy when there are only two families. In contrast, individual-no-envy with three or more families and individual-egalitarian-equivalence with two or more families are typically incompatible with efficiency, unlike the situation in an economy of individuals. The common market equilibrium approach to fairness is of limited use in economies with families. In contrast, the leximin approach is broadly applicable: it yields an efficient, individual-fair-share, and family-egalitarian-equivalent allocation.||||@arxiv||||2018/11/16||||Fair and Efficient Division among Families||||Fair division theory mostly involves individual consumption. But resources are often allocated to groups, such as families or countries, whose members consume the same bundle but have different...||||https://arxiv.org/abs/1811.06684v2||||econ||||
172||||None||||The Impact of Supervision and Incentive Process in Explaining Wage Profile and Variance||||arXiv.org||||2018/06/04||||The Impact of Supervision and Incentive Process in Explaining Wage Profile and Variance||||Kasir, Nitsa || Sohlberg, Idit||||https://arxiv.org/pdf/1806.01332||||1806.01332||||The implementation of a supervision and incentive process for identical workers may lead to wage variance that stems from employer and employee optimization. The harder it is to assess the nature of the labor output, the more important such a process becomes, and the influence of such a process on wage development growth. The dynamic model presented in this paper shows that an employer will choose to pay a worker a starting wage that is less than what he deserves, resulting in a wage profile that fits the classic profile in the human-capital literature. The wage profile and wage variance rise at times of technological advancements, which leads to increased turnover as older workers are replaced by younger workers due to a rise in the relative marginal cost of the former.||||@arxiv||||2018/06/04||||The Impact of Supervision and Incentive Process in Explaining Wage...||||The implementation of a supervision and incentive process for identical workers may lead to wage variance that stems from employer and employee optimization. The harder it is to assess the nature...||||https://arxiv.org/abs/1806.01332v1||||econ||||
173||||None||||A Community Microgrid Architecture with an Internal Local Market||||arXiv.org||||2019/02/20||||A Community Microgrid Architecture with an Internal Local Market||||Cornélusse, Bertrand || Savelli, Iacopo || Paoletti, Simone || Giannitrapani, Antonio || Vicino, Antonio||||https://arxiv.org/pdf/1810.09803||||1810.09803||||This work fits in the context of community microgrids, where members of a community can exchange energy and services among themselves, without going through the usual channels of the public electricity grid. We introduce and analyze a framework to operate a community microgrid, and to share the resulting revenues and costs among its members. A market-oriented pricing of energy exchanges within the community is obtained by implementing an internal local market based on the marginal pricing scheme. The market aims at maximizing the social welfare of the community, thanks to the more efficient allocation of resources, the reduction of the peak power to be paid, and the increased amount of reserve, achieved at an aggregate level. A community microgrid operator, acting as a benevolent planner, redistributes revenues and costs among the members, in such a way that the solution achieved by each member within the community is not worse than the solution it would achieve by acting individually. In this way, each member is incentivized to participate in the community on a voluntary basis. The overall framework is formulated in the form of a bilevel model, where the lower level problem clears the market, while the upper level problem plays the role of the community microgrid operator. Numerical results obtained on a real test case implemented in Belgium show around 54% cost savings on a yearly scale for the community, as compared to the case when its members act individually.||||@arxiv||||2018/10/23||||A Community Microgrid Architecture with an Internal Local Market||||This work fits in the context of community microgrids, where members of a community can exchange energy and services among themselves, without going through the usual channels of the public...||||https://arxiv.org/abs/1810.09803v3||||cs||||
174||||None||||Nonlinear Factor Models for Network and Panel Data||||arXiv.org||||2019/10/15||||Nonlinear Factor Models for Network and Panel Data||||Chen, Mingli || Fernández-Val, Iván || Weidner, Martin||||https://arxiv.org/pdf/1412.5647||||1412.5647||||Factor structures or interactive effects are convenient devices to incorporate latent variables in panel data models. We consider fixed effect estimation of nonlinear panel single-index models with factor structures in the unobservables, which include logit, probit, ordered probit and Poisson specifications. We establish that fixed effect estimators of model parameters and average partial effects have normal distributions when the two dimensions of the panel grow large, but might suffer of incidental parameter bias. We show how models with factor structures can also be applied to capture important features of network data such as reciprocity, degree heterogeneity, homophily in latent variables and clustering. We illustrate this applicability with an empirical example to the estimation of a gravity equation of international trade between countries using a Poisson model with multiple factors.||||@arxiv||||2014/12/17||||Nonlinear Factor Models for Network and Panel Data||||Factor structures or interactive effects are convenient devices to incorporate latent variables in panel data models. We consider fixed effect estimation of nonlinear panel single-index models...||||https://arxiv.org/abs/1412.5647v4||||econ||||
175||||None||||Estimation and Inference for Synthetic Control Methods with Spillover Effects||||arXiv.org||||2019/11/22||||Estimation and Inference for Synthetic Control Methods with Spillover Effects||||Cao, Jianfei || Dowd, Connor||||https://arxiv.org/pdf/1902.07343||||1902.07343||||The synthetic control method is often used in treatment effect estimation with panel data where only a few units are treated and a small number of post-treatment periods are available. Current estimation and inference procedures for synthetic control methods do not allow for the existence of spillover effects, which are plausible in many applications. In this paper, we consider estimation and inference for synthetic control methods, allowing for spillover effects. We propose estimators for both direct treatment effects and spillover effects and show they are asymptotically unbiased. In addition, we propose an inferential procedure and show it is asymptotically unbiased. Our estimation and inference procedure applies to cases with multiple treated units or periods, and where the underlying factor model is either stationary or cointegrated. In simulations, we confirm that the presence of spillovers renders current methods biased and have distorted sizes, whereas our methods yield properly sized tests and retain reasonable power. We apply our method to a classic empirical example that investigates the effect of California's tobacco control program as in Abadie et al. (2010) and find evidence of spillovers.||||@arxiv||||2019/02/19||||Estimation and Inference for Synthetic Control Methods with...||||The synthetic control method is often used in treatment effect estimation with panel data where only a few units are treated and a small number of post-treatment periods are available. Current...||||https://arxiv.org/abs/1902.07343v2||||econ||||
176||||None||||Nonparametric Identification in Panels using Quantiles||||arXiv.org||||2014/08/05||||Nonparametric Identification in Panels using Quantiles||||Chernozhukov, Victor || Fernandez-Val, Ivan || Hoderlein, Stefan || Holzmann, Hajo || Newey, Whitney||||https://arxiv.org/pdf/1312.4094||||1312.4094||||This paper considers identification and estimation of ceteris paribus effects of continuous regressors in nonseparable panel models with time homogeneity. The effects of interest are derivatives of the average and quantile structural functions of the model. We find that these derivatives are identified with two time periods for "stayers", i.e. for individuals with the same regressor values in two time periods. We show that the identification results carry over to models that allow location and scale time effects. We propose nonparametric series methods and a weighted bootstrap scheme to estimate and make inference on the identified effects. The bootstrap proposed allows uniform inference for function-valued parameters such as quantile effects uniformly over a region of quantile indices and/or regressor values. An empirical application to Engel curve estimation with panel data illustrates the results.||||@arxiv||||2013/12/15||||Nonparametric Identification in Panels using Quantiles||||This paper considers identification and estimation of ceteris paribus effects of continuous regressors in nonseparable panel models with time homogeneity. The effects of interest are derivatives...||||https://arxiv.org/abs/1312.4094v3||||econ||||
177||||None||||On Capital Allocation under Time and Information Constraints||||arXiv.org||||2019/06/14||||On Capital Allocation under Time and Information Constraints||||Börner, Christoph J. || Hoffmann, Ingo || Poetter, Fabian || Schmitz, Tim||||https://arxiv.org/pdf/1906.10624||||1906.10624||||Attempts to allocate capital to a selection of different investment objects often face the problem that investors' decisions are made under limited information (no historical return data) and an extremely limited timeframe. Nevertheless, in some cases, rational investors with a certain level of experience are able to ordinally rank investment alternatives through relative assessments of the probability that an investment will be successful. However, to apply traditional portfolio optimization models, analysts must use historical (or simulated/expected) return data as the basis for their calculations. Our paper develops an alternative portfolio optimization framework that is able to handle this kind of information (given by the ordinal ranking of investment alternatives) and to calculate an optimal capital allocation based on a Cobb-Douglas function. Considering risk-neutral investors, we show that the results of this portfolio optimization model usually outperform the output generated by the (intuitive) Equally Weighted Portfolio (EWP) of the different investment alternatives, which is the result of optimization when one is unable to incorporate additional data (the ordinal ranking of the alternatives). In a further extension, we show that our model is also able to address risk-averse investors to capture diversification benefits.||||@arxiv||||2019/06/14||||On Capital Allocation under Time and Information Constraints||||Attempts to allocate capital to a selection of different investment objects often face the problem that investors' decisions are made under limited information (no historical return data) and an...||||https://arxiv.org/abs/1906.10624v1||||econ||||
178||||None||||Chain effects of clean water: The Mills-Reincke phenomenon in early twentieth-century Japan||||arXiv.org||||2019/08/25||||Chain effects of clean water: The Mills-Reincke phenomenon in early twentieth-century Japan||||Inoue, Tatsuki || Ogasawara, Kota||||https://arxiv.org/pdf/1805.00875||||1805.00875||||This study explores the validity of chain effects of clean water, which are known as the "Mills-Reincke phenomenon," in early twentieth-century Japan. Recent studies have reported that water purifications systems are responsible for huge contributions to human capital. Although some studies have investigated the instantaneous effects of water-supply systems in pre-war Japan, little is known about the chain effects of these systems. By analyzing city-level cause-specific mortality data from 1922-1940, we find that a decline in typhoid deaths by one per 1,000 people decreased the risk of death due to non-waterborne diseases such as tuberculosis and pneumonia by 0.742-2.942 per 1,000 people. Our finding suggests that the observed Mills-Reincke phenomenon could have resulted in the relatively rapid decline in the mortality rate in early twentieth-century Japan.||||@arxiv||||2018/04/26||||Chain effects of clean water: The Mills-Reincke phenomenon in...||||This study explores the validity of chain effects of clean water, which are known as the "Mills-Reincke phenomenon," in early twentieth-century Japan. Recent studies have reported that water...||||https://arxiv.org/abs/1805.00875v3||||econ||||
179||||None||||A General Framework for Prediction in Time Series Models||||arXiv.org||||2019/02/05||||A General Framework for Prediction in Time Series Models||||Beutner, Eric || Heinemann, Alexander || Smeekes, Stephan||||https://arxiv.org/pdf/1902.01622||||1902.01622||||In this paper we propose a general framework to analyze prediction in time series models and show how a wide class of popular time series models satisfies this framework. We postulate a set of high-level assumptions, and formally verify these assumptions for the aforementioned time series models. Our framework coincides with that of Beutner et al. (2019, arXiv:1710.00643) who establish the validity of conditional confidence intervals for predictions made in this framework. The current paper therefore complements the results in Beutner et al. (2019, arXiv:1710.00643) by providing practically relevant applications of their theory.||||@arxiv||||2019/02/05||||A General Framework for Prediction in Time Series Models||||In this paper we propose a general framework to analyze prediction in time series models and show how a wide class of popular time series models satisfies this framework. We postulate a set of...||||https://arxiv.org/abs/1902.01622v1||||econ||||
180||||None||||Robust Monopoly Regulation||||arXiv.org||||2019/10/09||||Robust Monopoly Regulation||||Guo, Yingni || Shmaya, Eran||||https://arxiv.org/pdf/1910.04260||||1910.04260||||We study the regulation of a monopolistic firm using a robust-design approach. We solve for the policy that minimizes the regulator's worst-case regret, where the regret is the difference between his complete-information payoff minus his realized payoff. When the regulator's payoff is consumers' surplus, it is optimal to impose a price cap. The optimal cap balances the benefit from more surplus for consumers and the loss from underproduction. When his payoff is consumers' surplus plus the firm's profit, he offers a piece-rate subsidy in order to mitigate underproduction, but caps the total subsidy so as not to incentivize severe overproduction.||||@arxiv||||2019/10/09||||Robust Monopoly Regulation||||We study the regulation of a monopolistic firm using a robust-design approach. We solve for the policy that minimizes the regulator's worst-case regret, where the regret is the difference between...||||https://arxiv.org/abs/1910.04260v1||||cs||||
181||||None||||Equilibrium refinements in games with many players||||arXiv.org||||2019/12/30||||Equilibrium refinements in games with many players||||Chen, Enxian || Qiao, Lei || Sun, Xiang || Sun, Yeneng||||https://arxiv.org/pdf/1912.12908||||1912.12908||||This paper introduces three notions of perfect equilibrium for games with many players, respectively, in behavioral, mixed and pure strategies. The equivalence between behavioral strategy perfect equilibrium and mixed strategy perfect equilibrium is established. More importantly, it is shown that after the resolution of strategic uncertainty, a mixed strategy perfect equilibrium leads to a pure strategy perfect equilibrium almost surely. Various properties related to limit admissibility are also considered.||||@arxiv||||2019/12/30||||Equilibrium refinements in games with many players||||This paper introduces three notions of perfect equilibrium for games with many players, respectively, in behavioral, mixed and pure strategies. The equivalence between behavioral strategy perfect...||||https://arxiv.org/abs/1912.12908v1||||cs||||
182||||None||||Sophisticated and small versus simple and sizeable: When does it pay off to introduce drifting coefficients in Bayesian VARs?||||arXiv.org||||2017/11/29||||Sophisticated and small versus simple and sizeable: When does it pay off to introduce drifting coefficients in Bayesian VARs?||||Feldkircher, Martin || Huber, Florian || Kastner, Gregor||||https://arxiv.org/pdf/1711.00564||||1711.00564||||We assess the relationship between model size and complexity in the time-varying parameter VAR framework via thorough predictive exercises for the Euro Area, the United Kingdom and the United States. It turns out that sophisticated dynamics through drifting coefficients are important in small data sets while simpler models tend to perform better in sizeable data sets. To combine best of both worlds, novel shrinkage priors help to mitigate the curse of dimensionality, resulting in competitive forecasts for all scenarios considered. Furthermore, we discuss dynamic model selection to improve upon the best performing individual model for each point in time.||||@arxiv||||2017/11/01||||Sophisticated and small versus simple and sizeable: When does it...||||We assess the relationship between model size and complexity in the time-varying parameter VAR framework via thorough predictive exercises for the Euro Area, the United Kingdom and the United...||||https://arxiv.org/abs/1711.00564v2||||econ||||
183||||None||||Factor Investing: Hierarchical Ensemble Learning||||arXiv.org||||2019/02/04||||Factor Investing: Hierarchical Ensemble Learning||||Feng, Guanhao || He, Jingyu||||https://arxiv.org/pdf/1902.01015||||1902.01015||||We present a Bayesian hierarchical framework for both cross-sectional and time-series return prediction. Our approach builds on a market-timing predictive system that jointly allows for time-varying coefficients driven by fundamental characteristics. With a Bayesian formulation for ensemble learning, we examine the joint predictability as well as portfolio efficiency via predictive distribution. In the empirical analysis of asset-sector allocation, our hierarchical ensemble learning portfolio achieves 500% cumulative returns in the period 1998-2017, and outperforms most workhorse benchmarks as well as the passive investing index. Our Bayesian inference for model selection identifies useful macro predictors (long-term yield, inflation, and stock market variance) and asset characteristics (dividend yield, accrual, and gross profit). Using the selected model for predicting sector evolution, an equally weighted long-short portfolio on winners over losers achieves a 46% Sharpe ratio with a significant Jensen's alpha. Finally, we explore an underexploited connection between classical Bayesian forecasting and modern ensemble learning.||||@arxiv||||2019/02/04||||Factor Investing: Hierarchical Ensemble Learning||||We present a Bayesian hierarchical framework for both cross-sectional and time-series return prediction. Our approach builds on a market-timing predictive system that jointly allows for...||||https://arxiv.org/abs/1902.01015v1||||econ||||
184||||None||||On the Price of Satisficing in Network User Equilibria||||arXiv.org||||2019/11/18||||On the Price of Satisficing in Network User Equilibria||||Takalloo, Mahdi || Kwon, Changhyun||||https://arxiv.org/pdf/1911.07914||||1911.07914||||When network users are satisficing decision-makers, the resulting traffic pattern attains a satisficing user equilibrium, which may deviate from the (perfectly rational) user equilibrium. In a satisficing user equilibrium traffic pattern, the total system travel time can be worse than in the case of the PRUE. We show how bad the worst-case satisficing user equilibrium traffic pattern can be, compared to the perfectly rational user equilibrium. We call the ratio between the total system travel times of the two traffic patterns the price of satisficing, for which we provide an analytical bound. We compare the analytical bound with numerical bounds for several transportation networks.||||@arxiv||||2019/11/18||||On the Price of Satisficing in Network User Equilibria||||When network users are satisficing decision-makers, the resulting traffic pattern attains a satisficing user equilibrium, which may deviate from the (perfectly rational) user equilibrium. In a...||||https://arxiv.org/abs/1911.07914v1||||cs||||
185||||None||||On the Kolkata index as a measure of income inequality||||arXiv.org||||2019/10/18||||On the Kolkata index as a measure of income inequality||||Banerjee, Suchismita || Chakrabarti, Bikas K. || Mitra, Manipushpak || Mutuswami, Suresh||||https://arxiv.org/pdf/1905.03615||||1905.03615||||We study the mathematical and economic structure of the Kolkata (k) index of income inequality. We show that the k-index always exists and is a unique fixed point of the complementary Lorenz function, where the Lorenz function itself gives the fraction of cumulative income possessed by the cumulative fraction of population (when arranged from poorer to richer). We show that the k-index generalizes Pareto's 80/20 rule. Although the k and Pietra indices both split the society into two groups, we show that k-index is a more intensive measure for the poor-rich split. We compare the normalized k-index with the Gini coefficient and the Pietra index and discuss when they coincide. We establish that for any income distribution the value of Gini coefficient is no less than that of the Pietra index and the value of the Pietra index is no less than that of the normalized k-index. While the Gini coefficient and the Pietra index are affected by transfers exclusively among the rich or among the poor, the k-index is only affected by transfers across the two groups.||||@arxiv||||2019/04/30||||On the Kolkata index as a measure of income inequality||||We study the mathematical and economic structure of the Kolkata (k) index of income inequality. We show that the k-index always exists and is a unique fixed point of the complementary Lorenz...||||https://arxiv.org/abs/1905.03615v2||||cond-mat||||
186||||None||||An interim core for normal form games and exchange economies with incomplete information: a correction||||arXiv.org||||2019/03/23||||An interim core for normal form games and exchange economies with incomplete information: a correction||||Askoura, Youcef||||https://arxiv.org/pdf/1903.09867||||1903.09867||||We consider the interim core of normal form cooperative games and exchange economies with incomplete information based on the partition model. We develop a solution concept that we can situate roughly between Wilson's coarse core and Yannelis's private core. We investigate the interim negotiation of contracts and address the two situations of contract delivery: interim and ex post. Our solution differs from Wilson's concept because the measurability of strategies in our solution is postponed until the consumption date (assumed with respect to the information that will be known by the players at the consumption date). For interim consumption, our concept differs from Yannelis's private core because players can negotiate conditional on proper common knowledge events in our solution, which strengthens the interim aspect of the game, as we will illustrate with examples.||||@arxiv||||2019/03/23||||An interim core for normal form games and exchange economies with...||||We consider the interim core of normal form cooperative games and exchange economies with incomplete information based on the partition model. We develop a solution concept that we can situate...||||https://arxiv.org/abs/1903.09867v1||||econ||||
187||||None||||Post-Selection Inference for Generalized Linear Models with Many Controls||||arXiv.org||||2016/03/21||||Post-Selection Inference for Generalized Linear Models with Many Controls||||Belloni, Alexandre || Chernozhukov, Victor || Wei, Ying||||https://arxiv.org/pdf/1304.3969||||1304.3969||||This paper considers generalized linear models in the presence of many controls. We lay out a general methodology to estimate an effect of interest based on the construction of an instrument that immunize against model selection mistakes and apply it to the case of logistic binary choice model. More specifically we propose new methods for estimating and constructing confidence regions for a regression parameter of primary interest $α_0$, a parameter in front of the regressor of interest, such as the treatment variable or a policy variable. These methods allow to estimate $α_0$ at the root-$n$ rate when the total number $p$ of other regressors, called controls, potentially exceed the sample size $n$ using sparsity assumptions. The sparsity assumption means that there is a subset of $s<n$ controls which suffices to accurately approximate the nuisance part of the regression function. Importantly, the estimators and these resulting confidence regions are valid uniformly over $s$-sparse models satisfying $s^2\log^2 p = o(n)$ and other technical conditions. These procedures do not rely on traditional consistent model selection arguments for their validity. In fact, they are robust with respect to moderate model selection mistakes in variable selection. Under suitable conditions, the estimators are semi-parametrically efficient in the sense of attaining the semi-parametric efficiency bounds for the class of models in this paper.||||@arxiv||||2013/04/15||||Post-Selection Inference for Generalized Linear Models with Many Controls||||This paper considers generalized linear models in the presence of many controls. We lay out a general methodology to estimate an effect of interest based on the construction of an instrument that...||||https://arxiv.org/abs/1304.3969v3||||econ||||
188||||None||||The Bretton Woods Experience and ERM||||arXiv.org||||2018/07/02||||The Bretton Woods Experience and ERM||||Kirrane, Chris||||https://arxiv.org/pdf/1807.00418||||1807.00418||||Historical examination of the Bretton Woods system allows comparisons to be made with the current evolution of the EMS.||||@arxiv||||2018/07/02||||The Bretton Woods Experience and ERM||||Historical examination of the Bretton Woods system allows comparisons to be made with the current evolution of the EMS.||||https://arxiv.org/abs/1807.00418v1||||econ||||
189||||None||||Temporal-Difference estimation of dynamic discrete choice models||||arXiv.org||||2019/12/19||||Temporal-Difference estimation of dynamic discrete choice models||||Adusumilli, Karun || Eckardt, Dita||||https://arxiv.org/pdf/1912.09509||||1912.09509||||We propose a new algorithm to estimate the structural parameters in dynamic discrete choice models. The algorithm is based on the conditional choice probability approach, but uses the idea of Temporal-Difference learning from the Reinforcement Learning literature to estimate the different terms in the value functions. In estimating these terms with functional approximations using basis functions, our approach has the advantage of naturally allowing for continuous state spaces. Furthermore, it does not require specification of transition probabilities, and even estimation of choice probabilities can be avoided using a recursive procedure. Computationally, our algorithm only requires solving a low dimensional linear equation. We find that it is substantially faster than existing approaches when the finite dependence property does not hold, and comparable in speed to approaches that exploit this property. For the estimation of dynamic games, our procedure does not require integrating over the actions of other players, which further heightens the computational advantage. We show that our estimator is consistent, and efficient under discrete state spaces. In settings with continuous states, we propose easy to implement locally robust corrections in order to achieve parametric rates of convergence. Preliminary Monte Carlo simulations confirm the workings of our algorithm.||||@arxiv||||2019/12/19||||Temporal-Difference estimation of dynamic discrete choice models||||We propose a new algorithm to estimate the structural parameters in dynamic discrete choice models. The algorithm is based on the conditional choice probability approach, but uses the idea of...||||https://arxiv.org/abs/1912.09509v1||||econ||||
190||||None||||Demand forecasting techniques for build-to-order lean manufacturing supply chains||||arXiv.org||||2019/05/20||||Demand forecasting techniques for build-to-order lean manufacturing supply chains||||Rivera-Castro, Rodrigo || Nazarov, Ivan || Xiang, Yuke || Pletneev, Alexander || Maksimov, Ivan || Burnaev, Evgeny||||https://arxiv.org/pdf/1905.07902||||1905.07902||||Build-to-order (BTO) supply chains have become common-place in industries such as electronics, automotive and fashion. They enable building products based on individual requirements with a short lead time and minimum inventory and production costs. Due to their nature, they differ significantly from traditional supply chains. However, there have not been studies dedicated to demand forecasting methods for this type of setting. This work makes two contributions. First, it presents a new and unique data set from a manufacturer in the BTO sector. Second, it proposes a novel data transformation technique for demand forecasting of BTO products. Results from thirteen forecasting methods show that the approach compares well to the state-of-the-art while being easy to implement and to explain to decision-makers.||||@arxiv||||2019/05/20||||Demand forecasting techniques for build-to-order lean...||||Build-to-order (BTO) supply chains have become common-place in industries such as electronics, automotive and fashion. They enable building products based on individual requirements with a short...||||https://arxiv.org/abs/1905.07902v1||||cs||||
191||||None||||Multiplayer Bandit Learning, from Competition to Cooperation||||arXiv.org||||2019/10/11||||Multiplayer Bandit Learning, from Competition to Cooperation||||Brânzei, Simina || Peres, Yuval||||https://arxiv.org/pdf/1908.01135||||1908.01135||||The stochastic multi-armed bandit model captures the tradeoff between exploration and exploitation. We study the effects of competition and cooperation on this tradeoff. Suppose there are $k$ arms and two players, Alice and Bob. In every round, each player pulls an arm, receives the resulting reward, and observes the choice of the other player but not their reward. Alice's utility is $Γ_A + λΓ_B$ (and similarly for Bob), where $Γ_A$ is Alice's total reward and $λ\in [-1, 1]$ is a cooperation parameter. At $λ= -1$ the players are competing in a zero-sum game, at $λ= 1$, they are fully cooperating, and at $λ= 0$, they are neutral: each player's utility is their own reward. The model is related to the economics literature on strategic experimentation, where usually players observe each other's rewards.   With discount factor $β$, the Gittins index reduces the one-player problem to the comparison between a risky arm, with a prior $μ$, and a predictable arm, with success probability $p$. The value of $p$ where the player is indifferent between the arms is the Gittins index $g = g(μ,β) > m$, where $m$ is the mean of the risky arm.   We show that competing players explore less than a single player: there is $p^* \in (m, g)$ so that for all $p > p^*$, the players stay at the predictable arm. However, the players are not myopic: they still explore for some $p > m$. On the other hand, cooperating players explore more than a single player. We also show that neutral players learn from each other, receiving strictly higher total rewards than they would playing alone, for all $ p\in (p^*, g)$, where $p^*$ is the threshold from the competing case.   Finally, we show that competing and neutral players eventually settle on the same arm in every Nash equilibrium, while this can fail for cooperating players.||||@arxiv||||2019/08/03||||Multiplayer Bandit Learning, from Competition to Cooperation||||The stochastic multi-armed bandit model captures the tradeoff between exploration and exploitation. We study the effects of competition and cooperation on this tradeoff. Suppose there are $k$ arms...||||https://arxiv.org/abs/1908.01135v2||||cs||||
192||||None||||Solving Dynamic Discrete Choice Models: Integrated or Expected Value Function?||||arXiv.org||||2018/01/11||||Solving Dynamic Discrete Choice Models: Integrated or Expected Value Function?||||Mogensen, Patrick Kofod||||https://arxiv.org/pdf/1801.03978||||1801.03978||||Dynamic Discrete Choice Models (DDCMs) are important in the structural estimation literature. Since the structural errors are practically always continuous and unbounded in nature, researchers often use the expected value function. The idea to solve for the expected value function made solution more practical and estimation feasible. However, as we show in this paper, the expected value function is impractical compared to an alternative: the integrated (ex ante) value function. We provide brief descriptions of the inefficacy of the former, and benchmarks on actual problems with varying cardinality of the state space and number of decisions. Though the two approaches solve the same problem in theory, the benchmarks support the claim that the integrated value function is preferred in practice.||||@arxiv||||2018/01/11||||Solving Dynamic Discrete Choice Models: Integrated or Expected...||||Dynamic Discrete Choice Models (DDCMs) are important in the structural estimation literature. Since the structural errors are practically always continuous and unbounded in nature, researchers...||||https://arxiv.org/abs/1801.03978v1||||econ||||
193||||None||||Feature quantization for parsimonious and interpretable predictive models||||arXiv.org||||2019/03/21||||Feature quantization for parsimonious and interpretable predictive models||||Ehrhardt, Adrien || Biernacki, Christophe || Vandewalle, Vincent || Heinrich, Philippe||||https://arxiv.org/pdf/1903.08920||||1903.08920||||For regulatory and interpretability reasons, logistic regression is still widely used. To improve prediction accuracy and interpretability, a preprocessing step quantizing both continuous and categorical data is usually performed: continuous features are discretized and, if numerous, levels of categorical features are grouped. An even better predictive accuracy can be reached by embedding this quantization estimation step directly into the predictive estimation step itself. But doing so, the predictive loss has to be optimized on a huge set. To overcome this difficulty, we introduce a specific two-step optimization strategy: first, the optimization problem is relaxed by approximating discontinuous quantization functions by smooth functions; second, the resulting relaxed optimization problem is solved via a particular neural network. The good performances of this approach, which we call glmdisc, are illustrated on simulated and real data from the UCI library and Crédit Agricole Consumer Finance (a major European historic player in the consumer credit market).||||@arxiv||||2019/03/21||||Feature quantization for parsimonious and interpretable predictive models||||For regulatory and interpretability reasons, logistic regression is still widely used. To improve prediction accuracy and interpretability, a preprocessing step quantizing both continuous and...||||https://arxiv.org/abs/1903.08920v1||||econ||||
194||||None||||Stackelberg Independence||||arXiv.org||||2019/03/10||||Stackelberg Independence||||Hinnosaar, Toomas||||https://arxiv.org/pdf/1903.04060||||1903.04060||||The standard model of sequential capacity choices is the Stackelberg quantity leadership model with linear demand. I show that under the standard assumptions, leaders' actions are informative about market conditions and independent of leaders' beliefs about the arrivals of followers. However, this Stackelberg independence property relies on all standard assumptions being satisfied. It fails to hold whenever the demand function is non-linear, marginal cost is not constant, goods are differentiated, firms are non-identical, or there are any externalities. I show that small deviations from the linear demand assumption may make the leaders' choices completely uninformative.||||@arxiv||||2019/03/10||||Stackelberg Independence||||The standard model of sequential capacity choices is the Stackelberg quantity leadership model with linear demand. I show that under the standard assumptions, leaders' actions are informative...||||https://arxiv.org/abs/1903.04060v1||||cs||||
195||||None||||Estimating the effect of treatments allocated by randomized waiting lists||||arXiv.org||||2018/10/22||||Estimating the effect of treatments allocated by randomized waiting lists||||de Chaisemartin, Clement || Behaghel, Luc||||https://arxiv.org/pdf/1511.01453||||1511.01453||||Oversubscribed treatments are often allocated using randomized waiting lists. Applicants are ranked randomly, and treatment offers are made following that ranking until all seats are filled. To estimate causal effects, researchers often compare applicants getting and not getting an offer. We show that those two groups are not statistically comparable. Therefore, the estimator arising from that comparison is inconsistent. We propose a new estimator, and show that it is consistent. Finally, we revisit an application, and we show that using our estimator can lead to sizably different results from those obtained using the commonly used estimator.||||@arxiv||||2015/11/03||||Estimating the effect of treatments allocated by randomized waiting lists||||Oversubscribed treatments are often allocated using randomized waiting lists. Applicants are ranked randomly, and treatment offers are made following that ranking until all seats are filled. To...||||https://arxiv.org/abs/1511.01453v6||||econ||||
196||||None||||Smoothing quantile regressions||||arXiv.org||||2019/08/15||||Smoothing quantile regressions||||Fernandes, Marcelo || Guerre, Emmanuel || Horta, Eduardo||||https://arxiv.org/pdf/1905.08535||||1905.08535||||We propose to smooth the entire objective function, rather than only the check function, in a linear quantile regression context. Not only does the resulting smoothed quantile regression estimator yield a lower mean squared error and a more accurate Bahadur-Kiefer representation than the standard estimator, but it is also asymptotically differentiable. We exploit the latter to propose a quantile density estimator that does not suffer from the curse of dimensionality. This means estimating the conditional density function without worrying about the dimension of the covariate vector. It also allows for two-stage efficient quantile regression estimation. Our asymptotic theory holds uniformly with respect to the bandwidth and quantile level. Finally, we propose a rule of thumb for choosing the smoothing bandwidth that should approximate well the optimal bandwidth. Simulations confirm that our smoothed quantile regression estimator indeed performs very well in finite samples.||||@arxiv||||2019/05/21||||Smoothing quantile regressions||||We propose to smooth the entire objective function, rather than only the check function, in a linear quantile regression context. Not only does the resulting smoothed quantile regression estimator...||||https://arxiv.org/abs/1905.08535v3||||econ||||
197||||None||||Uniform Post Selection Inference for LAD Regression and Other Z-estimation problems||||arXiv.org||||2018/01/22||||Uniform Post Selection Inference for LAD Regression and Other Z-estimation problems||||Belloni, Alexandre || Chernozhukov, Victor || Kato, Kengo||||https://arxiv.org/pdf/1304.0282||||1304.0282||||We develop uniformly valid confidence regions for regression coefficients in a high-dimensional sparse median regression model with homoscedastic errors. Our methods are based on a moment equation that is immunized against non-regular estimation of the nuisance part of the median regression function by using Neyman's orthogonalization. We establish that the resulting instrumental median regression estimator of a target regression coefficient is asymptotically normally distributed uniformly with respect to the underlying sparse model and is semi-parametrically efficient. We also generalize our method to a general non-smooth Z-estimation framework with the number of target parameters $p_1$ being possibly much larger than the sample size $n$. We extend Huber's results on asymptotic normality to this setting, demonstrating uniform asymptotic normality of the proposed estimators over $p_1$-dimensional rectangles, constructing simultaneous confidence bands on all of the $p_1$ target parameters, and establishing asymptotic validity of the bands uniformly over underlying approximately sparse models.   Keywords: Instrument; Post-selection inference; Sparsity; Neyman's Orthogonal Score test; Uniformly valid inference; Z-estimation.||||@arxiv||||2013/04/01||||Uniform Post Selection Inference for LAD Regression and Other...||||We develop uniformly valid confidence regions for regression coefficients in a high-dimensional sparse median regression model with homoscedastic errors. Our methods are based on a moment equation...||||https://arxiv.org/abs/1304.0282v5||||econ||||
198||||None||||Sion's mini-max theorem and Nash equilibrium in a multi-players game with two groups which is zero-sum and symmetric in each group||||arXiv.org||||2018/09/04||||Sion's mini-max theorem and Nash equilibrium in a multi-players game with two groups which is zero-sum and symmetric in each group||||Satoh, Atsuhiro || Tanaka, Yasuhito||||https://arxiv.org/pdf/1809.01488||||1809.01488||||We consider the relation between Sion's minimax theorem for a continuous function and a Nash equilibrium in a multi-players game with two groups which is zero-sum and symmetric in each group. We will show the following results.   1. The existence of Nash equilibrium which is symmetric in each group implies Sion's minimax theorem with the coincidence of the maximin strategy and the minimax strategy for players in each group. %given the values of the strategic variables.   2. Sion's minimax theorem with the coincidence of the maximin strategy and the minimax strategy for players in each group implies the existence of a Nash equilibrium which is symmetric in each group.   Thus, they are equivalent. An example of such a game is a relative profit maximization game in each group under oligopoly with two groups such that firms in each group have the same cost functions and maximize their relative profits in each group, and the demand functions are symmetric for the firms in each group.||||@arxiv||||2018/09/04||||Sion's mini-max theorem and Nash equilibrium in a...||||We consider the relation between Sion's minimax theorem for a continuous function and a Nash equilibrium in a multi-players game with two groups which is zero-sum and symmetric in each group. We...||||https://arxiv.org/abs/1809.01488v1||||cs||||
199||||None||||Boomerang: Rebounding the Consequences of Reputation Feedback on Crowdsourcing Platforms||||arXiv.org||||2019/04/14||||Boomerang: Rebounding the Consequences of Reputation Feedback on Crowdsourcing Platforms||||Snehalkumar || Gaikwad, S. || Morina, Durim || Ginzberg, Adam || Mullings, Catherine || Goyal, Shirish || Gamage, Dilrukshi || Diemert, Christopher || Burton, Mathias || Zhou, Sharon || Whiting, Mark || Ziulkoski, Karolina || Ballav, Alipta || Gilbee, Aaron || Niranga, Senadhipathige S. || Sehgal, Vibhor || Lin, Jasmine || Kristianto, Leonardy || Richmond-Fuller, Angela || Regino, Jeff || Chhibber, Nalin || Majeti, Dinesh || Sharma, Sachin || Mananova, Kamila || Dhakal, Dinesh || Dai, William || Purynova, Victoria || Sandeep, Samarth || Chandrakanthan, Varshine || Sarma, Tejas || Matin, Sekandar || Nasser, Ahmed || Nistala, Rohit || Stolzoff, Alexander || Milland, Kristy || Mathur, Vinayak || Vaish, Rajan || Bernstein, Michael S.||||https://arxiv.org/pdf/1904.06722||||1904.06722||||Paid crowdsourcing platforms suffer from low-quality work and unfair rejections, but paradoxically, most workers and requesters have high reputation scores. These inflated scores, which make high-quality work and workers difficult to find, stem from social pressure to avoid giving negative feedback. We introduce Boomerang, a reputation system for crowdsourcing that elicits more accurate feedback by rebounding the consequences of feedback directly back onto the person who gave it. With Boomerang, requesters find that their highly-rated workers gain earliest access to their future tasks, and workers find tasks from their highly-rated requesters at the top of their task feed. Field experiments verify that Boomerang causes both workers and requesters to provide feedback that is more closely aligned with their private opinions. Inspired by a game-theoretic notion of incentive-compatibility, Boomerang opens opportunities for interaction design to incentivize honest reporting over strategic dishonesty.||||@arxiv||||2019/04/14||||Boomerang: Rebounding the Consequences of Reputation Feedback on...||||Paid crowdsourcing platforms suffer from low-quality work and unfair rejections, but paradoxically, most workers and requesters have high reputation scores. These inflated scores, which make...||||https://arxiv.org/abs/1904.06722v1||||cs||||
200||||None||||Spatial polarisation within foreign trade and transnational firms' networks. The Case of Central and Eastern Europe||||arXiv.org||||2019/10/31||||Spatial polarisation within foreign trade and transnational firms' networks. The Case of Central and Eastern Europe||||Zdanowska, Natalia||||https://arxiv.org/pdf/1910.14658||||1910.14658||||After the fall of the Berlin Wall, Central and Eastern Europe were subject to strong polarisation processes. This article proposes examines two neglected aspects regarding the transition period: a comparative static assessment of foreign trade since 1967 until 2012 and a city-centred analysis of transnational companies in 2013. Results show a growing economic differentiation between the North-West and South-East as well as a division between large metropolises and other cities. These findings may complement the targeting of specific regional strategies such as those conceived within the Cohesion policy of the European Union.||||@arxiv||||2019/10/31||||Spatial polarisation within foreign trade and transnational...||||After the fall of the Berlin Wall, Central and Eastern Europe were subject to strong polarisation processes. This article proposes examines two neglected aspects regarding the transition period: a...||||https://arxiv.org/abs/1910.14658v1||||econ||||
201||||None||||Machine Learning Methods Economists Should Know About||||arXiv.org||||2019/03/24||||Machine Learning Methods Economists Should Know About||||Athey, Susan || Imbens, Guido||||https://arxiv.org/pdf/1903.10075||||1903.10075||||We discuss the relevance of the recent Machine Learning (ML) literature for economics and econometrics. First we discuss the differences in goals, methods and settings between the ML literature and the traditional econometrics and statistics literatures. Then we discuss some specific methods from the machine learning literature that we view as important for empirical researchers in economics. These include supervised learning methods for regression and classification, unsupervised learning methods, as well as matrix completion methods. Finally, we highlight newly developed methods at the intersection of ML and econometrics, methods that typically perform better than either off-the-shelf ML or more traditional econometric methods when applied to particular classes of problems, problems that include causal inference for average treatment effects, optimal policy estimation, and estimation of the counterfactual effect of price changes in consumer choice models.||||@arxiv||||2019/03/24||||Machine Learning Methods Economists Should Know About||||We discuss the relevance of the recent Machine Learning (ML) literature for economics and econometrics. First we discuss the differences in goals, methods and settings between the ML literature...||||https://arxiv.org/abs/1903.10075v1||||econ||||
202||||None||||Resolving New Keynesian Anomalies with Wealth in the Utility Function||||arXiv.org||||2019/12/11||||Resolving New Keynesian Anomalies with Wealth in the Utility Function||||Michaillat, Pascal || Saez, Emmanuel||||https://arxiv.org/pdf/1905.13645||||1905.13645||||At the zero lower bound, the New Keynesian model predicts that output and inflation collapse to implausibly low levels, and that government spending and forward guidance have implausibly large effects. To resolve these anomalies, we introduce wealth into the utility function; the justification is that wealth is a marker of social status, and people value status. Since people partly save to accrue social status, the Euler equation is modified. As a result, when the marginal utility of wealth is sufficiently large, the dynamical system representing the zero-lower-bound equilibrium transforms from a saddle to a source---which resolves all the anomalies.||||@arxiv||||2019/05/31||||Resolving New Keynesian Anomalies with Wealth in the Utility Function||||At the zero lower bound, the New Keynesian model predicts that output and inflation collapse to implausibly low levels, and that government spending and forward guidance have implausibly large...||||https://arxiv.org/abs/1905.13645v3||||econ||||
203||||None||||Relative Maximum Likelihood Updating of Ambiguous Beliefs||||arXiv.org||||2019/12/11||||Relative Maximum Likelihood Updating of Ambiguous Beliefs||||Cheng, Xiaoyu||||https://arxiv.org/pdf/1911.02678||||1911.02678||||When ambiguous beliefs are represented by multiple priors, a decision maker (DM)'s updating rule may also include a step of refining the initial belief. Maximum Likelihood (ML) updating indicates one example of such a refinement, whereas Full Bayesian (FB) updating does not allow for any refinement. In fact, ML and FB are the two extremes of refining beliefs with likelihood in updating. To capture behaviors between these two extremes, the present paper proposes and axiomatizes a new updating rule, Relative Maximum Likelihood (RML), to represent conditional preferences that are not as extreme as either case. RML updates an intermediate set of priors, which can be expressed as a linear contraction of the initial set with respect to the set of maximum likelihood priors. The linear contraction parameter captures the DM's relative inclination towards ML with respect to FB, it is also the threshold of a relative likelihood ratio test. Moreover, RML includes both FB and ML as the two extreme special cases.   Additionally, the present paper provides an axiomatization of ML for preferences admit Maxmin Expected Utility (MEU) representation as a preliminary result for the exposition of RML.||||@arxiv||||2019/11/06||||Relative Maximum Likelihood Updating of Ambiguous Beliefs||||When ambiguous beliefs are represented by multiple priors, a decision maker (DM)'s updating rule may also include a step of refining the initial belief. Maximum Likelihood (ML) updating indicates...||||https://arxiv.org/abs/1911.02678v2||||econ||||
204||||None||||Bootstrap Methods in Econometrics||||arXiv.org||||2018/09/11||||Bootstrap Methods in Econometrics||||Horowitz, Joel L.||||https://arxiv.org/pdf/1809.04016||||1809.04016||||The bootstrap is a method for estimating the distribution of an estimator or test statistic by re-sampling the data or a model estimated from the data. Under conditions that hold in a wide variety of econometric applications, the bootstrap provides approximations to distributions of statistics, coverage probabilities of confidence intervals, and rejection probabilities of hypothesis tests that are more accurate than the approximations of first-order asymptotic distribution theory. The reductions in the differences between true and nominal coverage or rejection probabilities can be very large. In addition, the bootstrap provides a way to carry out inference in certain settings where obtaining analytic distributional approximations is difficult or impossible. This article explains the usefulness and limitations of the bootstrap in contexts of interest in econometrics. The presentation is informal and expository. It provides an intuitive understanding of how the bootstrap works. Mathematical details are available in references that are cited.||||@arxiv||||2018/09/11||||Bootstrap Methods in Econometrics||||The bootstrap is a method for estimating the distribution of an estimator or test statistic by re-sampling the data or a model estimated from the data. Under conditions that hold in a wide variety...||||https://arxiv.org/abs/1809.04016v1||||econ||||
205||||None||||Quantitative earnings enhancement from share buybacks||||arXiv.org||||2019/11/11||||Quantitative earnings enhancement from share buybacks||||Middleton, Lawrence || Dodd, James || Baird, Graham||||https://arxiv.org/pdf/1911.04199||||1911.04199||||This paper aims to explore the mechanical effect of a company's share repurchase on earnings per share (EPS). In particular, while a share repurchase scheme will reduce the overall number of shares, suggesting that the EPS may increase, clearly the expenditure will reduce the net earnings of a company, introducing a trade-off between these competing effects. We first of all review accretive share repurchases, then characterise the increase in EPS as a function of price paid by the company. Subsequently, we analyse and quantify the estimated difference in earnings growth between a company's natural growth in the absence of buyback scheme to that with its earnings altered as a result of the buybacks. We conclude with an examination of the effect of share repurchases in two cases studies in the US stock-market.||||@arxiv||||2019/11/11||||Quantitative earnings enhancement from share buybacks||||This paper aims to explore the mechanical effect of a company's share repurchase on earnings per share (EPS). In particular, while a share repurchase scheme will reduce the overall number of...||||https://arxiv.org/abs/1911.04199v1||||econ||||
206||||None||||Mechanism Design with News Utility||||arXiv.org||||2018/08/12||||Mechanism Design with News Utility||||Duraj, Jetlir||||https://arxiv.org/pdf/1808.04020||||1808.04020||||News utility is the idea that the utility of an agent depends on changes in her beliefs over consumption and money. We introduce news utility into otherwise classical static Bayesian mechanism design models. We show that a key role is played by the timeline of the mechanism, i.e. whether there are delays between the announcement stage, the participation stage, the play stage and the realization stage of a mechanism. Depending on the timing, agents with news utility can experience two additional news utility effects: a surprise effect derived from comparing to pre-mechanism beliefs, as well as a realization effect derived from comparing post-play beliefs with the actual outcome of the mechanism.   We look at two distinct mechanism design settings reflecting the two main strands of the classical literature. In the first model, a monopolist screens an agent according to the magnitude of her loss aversion. In the second model, we consider a general multi-agent Bayesian mechanism design setting where the uncertainty of each player stems from not knowing the intrinsic types of the other agents. We give applications to auctions and public good provision which illustrate how news utility changes classical results.   For both models we characterize the optimal design of the timeline. A timeline featuring no delay between participation and play but a delay in realization is never optimal in either model. In the screening model the optimal timeline is one without delays. In auction settings, under fairly natural assumptions the optimal timeline has delays between all three stages of the mechanism.||||@arxiv||||2018/08/12||||Mechanism Design with News Utility||||News utility is the idea that the utility of an agent depends on changes in her beliefs over consumption and money. We introduce news utility into otherwise classical static Bayesian mechanism...||||https://arxiv.org/abs/1808.04020v1||||econ||||
207||||None||||Single-crossing Implementation||||arXiv.org||||2019/06/23||||Single-crossing Implementation||||Cohenn, Nathann || Elkind, Edith || Lakhani, Foram||||https://arxiv.org/pdf/1906.09671||||1906.09671||||An election over a finite set of candidates is called single-crossing if, as we sweep through the list of voters from left to right, the relative order of every pair of candidates changes at most once. Such elections have many attractive properties: e.g., their majority relation is transitive and they admit efficient algorithms for problems that are NP-hard in general. If a given election is not single-crossing, it is important to understand what are the obstacles that prevent it from having this property. In this paper, we propose a mapping between elections and graphs that provides us with a convenient encoding of such obstacles. This mapping enables us to use the toolbox of graph theory in order to analyze the complexity of detecting nearly single-crossing elections, i.e., elections that can be made single-crossing by a small number of modifications.||||@arxiv||||2019/06/23||||Single-crossing Implementation||||An election over a finite set of candidates is called single-crossing if, as we sweep through the list of voters from left to right, the relative order of every pair of candidates changes at most...||||https://arxiv.org/abs/1906.09671v1||||cs||||
208||||None||||On the Identifying Content of Instrument Monotonicity||||arXiv.org||||2019/10/16||||On the Identifying Content of Instrument Monotonicity||||Kamat, Vishal||||https://arxiv.org/pdf/1807.01661||||1807.01661||||This paper studies the identifying content of the instrument monotonicity assumption of Imbens and Angrist (1994) on the distribution of potential outcomes in a model with a binary outcome, a binary treatment and an exogenous binary instrument. Specifically, I derive necessary and sufficient conditions on the distribution of the data under which the identified set for the distribution of potential outcomes when the instrument monotonicity assumption is imposed can be a strict subset of that when it is not imposed.||||@arxiv||||2018/07/04||||On the Identifying Content of Instrument Monotonicity||||This paper studies the identifying content of the instrument monotonicity assumption of Imbens and Angrist (1994) on the distribution of potential outcomes in a model with a binary outcome, a...||||https://arxiv.org/abs/1807.01661v2||||econ||||
209||||None||||Impact of Artificial Intelligence on Businesses: from Research, Innovation, Market Deployment to Future Shifts in Business Models||||arXiv.org||||2019/05/03||||Impact of Artificial Intelligence on Businesses: from Research, Innovation, Market Deployment to Future Shifts in Business Models||||Soni, Neha || Sharma, Enakshi Khular || Singh, Narotam || Kapoor, Amita||||https://arxiv.org/pdf/1905.02092||||1905.02092||||The fast pace of artificial intelligence (AI) and automation is propelling strategists to reshape their business models. This is fostering the integration of AI in the business processes but the consequences of this adoption are underexplored and need attention. This paper focuses on the overall impact of AI on businesses - from research, innovation, market deployment to future shifts in business models. To access this overall impact, we design a three-dimensional research model, based upon the Neo-Schumpeterian economics and its three forces viz. innovation, knowledge, and entrepreneurship. The first dimension deals with research and innovation in AI. In the second dimension, we explore the influence of AI on the global market and the strategic objectives of the businesses and finally, the third dimension examines how AI is shaping business contexts. Additionally, the paper explores AI implications on actors and its dark sides.||||@arxiv||||2019/05/03||||Impact of Artificial Intelligence on Businesses: from Research,...||||The fast pace of artificial intelligence (AI) and automation is propelling strategists to reshape their business models. This is fostering the integration of AI in the business processes but the...||||https://arxiv.org/abs/1905.02092v1||||cs||||
210||||None||||Revealed Stochastic Preference: A One-Paragraph Proof and Generalization||||arXiv.org||||2019/02/20||||Revealed Stochastic Preference: A One-Paragraph Proof and Generalization||||Stoye, Jörg||||https://arxiv.org/pdf/1810.10604||||1810.10604||||McFadden and Richter (1991) and later McFadden (2005) show that the Axiom of Revealed Stochastic Preference characterizes rationalizability of choice probabilities through random utility models on finite universal choice spaces. This note proves the result in one short, elementary paragraph and extends it to set valued choice. The latter requires a different axiom than is reported in McFadden (2005).||||@arxiv||||2018/10/24||||Revealed Stochastic Preference: A One-Paragraph Proof and Generalization||||McFadden and Richter (1991) and later McFadden (2005) show that the Axiom of Revealed Stochastic Preference characterizes rationalizability of choice probabilities through random utility models on...||||https://arxiv.org/abs/1810.10604v2||||econ||||
211||||None||||Fractional order statistic approximation for nonparametric conditional quantile inference||||arXiv.org||||2016/09/28||||Fractional order statistic approximation for nonparametric conditional quantile inference||||Goldman, Matt || Kaplan, David M.||||https://arxiv.org/pdf/1609.09035||||1609.09035||||Using and extending fractional order statistic theory, we characterize the $O(n^{-1})$ coverage probability error of the previously proposed confidence intervals for population quantiles using $L$-statistics as endpoints in Hutson (1999). We derive an analytic expression for the $n^{-1}$ term, which may be used to calibrate the nominal coverage level to get $O\bigl(n^{-3/2}[\log(n)]^3\bigr)$ coverage error. Asymptotic power is shown to be optimal. Using kernel smoothing, we propose a related method for nonparametric inference on conditional quantiles. This new method compares favorably with asymptotic normality and bootstrap methods in theory and in simulations. Code is available from the second author's website for both unconditional and conditional methods, simulations, and empirical examples.||||@arxiv||||2016/09/28||||Fractional order statistic approximation for nonparametric...||||Using and extending fractional order statistic theory, we characterize the $O(n^{-1})$ coverage probability error of the previously proposed confidence intervals for population quantiles using...||||https://arxiv.org/abs/1609.09035v1||||econ||||
212||||None||||Quasi-Experimental Shift-Share Research Designs||||arXiv.org||||2019/12/06||||Quasi-Experimental Shift-Share Research Designs||||Borusyak, Kirill || Hull, Peter || Jaravel, Xavier||||https://arxiv.org/pdf/1806.01221||||1806.01221||||Many studies use shift-share (or "Bartik") instruments, which average a set of shocks with exposure share weights. We provide a new econometric framework for such designs in which identification follows from the quasi-random assignment of shocks, allowing exposure shares to be endogenous. This framework is centered around a numerical equivalence: conventional shift-share instrumental variable (SSIV) regression coefficients are equivalently obtained from a transformed regression where the shocks are used directly as an instrument. This equivalence implies a shock-level translation of the SSIV exclusion restriction, which holds when shocks are as-good-as-randomly assigned and large in number, with sufficient dispersion in their average exposure. We discuss and illustrate several practical insights delivered by this framework.||||@arxiv||||2018/06/04||||Quasi-Experimental Shift-Share Research Designs||||Many studies use shift-share (or "Bartik") instruments, which average a set of shocks with exposure share weights. We provide a new econometric framework for such designs in which identification...||||https://arxiv.org/abs/1806.01221v6||||econ||||
213||||None||||Lattice Studies of Gerrymandering Strategies||||arXiv.org||||2018/08/08||||Lattice Studies of Gerrymandering Strategies||||Gatesman, Kyle || Unwin, James||||https://arxiv.org/pdf/1808.02826||||1808.02826||||We propose three novel gerrymandering algorithms which incorporate the spatial distribution of voters with the aim of constructing gerrymandered, equal-population, connected districts. Moreover, we develop lattice models of voter distributions, based on analogies to electrostatic potentials, in order to compare different gerrymandering strategies. Due to the probabilistic population fluctuations inherent to our voter models, Monte Carlo methods can be applied to the districts constructed via our gerrymandering algorithms. Through Monte Carlo studies we quantify the effectiveness of each of our gerrymandering algorithms and we also argue that gerrymandering strategies which do not include spatial data lead to (legally prohibited) highly disconnected districts. Of the three algorithms we propose, two are based on different strategies for packing opposition voters, and the third is a new approach to algorithmic gerrymandering based on genetic algorithms, which automatically guarantees that all districts are connected. Furthermore, we use our lattice voter model to examine the effectiveness of isoperimetric quotient tests and our results provide further quantitative support for implementing compactness tests in real-world political redistricting.||||@arxiv||||2018/08/08||||Lattice Studies of Gerrymandering Strategies||||We propose three novel gerrymandering algorithms which incorporate the spatial distribution of voters with the aim of constructing gerrymandered, equal-population, connected districts. Moreover,...||||https://arxiv.org/abs/1808.02826v1||||cs||||
214||||None||||Estimating Large Mixed-Frequency Bayesian VAR Models||||arXiv.org||||2019/12/04||||Estimating Large Mixed-Frequency Bayesian VAR Models||||Ankargren, Sebastian || Jonéus, Paulina||||https://arxiv.org/pdf/1912.02231||||1912.02231||||We discuss the issue of estimating large-scale vector autoregressive (VAR) models with stochastic volatility in real-time situations where data are sampled at different frequencies. In the case of a large VAR with stochastic volatility, the mixed-frequency data warrant an additional step in the already computationally challenging Markov Chain Monte Carlo algorithm used to sample from the posterior distribution of the parameters. We suggest the use of a factor stochastic volatility model to capture a time-varying error covariance structure. Because the factor stochastic volatility model renders the equations of the VAR conditionally independent, settling for this particular stochastic volatility model comes with major computational benefits. First, we are able to improve upon the mixed-frequency simulation smoothing step by leveraging a univariate and adaptive filtering algorithm. Second, the regression parameters can be sampled equation-by-equation in parallel. These computational features of the model alleviate the computational burden and make it possible to move the mixed-frequency VAR to the high-dimensional regime. We illustrate the model by an application to US data using our mixed-frequency VAR with 20, 34 and 119 variables.||||@arxiv||||2019/12/04||||Estimating Large Mixed-Frequency Bayesian VAR Models||||We discuss the issue of estimating large-scale vector autoregressive (VAR) models with stochastic volatility in real-time situations where data are sampled at different frequencies. In the case of...||||https://arxiv.org/abs/1912.02231v1||||econ||||
215||||None||||Bayesian shrinkage in mixture of experts models: Identifying robust determinants of class membership||||arXiv.org||||2019/01/12||||Bayesian shrinkage in mixture of experts models: Identifying robust determinants of class membership||||Zens, Gregor||||https://arxiv.org/pdf/1809.04853||||1809.04853||||A method for implicit variable selection in mixture of experts frameworks is proposed. We introduce a prior structure where information is taken from a set of independent covariates. Robust class membership predictors are identified using a normal gamma prior. The resulting model setup is used in a finite mixture of Bernoulli distributions to find homogenous clusters of women in Mozambique based on their information sources on HIV. Fully Bayesian inference is carried out via the implementation of a Gibbs sampler.||||@arxiv||||2018/09/13||||Bayesian shrinkage in mixture of experts models: Identifying...||||A method for implicit variable selection in mixture of experts frameworks is proposed. We introduce a prior structure where information is taken from a set of independent covariates. Robust class...||||https://arxiv.org/abs/1809.04853v2||||econ||||
216||||None||||Maastricht and Monetary Cooperation||||arXiv.org||||2018/07/02||||Maastricht and Monetary Cooperation||||Kirrane, Chris||||https://arxiv.org/pdf/1807.00419||||1807.00419||||This paper describes the opportunities and also the difficulties of EMU with regard to international monetary cooperation. Even though the institutional and intellectual assistance to the coordination of monetary policy in the EU will probably be strengthened with the EMU, among the shortcomings of the Maastricht Treaty concerns the relationship between the founder members and those countries who wish to remain outside monetary union.||||@arxiv||||2018/07/02||||Maastricht and Monetary Cooperation||||This paper describes the opportunities and also the difficulties of EMU with regard to international monetary cooperation. Even though the institutional and intellectual assistance to the...||||https://arxiv.org/abs/1807.00419v1||||econ||||
217||||None||||A micro-simulation model of irrigation farms in the southern Murray-Darling Basin||||arXiv.org||||2019/03/14||||A micro-simulation model of irrigation farms in the southern Murray-Darling Basin||||Dinh, Huong || Donoghoe, Manannan || Hughes, Neal || Goesch, Tim||||https://arxiv.org/pdf/1903.05781||||1903.05781||||This paper presents a farm level irrigation microsimulation model of the southern Murray-Darling Basin. The model leverages detailed ABARES survey data to estimate a series of input demand and output supply equations, derived from a normalised quadratic profit function. The parameters from this estimation are then used to simulate the impact on total cost, revenue and profit of a hypothetical 30 per cent increase in the price of water. The model is still under development, with several potential improvements suggested in the conclusion. This is a working paper, provided for the purpose of receiving feedback on the analytical approach to improve future iterations of the microsimulation model.||||@arxiv||||2019/03/14||||A micro-simulation model of irrigation farms in the southern...||||This paper presents a farm level irrigation microsimulation model of the southern Murray-Darling Basin. The model leverages detailed ABARES survey data to estimate a series of input demand and...||||https://arxiv.org/abs/1903.05781v1||||econ||||
218||||None||||A mixture autoregressive model based on Student's $t$-distribution||||arXiv.org||||2018/05/10||||A mixture autoregressive model based on Student's $t$-distribution||||Meitz, Mika || Preve, Daniel || Saikkonen, Pentti||||https://arxiv.org/pdf/1805.04010||||1805.04010||||A new mixture autoregressive model based on Student's $t$-distribution is proposed. A key feature of our model is that the conditional $t$-distributions of the component models are based on autoregressions that have multivariate $t$-distributions as their (low-dimensional) stationary distributions. That autoregressions with such stationary distributions exist is not immediate. Our formulation implies that the conditional mean of each component model is a linear function of past observations and the conditional variance is also time varying. Compared to previous mixture autoregressive models our model may therefore be useful in applications where the data exhibits rather strong conditional heteroskedasticity. Our formulation also has the theoretical advantage that conditions for stationarity and ergodicity are always met and these properties are much more straightforward to establish than is common in nonlinear autoregressive models. An empirical example employing a realized kernel series based on S&P 500 high-frequency data shows that the proposed model performs well in volatility forecasting.||||@arxiv||||2018/05/10||||A mixture autoregressive model based on Student's $t$-distribution||||A new mixture autoregressive model based on Student's $t$-distribution is proposed. A key feature of our model is that the conditional $t$-distributions of the component models are based on...||||https://arxiv.org/abs/1805.04010v1||||econ||||
219||||None||||A Multicriteria Decision Making Approach to Study the Barriers to the Adoption of Autonomous Vehicles||||arXiv.org||||2019/12/27||||A Multicriteria Decision Making Approach to Study the Barriers to the Adoption of Autonomous Vehicles||||Raj, Alok || Kumar, J Ajith || Bansal, Prateek||||https://arxiv.org/pdf/1904.12051||||1904.12051||||The automation technology is emerging, but the adoption rate of autonomous vehicles (AV) will largely depend upon how policymakers and the government address various challenges such as public acceptance and infrastructure development. This study proposes a five-step method to understand these barriers to AV adoption. First, based on a literature review followed by discussions with experts, ten barriers are identified. Second, the opinions of eighteen experts from industry and academia regarding inter-relations between these barriers are recorded. Third, a multicriteria decision making (MCDM) technique, the grey-based Decision-making Trial and Evaluation Laboratory (Grey-DEMATEL), is applied to characterize the structure of relationships between the barriers. Fourth, robustness of the results is tested using sensitivity analysis. Fifth, the key results are depicted in a causal loop diagram (CLD), a systems thinking approach, to comprehend cause-and-effect relationships between the barriers. The results indicate that the lack of customer acceptance (LCA) is the most prominent barrier, the one which should be addressed at the highest priority. The CLD suggests that LCA can be rather mitigated by addressing two other prominent, yet more tangible, barriers -- lack of industry standards and the absence of regulations and certifications. The study's overarching contribution thus lies in bringing to fore multiple barriers to AV adoption and their potential influences on each other. Moreover, the insights from this study can help associations related to AVs prioritize their endeavors to expedite AV adoption. From the methodological perspective, this is the first study in transportation literature that integrates Grey-DEMATEL with systems thinking.||||@arxiv||||2019/04/26||||A Multicriteria Decision Making Approach to Study the Barriers to...||||The automation technology is emerging, but the adoption rate of autonomous vehicles (AV) will largely depend upon how policymakers and the government address various challenges such as public...||||https://arxiv.org/abs/1904.12051v3||||econ||||
220||||None||||Most Important Fundamental Rule of Poker Strategy||||arXiv.org||||2019/06/28||||Most Important Fundamental Rule of Poker Strategy||||Ganzfried, Sam || Chiswick, Max||||https://arxiv.org/pdf/1906.09895||||1906.09895||||Poker is a large complex game of imperfect information, which has been singled out as a major AI challenge problem. Recently there has been a series of breakthroughs culminating in agents that have successfully defeated the strongest human players in two-player no-limit Texas hold 'em. The strongest agents are based on algorithms for approximating Nash equilibrium strategies, which are stored in massive binary files and unintelligible to humans. A recent line of research has explored approaches for extrapolating knowledge from strong game-theoretic strategies that can be understood by humans. This would be useful when humans are the ultimate decision maker and allow humans to make better decisions from massive algorithmically-generated strategies. Using techniques from machine learning we have uncovered a new simple, fundamental rule of poker strategy that leads to a significant improvement in performance over the best prior rule and can also easily be applied by human players.||||@arxiv||||2019/06/08||||Most Important Fundamental Rule of Poker Strategy||||Poker is a large complex game of imperfect information, which has been singled out as a major AI challenge problem. Recently there has been a series of breakthroughs culminating in agents that...||||https://arxiv.org/abs/1906.09895v2||||cs||||
221||||None||||Estimating Heterogeneous Consumer Preferences for Restaurants and Travel Time Using Mobile Location Data||||arXiv.org||||2018/01/22||||Estimating Heterogeneous Consumer Preferences for Restaurants and Travel Time Using Mobile Location Data||||Athey, Susan || Blei, David || Donnelly, Robert || Ruiz, Francisco || Schmidt, Tobias||||https://arxiv.org/pdf/1801.07826||||1801.07826||||This paper analyzes consumer choices over lunchtime restaurants using data from a sample of several thousand anonymous mobile phone users in the San Francisco Bay Area. The data is used to identify users' approximate typical morning location, as well as their choices of lunchtime restaurants. We build a model where restaurants have latent characteristics (whose distribution may depend on restaurant observables, such as star ratings, food category, and price range), each user has preferences for these latent characteristics, and these preferences are heterogeneous across users. Similarly, each item has latent characteristics that describe users' willingness to travel to the restaurant, and each user has individual-specific preferences for those latent characteristics. Thus, both users' willingness to travel and their base utility for each restaurant vary across user-restaurant pairs. We use a Bayesian approach to estimation. To make the estimation computationally feasible, we rely on variational inference to approximate the posterior distribution, as well as stochastic gradient descent as a computational approach. Our model performs better than more standard competing models such as multinomial logit and nested logit models, in part due to the personalization of the estimates. We analyze how consumers re-allocate their demand after a restaurant closes to nearby restaurants versus more distant restaurants with similar characteristics, and we compare our predictions to actual outcomes. Finally, we show how the model can be used to analyze counterfactual questions such as what type of restaurant would attract the most consumers in a given location.||||@arxiv||||2018/01/22||||Estimating Heterogeneous Consumer Preferences for Restaurants and...||||This paper analyzes consumer choices over lunchtime restaurants using data from a sample of several thousand anonymous mobile phone users in the San Francisco Bay Area. The data is used to...||||https://arxiv.org/abs/1801.07826v1||||cs||||
222||||None||||Generalized Beta Prime Distribution: Stochastic Model of Economic Exchange and Properties of Inequality Indices||||arXiv.org||||2019/06/11||||Generalized Beta Prime Distribution: Stochastic Model of Economic Exchange and Properties of Inequality Indices||||Moghaddam, M. Dashti || Mills, Jeffrey || Serota, R. A.||||https://arxiv.org/pdf/1906.04822||||1906.04822||||We argue that a stochastic model of economic exchange, whose steady-state distribution is a Generalized Beta Prime (also known as GB2), and some unique properties of the latter, are the reason for GB2's success in describing wealth/income distributions. We use housing sale prices as a proxy to wealth/income distribution to numerically illustrate this point. We also explore parametric limits of the distribution to do so analytically. We discuss parametric properties of the inequality indices -- Gini, Hoover, Theil T and Theil L -- vis-a-vis those of GB2 and introduce a new inequality index, which serves a similar purpose. We argue that Hoover and Theil L are more appropriate measures for distributions with power-law dependencies, especially fat tails, such as GB2.||||@arxiv||||2019/06/11||||Generalized Beta Prime Distribution: Stochastic Model of Economic...||||We argue that a stochastic model of economic exchange, whose steady-state distribution is a Generalized Beta Prime (also known as GB2), and some unique properties of the latter, are the reason for...||||https://arxiv.org/abs/1906.04822v1||||econ||||
223||||None||||How to Cut a Cake Fairly: A Generalization to Groups||||arXiv.org||||2020/01/10||||How to Cut a Cake Fairly: A Generalization to Groups||||Segal-Halevi, Erel || Suksompong, Warut||||https://arxiv.org/pdf/2001.03327||||2001.03327||||A fundamental result in cake cutting states that for any number of players with arbitrary preferences over a cake, there exists a division of the cake such that every player receives a single contiguous piece and no player is left envious. We generalize this result by showing that it is possible to partition the players into groups of any desired sizes and divide the cake among the groups, so that each group receives a single contiguous piece and no player finds the piece of another group better than that of the player's own group.||||@arxiv||||2020/01/10||||How to Cut a Cake Fairly: A Generalization to Groups||||A fundamental result in cake cutting states that for any number of players with arbitrary preferences over a cake, there exists a division of the cake such that every player receives a single...||||https://arxiv.org/abs/2001.03327v1||||econ||||
224||||None||||Loan maturity aggregation in interbank lending networks obscures mesoscale structure and economic functions||||arXiv.org||||2019/06/12||||Loan maturity aggregation in interbank lending networks obscures mesoscale structure and economic functions||||Van Soom, Marnix || Heuvel, Milan van den || Ryckebusch, Jan || Schoors, Koen||||https://arxiv.org/pdf/1906.08617||||1906.08617||||Since the 2007-2009 financial crisis, substantial academic effort has been dedicated to improving our understanding of interbank lending networks (ILNs). Because of data limitations or by choice, the literature largely lacks multiple loan maturities. We employ a complete interbank loan contract dataset to investigate whether maturity details are informative of the network structure. Applying the layered stochastic block model of Peixoto (2015) and other tools from network science on a time series of bilateral loans with multiple maturity layers in the Russian ILN, we find that collapsing all such layers consistently obscures mesoscale structure. The optimal maturity granularity lies between completely collapsing and completely separating the maturity layers and depends on the development phase of the interbank market, with a more developed market requiring more layers for optimal description. Closer inspection of the inferred maturity bins associated with the optimal maturity granularity reveals specific economic functions, from liquidity intermediation to financing. Collapsing a network with multiple underlying maturity layers or extracting one such layer, common in economic research, is therefore not only an incomplete representation of the ILN's mesoscale structure, but also conceals existing economic functions. This holds important insights and opportunities for theoretical and empirical studies on interbank market functioning, contagion, stability, and on the desirable level of regulatory data disclosure.||||@arxiv||||2019/06/12||||Loan maturity aggregation in interbank lending networks obscures...||||Since the 2007-2009 financial crisis, substantial academic effort has been dedicated to improving our understanding of interbank lending networks (ILNs). Because of data limitations or by choice,...||||https://arxiv.org/abs/1906.08617v1||||econ||||
225||||None||||Scoring Strategic Agents||||arXiv.org||||2019/11/08||||Scoring Strategic Agents||||Ball, Ian||||https://arxiv.org/pdf/1909.01888||||1909.01888||||I introduce a model of scoring. An intermediary aggregates multiple features of a sender into a score. A receiver sees this score and takes a decision. The receiver wants his decision to match the sender's latent characteristic. But the sender wants the most favorable decision, and she can distort each of her features at a private cost. I characterize the receiver-optimal scoring rule. This rule underweights some features to deter sender distortion, and overweights other features to keep the score unbiased. The receiver prefers this score to seeing the sender's full features because the coarser information mitigates his commitment problem.||||@arxiv||||2019/09/04||||Scoring Strategic Agents||||I introduce a model of scoring. An intermediary aggregates multiple features of a sender into a score. A receiver sees this score and takes a decision. The receiver wants his decision to match the...||||https://arxiv.org/abs/1909.01888v2||||econ||||
226||||None||||Technological Learning and Innovation Gestation Lags at the Frontier of Science: from CERN Procurement to Patent||||arXiv.org||||2019/05/23||||Technological Learning and Innovation Gestation Lags at the Frontier of Science: from CERN Procurement to Patent||||Bastianin, Andrea || Castelnovo, Paolo || Florio, Massimo || Giunta, Anna||||https://arxiv.org/pdf/1905.09552||||1905.09552||||This paper contributes to the literature on the impact of Big Science Centres on technological innovation. We exploit a unique dataset with information on CERN's procurement orders to study the collaborative innovation process between CERN and its industrial partners. After a qualitative discussion of case studies, survival and count data models are estimated; the impact of CERN procurement on suppliers' innovation is captured by the number of patent applications. The fact that firms in our sample received their first order over a long time span (1995-2008) delivers a natural partition of industrial partners into "suppliers" and "not yet suppliers". This allows estimating the impact of CERN on the hazard to file a patent for the first time and on the number of patent applications, as well as the time needed for these effects to show up. We find that a "CERN effect" does exist: being an industrial partner of CERN is associated with an increase in the hazard to file a patent for the first time and in the number of patent applications. These effects require a significant "gestation lag" in the range of five to eight years, pointing to a relatively slow process of absorption of new ideas.||||@arxiv||||2019/05/23||||Technological Learning and Innovation Gestation Lags at the...||||This paper contributes to the literature on the impact of Big Science Centres on technological innovation. We exploit a unique dataset with information on CERN's procurement orders to study the...||||https://arxiv.org/abs/1905.09552v1||||econ||||
227||||None||||Characterizing Shadow Price via Lagrangian Multiplier for Nonsmooth Problem||||arXiv.org||||2019/05/31||||Characterizing Shadow Price via Lagrangian Multiplier for Nonsmooth Problem||||Gao, Yan||||https://arxiv.org/pdf/1905.13622||||1905.13622||||In this paper, a relation between shadow price and the Lagrangian multiplier for nonsmooth problem is explored. It is shown that the Lagrangian Multiplier is the upper bound of shadow price for convex optimization and a class of Lipschtzian optimizations. This work can be used in shadow pricing for nonsmooth situation. The several nonsmooth functions involved in this class of Lipschtzian optimizations is listed. Finally, an application to electricity pricing is discussed.||||@arxiv||||2019/05/31||||Characterizing Shadow Price via Lagrangian Multiplier for Nonsmooth Problem||||In this paper, a relation between shadow price and the Lagrangian multiplier for nonsmooth problem is explored. It is shown that the Lagrangian Multiplier is the upper bound of shadow price for...||||https://arxiv.org/abs/1905.13622v1||||econ||||
228||||None||||Approximate State Space Modelling of Unobserved Fractional Components||||arXiv.org||||2019/03/04||||Approximate State Space Modelling of Unobserved Fractional Components||||Hartl, Tobias || Weigand, Roland||||https://arxiv.org/pdf/1812.09142||||1812.09142||||We propose convenient inferential methods for potentially nonstationary multivariate unobserved components models with fractional integration and cointegration. Based on finite-order ARMA approximations in the state space representation, maximum likelihood estimation can make use of the EM algorithm and related techniques. The approximation outperforms the frequently used autoregressive or moving average truncation, both in terms of computational costs and with respect to approximation quality. Monte Carlo simulations reveal good estimation properties of the proposed methods for processes of different complexity and dimension.||||@arxiv||||2018/12/21||||Approximate State Space Modelling of Unobserved Fractional Components||||We propose convenient inferential methods for potentially nonstationary multivariate unobserved components models with fractional integration and cointegration. Based on finite-order ARMA...||||https://arxiv.org/abs/1812.09142v2||||econ||||
229||||None||||Dynamic Competitive Persuasion||||arXiv.org||||2019/08/07||||Dynamic Competitive Persuasion||||Whitmeyer, Mark||||https://arxiv.org/pdf/1811.11664||||1811.11664||||We examine a dynamic game of competitive persuasion played between two long-lived sellers over $T \leq \infty$ periods. Each period, each seller provides information via a Blackwell experiment to a single short-lived buyer, who buys from the seller whose product has the highest expected quality. We solve for the unique subgame perfect equilibrium of this game, and conduct comparative statics: in particular we find that long horizons lead to less information.||||@arxiv||||2018/11/28||||Dynamic Competitive Persuasion||||We examine a dynamic game of competitive persuasion played between two long-lived sellers over $T \leq \infty$ periods. Each period, each seller provides information via a Blackwell experiment to...||||https://arxiv.org/abs/1811.11664v3||||cs||||
230||||None||||Supporting Crowd-Powered Science in Economics: FRACTI, a Conceptual Framework for Large-Scale Collaboration and Transparent Investigation in Financial Markets||||arXiv.org||||2018/08/23||||Supporting Crowd-Powered Science in Economics: FRACTI, a Conceptual Framework for Large-Scale Collaboration and Transparent Investigation in Financial Markets||||Faleiro, Jorge || Tsang, Edward||||https://arxiv.org/pdf/1808.07959||||1808.07959||||Modern investigation in economics and in other sciences requires the ability to store, share, and replicate results and methods of experiments that are often multidisciplinary and yield a massive amount of data. Given the increasing complexity and growing interaction across diverse bodies of knowledge it is becoming imperative to define a platform to properly support collaborative research and track origin, accuracy and use of data. This paper starts by defining a set of methods leveraging scientific principles and advocating the importance of those methods in multidisciplinary, computer intensive fields like computational finance. The next part of this paper defines a class of systems called scientific support systems, vis-a-vis usages in other research fields such as bioinformatics, physics and engineering. We outline a basic set of fundamental concepts, and list our goals and motivation for leveraging such systems to enable large-scale investigation, "crowd powered science", in economics. The core of this paper provides an outline of FRACTI in five steps. First we present definitions related to scientific support systems intrinsic to finance and describe common characteristics of financial use cases. The second step concentrates on what can be exchanged through the definition of shareable entities called contributions. The third step is the description of a classification system for building blocks of the conceptual framework, called facets. The fourth step introduces the meta-model that will enable provenance tracking and representation of data fragments and simulation. Finally we describe intended cases of use to highlight main strengths of FRACTI: application of the scientific method for investigation in computational finance, large-scale collaboration and simulation.||||@arxiv||||2018/08/23||||Supporting Crowd-Powered Science in Economics: FRACTI, a...||||Modern investigation in economics and in other sciences requires the ability to store, share, and replicate results and methods of experiments that are often multidisciplinary and yield a massive...||||https://arxiv.org/abs/1808.07959v1||||econ||||
231||||None||||Improved Density and Distribution Function Estimation||||arXiv.org||||2018/06/20||||Improved Density and Distribution Function Estimation||||Oryshchenko, Vitaliy || Smith, Richard J.||||https://arxiv.org/pdf/1711.04793||||1711.04793||||Given additional distributional information in the form of moment restrictions, kernel density and distribution function estimators with implied generalised empirical likelihood probabilities as weights achieve a reduction in variance due to the systematic use of this extra information. The particular interest here is the estimation of densities or distributions of (generalised) residuals in semi-parametric models defined by a finite number of moment restrictions. Such estimates are of great practical interest, being potentially of use for diagnostic purposes, including tests of parametric assumptions on an error distribution, goodness-of-fit tests or tests of overidentifying moment restrictions. The paper gives conditions for the consistency and describes the asymptotic mean squared error properties of the kernel density and distribution estimators proposed in the paper. A simulation study evaluates the small sample performance of these estimators. Supplements provide analytic examples to illustrate situations where kernel weighting provides a reduction in variance together with proofs of the results in the paper.||||@arxiv||||2017/11/13||||Improved Density and Distribution Function Estimation||||Given additional distributional information in the form of moment restrictions, kernel density and distribution function estimators with implied generalised empirical likelihood probabilities as...||||https://arxiv.org/abs/1711.04793v2||||econ||||
232||||None||||Bayesian state-space modeling for analyzing heterogeneous network effects of US monetary policy||||arXiv.org||||2020/02/04||||Bayesian state-space modeling for analyzing heterogeneous network effects of US monetary policy||||Hauzenberger, Niko || Pfarrhofer, Michael||||https://arxiv.org/pdf/1911.06206||||1911.06206||||Understanding disaggregate channels in the transmission of monetary policy to the real and financial sectors is of crucial importance for effectively implementing policy measures. We extend the empirical econometric literature on the role of production networks in the propagation of shocks along two dimensions. First, we set forth a Bayesian spatial panel state-space model that assumes time variation in the spatial dependence parameter, and apply the framework to a study of measuring network effects of US monetary policy on the industry level. Second, we account for cross-sectional heterogeneity and cluster impacts of monetary policy shocks to production industries via a sparse finite Gaussian mixture model. The results suggest substantial heterogeneities in the responses of industries to surprise monetary policy shocks. Moreover, we find that the role of network effects varies strongly over time. In particular, US recessions tend to coincide with periods where between 40 to 60 percent of the overall effects can be attributed to network effects; expansionary economic episodes show muted network effects with magnitudes of roughly 20 to 30 percent.||||@arxiv||||2019/11/14||||Bayesian state-space modeling for analyzing heterogeneous network...||||Understanding disaggregate channels in the transmission of monetary policy to the real and financial sectors is of crucial importance for effectively implementing policy measures. We extend the...||||https://arxiv.org/abs/1911.06206v2||||econ||||
233||||None||||Machine Learning for Dynamic Discrete Choice||||arXiv.org||||2018/11/06||||Machine Learning for Dynamic Discrete Choice||||Semenova, Vira||||https://arxiv.org/pdf/1808.02569||||1808.02569||||Dynamic discrete choice models often discretize the state vector and restrict its dimension in order to achieve valid inference. I propose a novel two-stage estimator for the set-identified structural parameter that incorporates a high-dimensional state space into the dynamic model of imperfect competition. In the first stage, I estimate the state variable's law of motion and the equilibrium policy function using machine learning tools. In the second stage, I plug the first-stage estimates into a moment inequality and solve for the structural parameter. The moment function is presented as the sum of two components, where the first one expresses the equilibrium assumption and the second one is a bias correction term that makes the sum insensitive (i.e., orthogonal) to first-stage bias. The proposed estimator uniformly converges at the root-N rate and I use it to construct confidence regions. The results developed here can be used to incorporate high-dimensional state space into classic dynamic discrete choice models, for example, those considered in Rust (1987), Bajari et al. (2007), and Scott (2013).||||@arxiv||||2018/08/07||||Machine Learning for Dynamic Discrete Choice||||Dynamic discrete choice models often discretize the state vector and restrict its dimension in order to achieve valid inference. I propose a novel two-stage estimator for the set-identified...||||https://arxiv.org/abs/1808.02569v2||||econ||||
234||||None||||Does agricultural subsidies foster Italian southern farms? A Spatial Quantile Regression Approach||||arXiv.org||||2018/03/15||||Does agricultural subsidies foster Italian southern farms? A Spatial Quantile Regression Approach||||De Castris, Marusca || Di Gennaro, Daniele||||https://arxiv.org/pdf/1803.05659||||1803.05659||||During the last decades, public policies become a central pillar in supporting and stabilising agricultural sector. In 1962, EU policy-makers developed the so-called Common Agricultural Policy (CAP) to ensure competitiveness and a common market organisation for agricultural products, while 2003 reform decouple the CAP from the production to focus only on income stabilization and the sustainability of agricultural sector. Notwithstanding farmers are highly dependent to public support, literature on the role played by the CAP in fostering agricultural performances is still scarce and fragmented. Actual CAP policies increases performance differentials between Northern Central EU countries and peripheral regions. This paper aims to evaluate the effectiveness of CAP in stimulate performances by focusing on Italian lagged Regions. Moreover, agricultural sector is deeply rooted in place-based production processes. In this sense, economic analysis which omit the presence of spatial dependence produce biased estimates of the performances. Therefore, this paper, using data on subsidies and economic results of farms from the RICA dataset which is part of the Farm Accountancy Data Network (FADN), proposes a spatial Augmented Cobb-Douglas Production Function to evaluate the effects of subsidies on farm's performances. The major innovation in this paper is the implementation of a micro-founded quantile version of a spatial lag model to examine how the impact of the subsidies may vary across the conditional distribution of agricultural performances. Results show an increasing shape which switch from negative to positive at the median and becomes statistical significant for higher quantiles. Additionally, spatial autocorrelation parameter is positive and significant across all the conditional distribution, suggesting the presence of significant spatial spillovers in agricultural performances.||||@arxiv||||2018/03/15||||Does agricultural subsidies foster Italian southern farms? A...||||During the last decades, public policies become a central pillar in supporting and stabilising agricultural sector. In 1962, EU policy-makers developed the so-called Common Agricultural Policy...||||https://arxiv.org/abs/1803.05659v1||||econ||||
235||||None||||Optimal Insurance with Limited Commitment in a Finite Horizon||||arXiv.org||||2019/01/11||||Optimal Insurance with Limited Commitment in a Finite Horizon||||Jeon, Junkee || Koo, Hyeng Keun || Park, Kyunghyun||||https://arxiv.org/pdf/1812.11669||||1812.11669||||We study a finite horizon optimal contracting problem of a risk-neutral principal and a risk-averse agent who receives a stochastic income stream when the agent is unable to make commitments. The problem involves an infinite number of constraints at each time and each state of the world. Miao and Zhang (2015) have developed a dual approach to the problem by considering a Lagrangian and derived a Hamilton-Jacobi-Bellman equation in an infinite horizon. We consider a similar Lagrangian in a finite horizon, but transform the dual problem into an infinite series of optimal stopping problems. For each optimal stopping problem we provide an analytic solution by providing an integral equation representation for the free boundary. We provide a verification theorem that the value function of the original principal's problem is the Legender-Fenchel transform of the integral of the value functions of the optimal stopping problems. We also provide some numerical simulation results of optimal contracting strategies||||@arxiv||||2018/12/31||||Optimal Insurance with Limited Commitment in a Finite Horizon||||We study a finite horizon optimal contracting problem of a risk-neutral principal and a risk-averse agent who receives a stochastic income stream when the agent is unable to make commitments. The...||||https://arxiv.org/abs/1812.11669v3||||econ||||
236||||None||||Deep Neural Networks for Estimation and Inference||||arXiv.org||||2019/09/18||||Deep Neural Networks for Estimation and Inference||||Farrell, Max H. || Liang, Tengyuan || Misra, Sanjog||||https://arxiv.org/pdf/1809.09953||||1809.09953||||We study deep neural networks and their use in semiparametric inference. We establish novel rates of convergence for deep feedforward neural nets. Our new rates are sufficiently fast (in some cases minimax optimal) to allow us to establish valid second-step inference after first-step estimation with deep learning, a result also new to the literature. Our estimation rates and semiparametric inference results handle the current standard architecture: fully connected feedforward neural networks (multi-layer perceptrons), with the now-common rectified linear unit activation function and a depth explicitly diverging with the sample size. We discuss other architectures as well, including fixed-width, very deep networks. We establish nonasymptotic bounds for these deep nets for a general class of nonparametric regression-type loss functions, which includes as special cases least squares, logistic regression, and other generalized linear models. We then apply our theory to develop semiparametric inference, focusing on causal parameters for concreteness, such as treatment effects, expected welfare, and decomposition effects. Inference in many other semiparametric contexts can be readily obtained. We demonstrate the effectiveness of deep learning with a Monte Carlo analysis and an empirical application to direct mail marketing.||||@arxiv||||2018/09/26||||Deep Neural Networks for Estimation and Inference||||We study deep neural networks and their use in semiparametric inference. We establish novel rates of convergence for deep feedforward neural nets. Our new rates are sufficiently fast (in some...||||https://arxiv.org/abs/1809.09953v3||||cs||||
237||||None||||Smoothed GMM for quantile models||||arXiv.org||||2018/02/27||||Smoothed GMM for quantile models||||de Castro, Luciano || Galvao, Antonio F. || Kaplan, David M. || Liu, Xin||||https://arxiv.org/pdf/1707.03436||||1707.03436||||This paper develops theory for feasible estimators of finite-dimensional parameters identified by general conditional quantile restrictions, under much weaker assumptions than previously seen in the literature. This includes instrumental variables nonlinear quantile regression as a special case. More specifically, we consider a set of unconditional moments implied by the conditional quantile restrictions, providing conditions for local identification. Since estimators based on the sample moments are generally impossible to compute numerically in practice, we study feasible estimators based on smoothed sample moments. We propose a method of moments estimator for exactly identified models, as well as a generalized method of moments estimator for over-identified models. We establish consistency and asymptotic normality of both estimators under general conditions that allow for weakly dependent data and nonlinear structural models. Simulations illustrate the finite-sample properties of the methods. Our in-depth empirical application concerns the consumption Euler equation derived from quantile utility maximization. Advantages of the quantile Euler equation include robustness to fat tails, decoupling of risk attitude from the elasticity of intertemporal substitution, and log-linearization without any approximation error. For the four countries we examine, the quantile estimates of discount factor and elasticity of intertemporal substitution are economically reasonable for a range of quantiles above the median, even when two-stage least squares estimates are not reasonable.||||@arxiv||||2017/07/11||||Smoothed GMM for quantile models||||This paper develops theory for feasible estimators of finite-dimensional parameters identified by general conditional quantile restrictions, under much weaker assumptions than previously seen in...||||https://arxiv.org/abs/1707.03436v2||||econ||||
238||||None||||Modelling Social Evolutionary Processes and Peer Effects in Agricultural Trade Networks: the Rubber Value Chain in Indonesia||||arXiv.org||||2018/11/28||||Modelling Social Evolutionary Processes and Peer Effects in Agricultural Trade Networks: the Rubber Value Chain in Indonesia||||Kopp, Thomas || Salecker, Jan||||https://arxiv.org/pdf/1811.11476||||1811.11476||||Understanding market participants' channel choices is important to policy makers because it yields information on which channels are effective in transmitting information. These channel choices are the result of a recursive process of social interactions and determine the observable trading networks. They are characterized by feedback mechanisms due to peer interaction and therefore need to be understood as complex adaptive systems (CAS). When modelling CAS, conventional approaches like regression analyses face severe drawbacks since endogeneity is omnipresent. As an alternative, process-based analyses allow researchers to capture these endogenous processes and multiple feedback loops. This paper applies an agent-based modelling approach (ABM) to the empirical example of the Indonesian rubber trade. The feedback mechanisms are modelled via an innovative approach of a social matrix, which allows decisions made in a specific period to feed back into the decision processes in subsequent periods, and allows agents to systematically assign different weights to the decision parameters based on their individual characteristics. In the validation against the observed network, uncertainty in the found estimates, as well as under determination of the model, are dealt with via an approach of evolutionary calibration: a genetic algorithm finds the combination of parameters that maximizes the similarity between the simulated and the observed network. Results indicate that the sellers' channel choice decisions are mostly driven by physical distance and debt obligations, as well as peer-interaction. Within the social matrix, the most influential individuals are sellers that live close by to other traders, are active in social groups and belong to the ethnic majority in their village.||||@arxiv||||2018/11/28||||Modelling Social Evolutionary Processes and Peer Effects in...||||Understanding market participants' channel choices is important to policy makers because it yields information on which channels are effective in transmitting information. These channel choices...||||https://arxiv.org/abs/1811.11476v1||||econ||||
239||||None||||Penalized Sieve GEL for Weighted Average Derivatives of Nonparametric Quantile IV Regressions||||arXiv.org||||2019/02/26||||Penalized Sieve GEL for Weighted Average Derivatives of Nonparametric Quantile IV Regressions||||Chen, Xiaohong || Pouzo, Demian || Powell, James L.||||https://arxiv.org/pdf/1902.10100||||1902.10100||||This paper considers estimation and inference for a weighted average derivative (WAD) of a nonparametric quantile instrumental variables regression (NPQIV). NPQIV is a non-separable and nonlinear ill-posed inverse problem, which might be why there is no published work on the asymptotic properties of any estimator of its WAD. We first characterize the semiparametric efficiency bound for a WAD of a NPQIV, which, unfortunately, depends on an unknown conditional derivative operator and hence an unknown degree of ill-posedness, making it difficult to know if the information bound is singular or not. In either case, we propose a penalized sieve generalized empirical likelihood (GEL) estimation and inference procedure, which is based on the unconditional WAD moment restriction and an increasing number of unconditional moments that are implied by the conditional NPQIV restriction, where the unknown quantile function is approximated by a penalized sieve. Under some regularity conditions, we show that the self-normalized penalized sieve GEL estimator of the WAD of a NPQIV is asymptotically standard normal. We also show that the quasi likelihood ratio statistic based on the penalized sieve GEL criterion is asymptotically chi-square distributed regardless of whether or not the information bound is singular.||||@arxiv||||2019/02/26||||Penalized Sieve GEL for Weighted Average Derivatives of...||||This paper considers estimation and inference for a weighted average derivative (WAD) of a nonparametric quantile instrumental variables regression (NPQIV). NPQIV is a non-separable and nonlinear...||||https://arxiv.org/abs/1902.10100v1||||econ||||
240||||None||||ppmlhdfe: Fast Poisson Estimation with High-Dimensional Fixed Effects||||arXiv.org||||2019/08/02||||ppmlhdfe: Fast Poisson Estimation with High-Dimensional Fixed Effects||||Correia, Sergio || Guimarães, Paulo || Zylkin, Thomas||||https://arxiv.org/pdf/1903.01690||||1903.01690||||In this paper we present ppmlhdfe, a new Stata command for estimation of (pseudo) Poisson regression models with multiple high-dimensional fixed effects (HDFE). Estimation is implemented using a modified version of the iteratively reweighted least-squares (IRLS) algorithm that allows for fast estimation in the presence of HDFE. Because the code is built around the reghdfe package, it has similar syntax, supports many of the same functionalities, and benefits from reghdfe's fast convergence properties for computing high-dimensional least squares problems.   Performance is further enhanced by some new techniques we introduce for accelerating HDFE-IRLS estimation specifically. ppmlhdfe also implements a novel and more robust approach to check for the existence of (pseudo) maximum likelihood estimates.||||@arxiv||||2019/03/05||||ppmlhdfe: Fast Poisson Estimation with High-Dimensional Fixed Effects||||In this paper we present ppmlhdfe, a new Stata command for estimation of (pseudo) Poisson regression models with multiple high-dimensional fixed effects (HDFE). Estimation is implemented using a...||||https://arxiv.org/abs/1903.01690v3||||econ||||
241||||None||||Understanding the Great Recession Using Machine Learning Algorithms||||arXiv.org||||2020/01/02||||Understanding the Great Recession Using Machine Learning Algorithms||||Nyman, Rickard || Ormerod, Paul||||https://arxiv.org/pdf/2001.02115||||2001.02115||||Nyman and Ormerod (2017) show that the machine learning technique of random forests has the potential to give early warning of recessions. Applying the approach to a small set of financial variables and replicating as far as possible a genuine ex ante forecasting situation, over the period since 1990 the accuracy of the four-step ahead predictions is distinctly superior to those actually made by the professional forecasters. Here we extend the analysis by examining the contributions made to the Great Recession of the late 2000s by each of the explanatory variables. We disaggregate private sector debt into its household and non-financial corporate components. We find that both household and non-financial corporate debt were key determinants of the Great Recession. We find a considerable degree of non-linearity in the explanatory models. In contrast, the public sector debt to GDP ratio appears to have made very little contribution. It did rise sharply during the Great Recession, but this was as a consequence of the sharp fall in economic activity rather than it being a cause. We obtain similar results for both the United States and the United Kingdom.||||@arxiv||||2020/01/02||||Understanding the Great Recession Using Machine Learning Algorithms||||Nyman and Ormerod (2017) show that the machine learning technique of random forests has the potential to give early warning of recessions. Applying the approach to a small set of financial...||||https://arxiv.org/abs/2001.02115v1||||econ||||
242||||None||||The network paradigm as a modeling tool in regional economy: the case of interregional commuting in Greece||||arXiv.org||||2020/01/27||||The network paradigm as a modeling tool in regional economy: the case of interregional commuting in Greece||||Tsiotas, Dimitrios || Sdrolias, Labros || Belias, Dimitrios||||https://arxiv.org/pdf/2001.09664||||2001.09664||||Network Science is an emerging discipline using the network paradigm to model communication systems as pair-sets of interconnected nodes and their linkages (edges). This paper applies this paradigm to study an interacting system in regional economy consisting of daily road transportation flows for labor purposes, the so-called commuting phenomenon. In particular, the commuting system in Greece including 39 non-insular prefectures is modeled into a complex network and it is studied using measures and methods of complex network analysis and empirical techniques. The study aims to detect the structural characteristics of the Greek interregional commuting network (GCN) and to interpret how this network is related to the regional development. The analysis highlights the effect of the spatial constraints in the structure of the GCN, it provides insights about the major road transport projects constructed the last decade, and it outlines a populationcontrolled (gravity) pattern of commuting, illustrating that high-populated regions attract larger volumes of the commuting activity, which consequently affects their productivity. Overall, this paper highlights the effectiveness of complex network analysis in the modeling of systems of regional economy, such as the systems of spatial interaction and the transportation networks, and it promotes the use of the network paradigm to the regional research.||||@arxiv||||2020/01/27||||The network paradigm as a modeling tool in regional economy: the...||||Network Science is an emerging discipline using the network paradigm to model communication systems as pair-sets of interconnected nodes and their linkages (edges). This paper applies this...||||https://arxiv.org/abs/2001.09664v1||||econ||||
243||||None||||Contract Design with Costly Convex Self-Control||||arXiv.org||||2019/07/17||||Contract Design with Costly Convex Self-Control||||Masatlioglu, Yusufcan || Nakajima, Daisuke || Ozdenoren, Emre||||https://arxiv.org/pdf/1907.07628||||1907.07628||||In this note, we consider the pricing problem of a profit-maximizing monopolist who faces naive consumers with convex self-control preferences.||||@arxiv||||2019/07/17||||Contract Design with Costly Convex Self-Control||||In this note, we consider the pricing problem of a profit-maximizing monopolist who faces naive consumers with convex self-control preferences.||||https://arxiv.org/abs/1907.07628v1||||econ||||
244||||None||||Data-Driven Investment Decision-Making: Applying Moore's Law and S-Curves to Business Strategies||||arXiv.org||||2018/05/16||||Data-Driven Investment Decision-Making: Applying Moore's Law and S-Curves to Business Strategies||||Benson, Christopher L. || Magee, Christopher L.||||https://arxiv.org/pdf/1805.06339||||1805.06339||||This paper introduces a method for linking technological improvement rates (i.e. Moore's Law) and technology adoption curves (i.e. S-Curves). There has been considerable research surrounding Moore's Law and the generalized versions applied to the time dependence of performance for other technologies. The prior work has culminated with methodology for quantitative estimation of technological improvement rates for nearly any technology. This paper examines the implications of such regular time dependence for performance upon the timing of key events in the technological adoption process. We propose a simple crossover point in performance which is based upon the technological improvement rates and current level differences for target and replacement technologies. The timing for the cross-over is hypothesized as corresponding to the first 'knee'? in the technology adoption "S-curve" and signals when the market for a given technology will start to be rewarding for innovators. This is also when potential entrants are likely to intensely experiment with product-market fit and when the competition to achieve a dominant design begins. This conceptual framework is then back-tested by examining two technological changes brought about by the internet, namely music and video transmission. The uncertainty analysis around the cases highlight opportunities for organizations to reduce future technological uncertainty. Overall, the results from the case studies support the reliability and utility of the conceptual framework in strategic business decision-making with the caveat that while technical uncertainty is reduced, it is not eliminated.||||@arxiv||||2018/05/16||||Data-Driven Investment Decision-Making: Applying Moore's Law...||||This paper introduces a method for linking technological improvement rates (i.e. Moore's Law) and technology adoption curves (i.e. S-Curves). There has been considerable research surrounding...||||https://arxiv.org/abs/1805.06339v1||||econ||||
245||||None||||Arrow, Hausdorff, and Ambiguities in the Choice of Preferred States in Complex Systems||||arXiv.org||||2019/09/10||||Arrow, Hausdorff, and Ambiguities in the Choice of Preferred States in Complex Systems||||Erber, T. || Frank, M. J.||||https://arxiv.org/pdf/1909.07771||||1909.07771||||Arrow's `impossibility' theorem asserts that there are no satisfactory methods of aggregating individual preferences into collective preferences in many complex situations. This result has ramifications in economics, politics, i.e., the theory of voting, and the structure of tournaments. By identifying the objects of choice with mathematical sets, and preferences with Hausdorff measures of the distances between sets, it is possible to extend Arrow's arguments from a sociological to a mathematical setting. One consequence is that notions of reversibility can be expressed in terms of the relative configurations of patterns of sets.||||@arxiv||||2019/09/10||||Arrow, Hausdorff, and Ambiguities in the Choice of Preferred...||||Arrow's `impossibility' theorem asserts that there are no satisfactory methods of aggregating individual preferences into collective preferences in many complex situations. This result has...||||https://arxiv.org/abs/1909.07771v1||||cs||||
246||||None||||The Broad Consequences of Narrow Banking||||arXiv.org||||2018/10/12||||The Broad Consequences of Narrow Banking||||Grasselli, Matheus R || Lipton, Alexander||||https://arxiv.org/pdf/1810.05689||||1810.05689||||We investigate the macroeconomic consequences of narrow banking in the context of stock-flow consistent models. We begin with an extension of the Goodwin-Keen model incorporating time deposits, government bills, cash, and central bank reserves to the base model with loans and demand deposits and use it to describe a fractional reserve banking system. We then characterize narrow banking by a full reserve requirement on demand deposits and describe the resulting separation between the payment system and lending functions of the resulting banking sector. By way of numerical examples, we explore the properties of fractional and full reserve versions of the model and compare their asymptotic properties. We find that narrow banking does not lead to any loss in economic growth when the models converge to a finite equilibrium, while allowing for more direct monitoring and prevention of financial breakdowns in the case of explosive asymptotic behaviour.||||@arxiv||||2018/10/12||||The Broad Consequences of Narrow Banking||||We investigate the macroeconomic consequences of narrow banking in the context of stock-flow consistent models. We begin with an extension of the Goodwin-Keen model incorporating time deposits,...||||https://arxiv.org/abs/1810.05689v1||||econ||||
247||||None||||Racial Disparities in Voting Wait Times: Evidence from Smartphone Data||||arXiv.org||||2019/08/30||||Racial Disparities in Voting Wait Times: Evidence from Smartphone Data||||Chen, M. Keith || Haggag, Kareem || Pope, Devin G. || Rohla, Ryne||||https://arxiv.org/pdf/1909.00024||||1909.00024||||Equal access to voting is a core feature of democratic government. Using data from millions of smartphone users, we quantify a racial disparity in voting wait times across a nationwide sample of polling places during the 2016 US presidential election. Relative to entirely-white neighborhoods, residents of entirely-black neighborhoods waited 29% longer to vote and were 74% more likely to spend more than 30 minutes at their polling place. This disparity holds when comparing predominantly white and black polling places within the same states and counties, and survives numerous robustness and placebo tests. Our results document large racial differences in voting wait times and demonstrates that geospatial data can be an effective tool to both measure and monitor these disparities.||||@arxiv||||2019/08/30||||Racial Disparities in Voting Wait Times: Evidence from Smartphone Data||||Equal access to voting is a core feature of democratic government. Using data from millions of smartphone users, we quantify a racial disparity in voting wait times across a nationwide sample of...||||https://arxiv.org/abs/1909.00024v1||||econ||||
248||||None||||Pathways to Good Healthcare Services and Patient Satisfaction: An Evolutionary Game Theoretical Approach||||arXiv.org||||2019/07/06||||Pathways to Good Healthcare Services and Patient Satisfaction: An Evolutionary Game Theoretical Approach||||Alalawi, Zainab || Han, The Anh || Zeng, Yifeng || Elragig, Aiman||||https://arxiv.org/pdf/1907.07132||||1907.07132||||Spending by the UK's National Health Service (NHS) on independent healthcare treatment has been increased in recent years and is predicted to sustain its upward trend with the forecast of population growth. Some have viewed this increase as an attempt not to expand the patients' choices but to privatize public healthcare. This debate poses a social dilemma whether the NHS should stop cooperating with Private providers. This paper contributes to healthcare economic modelling by investigating the evolution of cooperation among three proposed populations: Public Healthcare Providers, Private Healthcare Providers and Patients. The Patient population is included as a main player in the decision-making process by expanding patient's choices of treatment. We develop a generic basic model that measures the cost of healthcare provision based on given parameters, such as NHS and private healthcare providers' cost of investments in both sectors, cost of treatments and gained benefits. A patient's costly punishment is introduced as a mechanism to enhance cooperation among the three populations. Our findings show that cooperation can be improved with the introduction of punishment (patient's punishment) against defecting providers. Although punishment increases cooperation, it is very costly considering the small improvement in cooperation in comparison to the basic model.||||@arxiv||||2019/07/06||||Pathways to Good Healthcare Services and Patient Satisfaction: An...||||Spending by the UK's National Health Service (NHS) on independent healthcare treatment has been increased in recent years and is predicted to sustain its upward trend with the forecast of...||||https://arxiv.org/abs/1907.07132v1||||cs||||
249||||None||||Limits to green growth and the dynamics of innovation||||arXiv.org||||2019/05/03||||Limits to green growth and the dynamics of innovation||||Pueyo, Salvador||||https://arxiv.org/pdf/1904.09586||||1904.09586||||Central to the official "green growth" discourse is the conjecture that absolute decoupling can be achieved with certain market instruments. This paper evaluates this claim focusing on the role of technology, while changes in GDP composition are treated elsewhere. Some fundamental difficulties for absolute decoupling, referring specifically to thermodynamic costs, are identified through a stylized model based on empirical knowledge on innovation and learning. Normally, monetary costs decrease more slowly than production grows, and this is unlikely to change should monetary costs align with thermodynamic costs, except, potentially, in the transition after the price reform. Furthermore, thermodynamic efficiency must eventually saturate for physical reasons. While this model, as usual, introduces technological innovation just as a source of efficiency, innovation also creates challenges: therefore, attempts to sustain growth by ever-accelerating innovation collide also with the limited reaction capacity of people and institutions. Information technology could disrupt innovation dynamics in the future, permitting quicker gains in eco-efficiency, but only up to saturation and exacerbating the downsides of innovation. These observations suggest that long-term sustainability requires much deeper transformations than the green growth discourse presumes, exposing the need to rethink scales, tempos and institutions, in line with ecological economics and the degrowth literature.||||@arxiv||||2019/04/21||||Limits to green growth and the dynamics of innovation||||Central to the official "green growth" discourse is the conjecture that absolute decoupling can be achieved with certain market instruments. This paper evaluates this claim focusing on the role of...||||https://arxiv.org/abs/1904.09586v2||||econ||||
250||||None||||A Higher-Order Correct Fast Moving-Average Bootstrap for Dependent Data||||arXiv.org||||2020/01/14||||A Higher-Order Correct Fast Moving-Average Bootstrap for Dependent Data||||La Vecchia, Davide || Moor, Alban || Scaillet, Olivier||||https://arxiv.org/pdf/2001.04867||||2001.04867||||We develop and implement a novel fast bootstrap for dependent data. Our scheme is based on the i.i.d. resampling of the smoothed moment indicators. We characterize the class of parametric and semi-parametric estimation problems for which the method is valid. We show the asymptotic refinements of the proposed procedure, proving that it is higher-order correct under mild assumptions on the time series, the estimating functions, and the smoothing kernel. We illustrate the applicability and the advantages of our procedure for Generalized Empirical Likelihood estimation. As a by-product, our fast bootstrap provides higher-order correct asymptotic confidence distributions. Monte Carlo simulations on an autoregressive conditional duration model provide numerical evidence that the novel bootstrap yields higher-order accurate confidence intervals. A real-data application on dynamics of trading volume of stocks illustrates the advantage of our method over the routinely-applied first-order asymptotic theory, when the underlying distribution of the test statistic is skewed or fat-tailed.||||@arxiv||||2020/01/14||||A Higher-Order Correct Fast Moving-Average Bootstrap for Dependent Data||||We develop and implement a novel fast bootstrap for dependent data. Our scheme is based on the i.i.d. resampling of the smoothed moment indicators. We characterize the class of parametric and...||||https://arxiv.org/abs/2001.04867v1||||econ||||
251||||None||||Semi-parametric Realized Nonlinear Conditional Autoregressive Expectile and Expected Shortfall||||arXiv.org||||2019/06/21||||Semi-parametric Realized Nonlinear Conditional Autoregressive Expectile and Expected Shortfall||||Wang, Chao || Gerlach, Richard||||https://arxiv.org/pdf/1906.09961||||1906.09961||||A joint conditional autoregressive expectile and Expected Shortfall framework is proposed. The framework is extended through incorporating a measurement equation which models the contemporaneous dependence between the realized measures and the latent conditional expectile. Nonlinear threshold specification is further incorporated into the proposed framework. A Bayesian Markov Chain Monte Carlo method is adapted for estimation, whose properties are assessed and compared with maximum likelihood via a simulation study. One-day-ahead VaR and ES forecasting studies, with seven market indices, provide empirical support to the proposed models.||||@arxiv||||2019/06/21||||Semi-parametric Realized Nonlinear Conditional Autoregressive...||||A joint conditional autoregressive expectile and Expected Shortfall framework is proposed. The framework is extended through incorporating a measurement equation which models the contemporaneous...||||https://arxiv.org/abs/1906.09961v1||||econ||||
252||||None||||The Price of BitCoin: GARCH Evidence from High Frequency Data||||arXiv.org||||2018/12/22||||The Price of BitCoin: GARCH Evidence from High Frequency Data||||Ciaian, Pavel || Kancs, d'Artis || Rajcaniova, Miroslava||||https://arxiv.org/pdf/1812.09452||||1812.09452||||This is the first paper that estimates the price determinants of BitCoin in a Generalised Autoregressive Conditional Heteroscedasticity framework using high frequency data. Derived from a theoretical model, we estimate BitCoin transaction demand and speculative demand equations in a GARCH framework using hourly data for the period 2013-2018. In line with the theoretical model, our empirical results confirm that both the BitCoin transaction demand and speculative demand have a statistically significant impact on the BitCoin price formation. The BitCoin price responds negatively to the BitCoin velocity, whereas positive shocks to the BitCoin stock, interest rate and the size of the BitCoin economy exercise an upward pressure on the BitCoin price.||||@arxiv||||2018/12/22||||The Price of BitCoin: GARCH Evidence from High Frequency Data||||This is the first paper that estimates the price determinants of BitCoin in a Generalised Autoregressive Conditional Heteroscedasticity framework using high frequency data. Derived from a...||||https://arxiv.org/abs/1812.09452v1||||econ||||
253||||None||||Dependencies and systemic risk in the European insurance sector: Some new evidence based on copula-DCC-GARCH model and selected clustering methods||||arXiv.org||||2019/05/08||||Dependencies and systemic risk in the European insurance sector: Some new evidence based on copula-DCC-GARCH model and selected clustering methods||||Denkowska, Anna || Wanat, Stanisław||||https://arxiv.org/pdf/1905.03273||||1905.03273||||The subject of the present article is the study of correlations between large insurance companies and their contribution to systemic risk in the insurance sector. Our main goal is to analyze the conditional structure of the correlation on the European insurance market and to compare systemic risk in different regimes of this market. These regimes are identified by monitoring the weekly rates of returns of eight of the largest insurers (five from Europe and the biggest insurers from the USA, Canada and China) during the period January 2005 to December 2018. To this aim we use statistical clustering methods for time units (weeks) to which we assigned the conditional variances obtained from the estimated copula-DCC-GARCH model. The advantage of such an approach is that there is no need to assume a priori a number of market regimes, since this number has been identified by means of clustering quality validation. In each of the identified market regimes we determined the commonly now used CoVaR systemic risk measure. From the performed analysis we conclude that all the considered insurance companies are positively correlated and this correlation is stronger in times of turbulences on global markets which shows an increased exposure of the European insurance sector to systemic risk during crisis. Moreover, in times of turbulences on global markets the value level of the CoVaR systemic risk index is much higher than in "normal conditions".||||@arxiv||||2019/05/08||||Dependencies and systemic risk in the European insurance sector:...||||The subject of the present article is the study of correlations between large insurance companies and their contribution to systemic risk in the insurance sector. Our main goal is to analyze the...||||https://arxiv.org/abs/1905.03273v1||||econ||||
254||||None||||A multi-scale symmetry analysis of uninterrupted trends returns of daily financial indices||||arXiv.org||||2019/08/27||||A multi-scale symmetry analysis of uninterrupted trends returns of daily financial indices||||Rodríguez-Martínez, C. M. || Coronel-Brizio, H. F. || Hernández-Montoya, A. R.||||https://arxiv.org/pdf/1908.11204||||1908.11204||||We present a symmetry analysis of the distribution of variations of different financial indices, by means of a statistical procedure developed by the authors based on a symmetry statistic by Einmahl and Mckeague. We applied this statistical methodology to financial uninterrupted daily trends returns and to other derived observable. In our opinion, to study distributional symmetry, trends returns offer more advantages than the commonly used daily financial returns; the two most important being: 1) Trends returns involve sampling over different time scales and 2) By construction, this variable time series contains practically the same number of non-negative and negative entry values. We also show that these time multi-scale returns display distributional bi-modality. Daily financial indices analyzed in this work, are the Mexican IPC, the American DJIA, DAX from Germany and the Japanese Market index Nikkei, covering a time period from 11-08-1991 to 06-30-2017. We show that, at the time scale resolution and significance considered in this paper, it is almost always feasible to find an interval of possible symmetry points containing one most plausible symmetry point denoted by C. Finally, we study the temporal evolution of C showing that this point is seldom zero and responds with sensitivity to extreme market events.||||@arxiv||||2019/08/27||||A multi-scale symmetry analysis of uninterrupted trends returns of...||||We present a symmetry analysis of the distribution of variations of different financial indices, by means of a statistical procedure developed by the authors based on a symmetry statistic by...||||https://arxiv.org/abs/1908.11204v1||||econ||||
255||||None||||A simple way to assess inference methods||||arXiv.org||||2019/12/30||||A simple way to assess inference methods||||Ferman, Bruno||||https://arxiv.org/pdf/1912.08772||||1912.08772||||We propose a simple way to assess the quality of asymptotic approximations required for inference methods. Our assessment can detect problems when the asymptotic theory that justifies the inference method is invalid and/or the structure of the empirical application is far from "Asymptopia". Our assessment can be easily applied to a wide range of applications. We illustrate the use of our assessment for the case of stratified randomized experiments.||||@arxiv||||2019/12/18||||A simple way to assess inference methods||||We propose a simple way to assess the quality of asymptotic approximations required for inference methods. Our assessment can detect problems when the asymptotic theory that justifies the...||||https://arxiv.org/abs/1912.08772v2||||econ||||
256||||None||||Uncertainty and Robustness of Surplus Extraction||||arXiv.org||||2018/11/04||||Uncertainty and Robustness of Surplus Extraction||||Lopomo, Giuseppe || Rigotti, Luca || Shannon, Chris||||https://arxiv.org/pdf/1811.01320||||1811.01320||||This paper studies a robust version of the classic surplus extraction problem, in which the designer knows only that the beliefs of each type belong to some set, and designs mechanisms that are suitable for all possible beliefs in that set. We derive necessary and sufficient conditions for full extraction in this setting, and show that these are natural set-valued analogues of the classic convex independence condition identified by Cremer and McLean (1985, 1988). We show that full extraction is neither generically possible nor generically impossible, in contrast to the standard setting in which full extraction is generic. When full extraction fails, we show that natural additional conditions can restrict both the nature of the contracts a designer can offer and the surplus the designer can obtain.||||@arxiv||||2018/11/04||||Uncertainty and Robustness of Surplus Extraction||||This paper studies a robust version of the classic surplus extraction problem, in which the designer knows only that the beliefs of each type belong to some set, and designs mechanisms that are...||||https://arxiv.org/abs/1811.01320v1||||econ||||
257||||None||||Robust Inference Using Inverse Probability Weighting||||arXiv.org||||2019/05/24||||Robust Inference Using Inverse Probability Weighting||||Ma, Xinwei || Wang, Jingshen||||https://arxiv.org/pdf/1810.11397||||1810.11397||||Inverse Probability Weighting (IPW) is widely used in empirical work in economics and other disciplines. As Gaussian approximations perform poorly in the presence of "small denominators," trimming is routinely employed as a regularization strategy. However, ad hoc trimming of the observations renders usual inference procedures invalid for the target estimand, even in large samples. In this paper, we first show that the IPW estimator can have different (Gaussian or non-Gaussian) asymptotic distributions, depending on how "close to zero" the probability weights are and on how large the trimming threshold is. As a remedy, we propose an inference procedure that is robust not only to small probability weights entering the IPW estimator but also to a wide range of trimming threshold choices, by adapting to these different asymptotic distributions. This robustness is achieved by employing resampling techniques and by correcting a non-negligible trimming bias. We also propose an easy-to-implement method for choosing the trimming threshold by minimizing an empirical analogue of the asymptotic mean squared error. In addition, we show that our inference procedure remains valid with the use of a data-driven trimming threshold. We illustrate our method by revisiting a dataset from the National Supported Work program.||||@arxiv||||2018/10/26||||Robust Inference Using Inverse Probability Weighting||||Inverse Probability Weighting (IPW) is widely used in empirical work in economics and other disciplines. As Gaussian approximations perform poorly in the presence of "small denominators," trimming...||||https://arxiv.org/abs/1810.11397v2||||econ||||
258||||None||||Generalized Log-Normal Chain-Ladder||||arXiv.org||||2018/06/15||||Generalized Log-Normal Chain-Ladder||||Kuang, D. || Nielsen, B.||||https://arxiv.org/pdf/1806.05939||||1806.05939||||We propose an asymptotic theory for distribution forecasting from the log normal chain-ladder model. The theory overcomes the difficulty of convoluting log normal variables and takes estimation error into account. The results differ from that of the over-dispersed Poisson model and from the chain-ladder based bootstrap. We embed the log normal chain-ladder model in a class of infinitely divisible distributions called the generalized log normal chain-ladder model. The asymptotic theory uses small $σ$ asymptotics where the dimension of the reserving triangle is kept fixed while the standard deviation is assumed to decrease. The resulting asymptotic forecast distributions follow t distributions. The theory is supported by simulations and an empirical application.||||@arxiv||||2018/06/15||||Generalized Log-Normal Chain-Ladder||||We propose an asymptotic theory for distribution forecasting from the log normal chain-ladder model. The theory overcomes the difficulty of convoluting log normal variables and takes estimation...||||https://arxiv.org/abs/1806.05939v1||||econ||||
259||||None||||Strategic research funding||||arXiv.org||||2019/03/21||||Strategic research funding||||Escudé, Matteo || Sinander, Ludvig||||https://arxiv.org/pdf/1903.09055||||1903.09055||||We study a dynamic game in which information arrives gradually as long as a principal funds research, and an agent takes an action in each period. In equilibrium, the principal's patience is the key determinant of her information provision: the lower her discount rate, the more eagerly she funds. When she is sufficiently patient, her information provision and value function are well-approximated by the 'Bayesian persuasion' model. If the conflict of interest is purely belief-based and information is valuable, then she provides full information if she is patient. We also obtain a sharp characterisation of the principal's value function. Our proofs rely on a novel dynamic programming principle rooted in the theory of viscosity solutions of differential equations.||||@arxiv||||2019/03/21||||Strategic research funding||||We study a dynamic game in which information arrives gradually as long as a principal funds research, and an agent takes an action in each period. In equilibrium, the principal's patience is the...||||https://arxiv.org/abs/1903.09055v1||||econ||||
260||||None||||Rational hyperbolic discounting||||arXiv.org||||2019/10/11||||Rational hyperbolic discounting||||Nascimento, José Cláudio do||||https://arxiv.org/pdf/1910.05209||||1910.05209||||This paper shows that the $q$-exponential function rationally evaluate the time discounting. When we consider two processes of wealth accumulation with different frequencies, then the discount rate and the relative frequency between them are essentials to choose the best process. In this context, the exponential discounting is a particular case, where one of the processes has a much higher frequency related to the other. In addition, one can note that some behaviors observed empirically in decision makers, such as subadditivity, magnitude effect, and preference reversal, are consistent with processes which have a low relative frequency.||||@arxiv||||2019/10/11||||Rational hyperbolic discounting||||This paper shows that the $q$-exponential function rationally evaluate the time discounting. When we consider two processes of wealth accumulation with different frequencies, then the discount...||||https://arxiv.org/abs/1910.05209v1||||econ||||
261||||None||||Economics of Human-AI Ecosystem: Value Bias and Lost Utility in Multi-Dimensional Gaps||||arXiv.org||||2018/11/19||||Economics of Human-AI Ecosystem: Value Bias and Lost Utility in Multi-Dimensional Gaps||||Muller, Daniel||||https://arxiv.org/pdf/1811.06606||||1811.06606||||In recent years, artificial intelligence (AI) decision-making and autonomous systems became an integrated part of the economy, industry, and society. The evolving economy of the human-AI ecosystem raising concerns regarding the risks and values inherited in AI systems. This paper investigates the dynamics of creation and exchange of values and points out gaps in perception of cost-value, knowledge, space and time dimensions. It shows aspects of value bias in human perception of achievements and costs that encoded in AI systems. It also proposes rethinking hard goals definitions and cost-optimal problem-solving principles in the lens of effectiveness and efficiency in the development of trusted machines. The paper suggests a value-driven with cost awareness strategy and principles for problem-solving and planning of effective research progress to address real-world problems that involve diverse forms of achievements, investments, and survival scenarios.||||@arxiv||||2018/11/15||||Economics of Human-AI Ecosystem: Value Bias and Lost Utility in...||||In recent years, artificial intelligence (AI) decision-making and autonomous systems became an integrated part of the economy, industry, and society. The evolving economy of the human-AI ecosystem...||||https://arxiv.org/abs/1811.06606v2||||cs||||
262||||None||||Salvaging Falsified Instrumental Variable Models||||arXiv.org||||2020/01/06||||Salvaging Falsified Instrumental Variable Models||||Masten, Matthew A. || Poirier, Alexandre||||https://arxiv.org/pdf/1812.11598||||1812.11598||||What should researchers do when their baseline model is refuted? We provide four constructive answers. First, researchers can measure the extent of falsification. To do this, we consider continuous relaxations of the baseline assumptions of concern. We then define the falsification frontier: The smallest relaxations of the baseline model which are not refuted. This frontier provides a quantitative measure of the extent of falsification. Second, researchers can present the identified set for the parameter of interest under the assumption that the true model lies somewhere on this frontier. We call this the falsification adaptive set. This set generalizes the standard baseline estimand to account for possible falsification. Third, researchers can present the identified set for a specific point on this frontier. Finally, as a sensitivity analysis, researchers can present identified sets for points beyond the frontier. To illustrate these four ways of salvaging falsified models, we study overidentifying restrictions in two instrumental variable models: a homogeneous effects linear model, and heterogeneous effect models with either binary or continuous outcomes. In the linear model, we consider the classical overidentifying restrictions implied when multiple instruments are observed. We generalize these conditions by considering continuous relaxations of the classical exclusion restrictions. By sufficiently weakening the assumptions, a falsified baseline model becomes non-falsified. We obtain analogous results in the heterogeneous effect models, where we derive identified sets for marginal distributions of potential outcomes, falsification frontiers, and falsification adaptive sets under continuous relaxations of the instrument exogeneity assumptions. We illustrate our results in four different empirical applications.||||@arxiv||||2018/12/30||||Salvaging Falsified Instrumental Variable Models||||What should researchers do when their baseline model is refuted? We provide four constructive answers. First, researchers can measure the extent of falsification. To do this, we consider...||||https://arxiv.org/abs/1812.11598v3||||econ||||
263||||None||||A Path Integral Approach to Business Cycle Models with Large Number of Agents||||arXiv.org||||2018/10/16||||A Path Integral Approach to Business Cycle Models with Large Number of Agents||||Lotz, Aïleen || Gosselin, Pierre || Wambst, Marc||||https://arxiv.org/pdf/1810.07178||||1810.07178||||This paper presents an analytical treatment of economic systems with an arbitrary number of agents that keeps track of the systems' interactions and agents' complexity. This formalism does not seek to aggregate agents. It rather replaces the standard optimization approach by a probabilistic description of both the entire system and agents'behaviors. This is done in two distinct steps. A first step considers an interacting system involving an arbitrary number of agents, where each agent's utility function is subject to unpredictable shocks. In such a setting, individual optimization problems need not be resolved. Each agent is described by a time-dependent probability distribution centered around his utility optimum. The entire system of agents is thus defined by a composite probability depending on time, agents' interactions and forward-looking behaviors. This dynamic system is described by a path integral formalism in an abstract space-the space of the agents' actions-and is very similar to a statistical physics or quantum mechanics system. We show that this description, applied to the space of agents'actions, reduces to the usual optimization results in simple cases. Compared to a standard optimization, such a description markedly eases the treatment of systems with small number of agents. It becomes however useless for a large number of agents. In a second step therefore, we show that for a large number of agents, the previous description is equivalent to a more compact description in terms of field theory. This yields an analytical though approximate treatment of the system. This field theory does not model the aggregation of a microeconomic system in the usual sense. It rather describes an environment of a large number of interacting agents. From this description, various phases or equilibria may be retrieved, along with individual agents' behaviors and their interactions with the environment. For illustrative purposes, this paper studies a Business Cycle model with a large number of agents.||||@arxiv||||2018/10/16||||A Path Integral Approach to Business Cycle Models with Large...||||This paper presents an analytical treatment of economic systems with an arbitrary number of agents that keeps track of the systems' interactions and agents' complexity. This formalism does not...||||https://arxiv.org/abs/1810.07178v1||||cond-mat||||
264||||None||||Doubly Robust Uniform Confidence Band for the Conditional Average Treatment Effect Function||||arXiv.org||||2016/10/28||||Doubly Robust Uniform Confidence Band for the Conditional Average Treatment Effect Function||||Lee, Sokbae || Okui, Ryo || Whang, Yoon-Jae||||https://arxiv.org/pdf/1601.02801||||1601.02801||||In this paper, we propose a doubly robust method to present the heterogeneity of the average treatment effect with respect to observed covariates of interest. We consider a situation where a large number of covariates are needed for identifying the average treatment effect but the covariates of interest for analyzing heterogeneity are of much lower dimension. Our proposed estimator is doubly robust and avoids the curse of dimensionality. We propose a uniform confidence band that is easy to compute, and we illustrate its usefulness via Monte Carlo experiments and an application to the effects of smoking on birth weights.||||@arxiv||||2016/01/12||||Doubly Robust Uniform Confidence Band for the Conditional Average...||||In this paper, we propose a doubly robust method to present the heterogeneity of the average treatment effect with respect to observed covariates of interest. We consider a situation where a large...||||https://arxiv.org/abs/1601.02801v2||||econ||||
265||||None||||Building and Testing Yield Curve Generators for P&C Insurance||||arXiv.org||||2019/12/22||||Building and Testing Yield Curve Generators for P&C Insurance||||Venter, Gary || Shang, Kailan||||https://arxiv.org/pdf/1912.10526||||1912.10526||||Interest-rate risk is a key factor for property-casualty insurer capital. P&C companies tend to be highly leveraged, with bond holdings much greater than capital. For GAAP capital, bonds are marked to market but liabilities are not, so shifts in the yield curve can have a significant impact on capital. Yield-curve scenario generators are one approach to quantifying this risk. They produce many future simulated evolutions of the yield curve, which can be used to quantify the probabilities of bond-value changes that would result from various maturity-mix strategies. Some of these generators are provided as black-box models where the user gets only the projected scenarios. One focus of this paper is to provide methods for testing generated scenarios from such models by comparing to known distributional properties of yield curves.   P&C insurers hold bonds to maturity and manage cash-flow risk by matching asset and liability flows. Derivative pricing and stochastic volatility are of little concern over the relevant time frames. This requires different models and model testing than what is common in the broader financial markets.   To complicate things further, interest rates for the last decade have not been following the patterns established in the sixty years following WWII. We are now coming out of the period of very low rates, yet are still not returning to what had been thought of as normal before that. Modeling and model testing are in an evolving state while new patterns emerge.   Our analysis starts with a review of the literature on interest-rate model testing, with a P&C focus, and an update of the tests for current market behavior. We then discuss models, and use them to illustrate the fitting and testing methods. The testing discussion does not require the model-building section.||||@arxiv||||2019/12/22||||Building and Testing Yield Curve Generators for P&C Insurance||||Interest-rate risk is a key factor for property-casualty insurer capital. P&C companies tend to be highly leveraged, with bond holdings much greater than capital. For GAAP capital, bonds are...||||https://arxiv.org/abs/1912.10526v1||||econ||||
266||||None||||Heterogenous Coefficients, Discrete Instruments, and Identification of Treatment Effects||||arXiv.org||||2018/11/24||||Heterogenous Coefficients, Discrete Instruments, and Identification of Treatment Effects||||Newey, Whitney K. || Stouli, Sami||||https://arxiv.org/pdf/1811.09837||||1811.09837||||Multidimensional heterogeneity and endogeneity are important features of a wide class of econometric models. We consider heterogenous coefficients models where the outcome is a linear combination of known functions of treatment and heterogenous coefficients. We use control variables to obtain identification results for average treatment effects. With discrete instruments in a triangular model we find that average treatment effects cannot be identified when the number of support points is less than or equal to the number of coefficients. A sufficient condition for identification is that the second moment matrix of the treatment functions given the control is nonsingular with probability one. We relate this condition to identification of average treatment effects with multiple treatments.||||@arxiv||||2018/11/24||||Heterogenous Coefficients, Discrete Instruments, and...||||Multidimensional heterogeneity and endogeneity are important features of a wide class of econometric models. We consider heterogenous coefficients models where the outcome is a linear combination...||||https://arxiv.org/abs/1811.09837v1||||econ||||
267||||None||||Investigating the Investment Behaviors in Cryptocurrency||||arXiv.org||||2019/12/06||||Investigating the Investment Behaviors in Cryptocurrency||||Xi, Dingli || O'Brien, Timothy Ian || Irannezhad, Elnaz||||https://arxiv.org/pdf/1912.03311||||1912.03311||||This study investigates the socio-demographic characteristics that individual cryptocurrency investors exhibit and the factors which go into their investment decisions in different Initial Coin Offerings. A web based revealed preference survey was conducted among Australian and Chinese blockchain and cryptocurrency followers, and a Multinomial Logit model was applied to inferentially analyze the characteristics of cryptocurrency investors and the determinants of the choice of investment in cryptocurrency coins versus other types of ICO tokens. The results show a difference between the determinant of these two choices among Australian and Chinese cryptocurrency folks. The significant factors of these two choices include age, gender, education, occupation, and investment experience, and they align well with the behavioural literature. Furthermore, alongside differences in how they rank the attributes of ICOs, there is further variance between how Chinese and Australian investors rank deterrence factors and investment strategies.||||@arxiv||||2019/12/06||||Investigating the Investment Behaviors in Cryptocurrency||||This study investigates the socio-demographic characteristics that individual cryptocurrency investors exhibit and the factors which go into their investment decisions in different Initial Coin...||||https://arxiv.org/abs/1912.03311v1||||econ||||
268||||None||||Monte Carlo Confidence Sets for Identified Sets||||arXiv.org||||2017/09/26||||Monte Carlo Confidence Sets for Identified Sets||||Chen, Xiaohong || Christensen, Timothy || Tamer, Elie||||https://arxiv.org/pdf/1605.00499||||1605.00499||||In complicated/nonlinear parametric models, it is generally hard to know whether the model parameters are point identified. We provide computationally attractive procedures to construct confidence sets (CSs) for identified sets of full parameters and of subvectors in models defined through a likelihood or a vector of moment equalities or inequalities. These CSs are based on level sets of optimal sample criterion functions (such as likelihood or optimally-weighted or continuously-updated GMM criterions). The level sets are constructed using cutoffs that are computed via Monte Carlo (MC) simulations directly from the quasi-posterior distributions of the criterions. We establish new Bernstein-von Mises (or Bayesian Wilks) type theorems for the quasi-posterior distributions of the quasi-likelihood ratio (QLR) and profile QLR in partially-identified regular models and some non-regular models. These results imply that our MC CSs have exact asymptotic frequentist coverage for identified sets of full parameters and of subvectors in partially-identified regular models, and have valid but potentially conservative coverage in models with reduced-form parameters on the boundary. Our MC CSs for identified sets of subvectors are shown to have exact asymptotic coverage in models with singularities. We also provide results on uniform validity of our CSs over classes of DGPs that include point and partially identified models. We demonstrate good finite-sample coverage properties of our procedures in two simulation experiments. Finally, our procedures are applied to two non-trivial empirical examples: an airline entry game and a model of trade flows.||||@arxiv||||2016/05/02||||Monte Carlo Confidence Sets for Identified Sets||||In complicated/nonlinear parametric models, it is generally hard to know whether the model parameters are point identified. We provide computationally attractive procedures to construct confidence...||||https://arxiv.org/abs/1605.00499v3||||econ||||
269||||None||||Open shop scheduling games||||arXiv.org||||2019/07/30||||Open shop scheduling games||||Atay, Ata || Calleja, Pedro || Soteras, Sergio||||https://arxiv.org/pdf/1907.12909||||1907.12909||||This paper takes a game theoretical approach to open shop scheduling problems with unit execution times to minimize the sum of completion times. By supposing an initial schedule and associating each job (consisting in a number of operations) to a different player, we can construct a cooperative TU-game associated with any open shop scheduling problem. We assign to each coalition the maximal cost savings it can obtain through admissible rearrangements of jobs' operations. By providing a core allocation, we show that the associated games are balanced. Finally, we relax the definition of admissible rearrangements for a coalition to study to what extend balancedness still holds.||||@arxiv||||2019/07/30||||Open shop scheduling games||||This paper takes a game theoretical approach to open shop scheduling problems with unit execution times to minimize the sum of completion times. By supposing an initial schedule and associating...||||https://arxiv.org/abs/1907.12909v1||||cs||||
270||||None||||Bayesian estimation of large dimensional time varying VARs using copulas||||arXiv.org||||2019/12/28||||Bayesian estimation of large dimensional time varying VARs using copulas||||Tsionas, Mike || Izzeldin, Marwan || Trapani, Lorenzo||||https://arxiv.org/pdf/1912.12527||||1912.12527||||This paper provides a simple, yet reliable, alternative to the (Bayesian) estimation of large multivariate VARs with time variation in the conditional mean equations and/or in the covariance structure. With our new methodology, the original multivariate, n dimensional model is treated as a set of n univariate estimation problems, and cross-dependence is handled through the use of a copula. Thus, only univariate distribution functions are needed when estimating the individual equations, which are often available in closed form, and easy to handle with MCMC (or other techniques). Estimation is carried out in parallel for the individual equations. Thereafter, the individual posteriors are combined with the copula, so obtaining a joint posterior which can be easily resampled. We illustrate our approach by applying it to a large time-varying parameter VAR with 25 macroeconomic variables.||||@arxiv||||2019/12/28||||Bayesian estimation of large dimensional time varying VARs using copulas||||This paper provides a simple, yet reliable, alternative to the (Bayesian) estimation of large multivariate VARs with time variation in the conditional mean equations and/or in the covariance...||||https://arxiv.org/abs/1912.12527v1||||econ||||
271||||None||||The Age-Period-Cohort-Interaction Model for Describing and Investigating Inter-Cohort Deviations and Intra-Cohort Life-Course Dynamics||||arXiv.org||||2019/06/03||||The Age-Period-Cohort-Interaction Model for Describing and Investigating Inter-Cohort Deviations and Intra-Cohort Life-Course Dynamics||||Luo, Liying || Hodges, James||||https://arxiv.org/pdf/1906.08357||||1906.08357||||Social scientists have frequently sought to understand the distinct effects of age, period, and cohort, but disaggregation of the three dimensions is difficult because cohort = period - age. We argue that this technical difficulty reflects a disconnection between how cohort effect is conceptualized and how it is modeled in the traditional age-period-cohort framework. We propose a new method, called the age-period-cohort-interaction (APC-I) model, that is qualitatively different from previous methods in that it represents Ryder's (1965) theoretical account about the conditions under which cohort differentiation may arise. This APC-I model does not require problematic statistical assumptions and the interpretation is straightforward. It quantifies inter-cohort deviations from the age and period main effects and also permits hypothesis testing about intra-cohort life-course dynamics. We demonstrate how this new model can be used to examine age, period, and cohort patterns in women's labor force participation.||||@arxiv||||2019/06/03||||The Age-Period-Cohort-Interaction Model for Describing and...||||Social scientists have frequently sought to understand the distinct effects of age, period, and cohort, but disaggregation of the three dimensions is difficult because cohort = period - age. We...||||https://arxiv.org/abs/1906.08357v1||||econ||||
272||||None||||Spreading of an infectious disease between different locations||||arXiv.org||||2018/12/19||||Spreading of an infectious disease between different locations||||Muscillo, Alessio || Pin, Paolo || Razzolini, Tiziano||||https://arxiv.org/pdf/1812.07827||||1812.07827||||The endogenous adaptation of agents, that may adjust their local contact network in response to the risk of being infected, can have the perverse effect of increasing the overall systemic infectiveness of a disease. We study a dynamical model over two geographically distinct but interacting locations, to better understand theoretically the mechanism at play. Moreover, we provide empirical motivation from the Italian National Bovine Database, for the period 2006-2013.||||@arxiv||||2018/12/19||||Spreading of an infectious disease between different locations||||The endogenous adaptation of agents, that may adjust their local contact network in response to the risk of being infected, can have the perverse effect of increasing the overall systemic...||||https://arxiv.org/abs/1812.07827v1||||econ||||
273||||None||||Empirical bias of extreme-price auctions: analysis||||arXiv.org||||2019/05/13||||Empirical bias of extreme-price auctions: analysis||||Velez, Rodrigo A. || Brown, Alexander L.||||https://arxiv.org/pdf/1905.08234||||1905.08234||||We advance empirical equilibrium analysis (Velez and Brown, 2019) of the winner-bid and loser-bid auctions for the dissolution of a partnership. We show, in a complete information environment, that even though these auctions are essentially equivalent for the Nash equilibrium prediction, they can be expected to differ in fundamental ways when they are operated. Besides the direct policy implications, two general consequences follow. First, a mechanism designer who accounts for the empirical plausibility of equilibria may not be constrained by Maskin invariance. Second, a mechanism designer who does not account for the empirical plausibility of equilibria may inadvertently design biased mechanisms.||||@arxiv||||2019/05/13||||Empirical bias of extreme-price auctions: analysis||||We advance empirical equilibrium analysis (Velez and Brown, 2019) of the winner-bid and loser-bid auctions for the dissolution of a partnership. We show, in a complete information environment,...||||https://arxiv.org/abs/1905.08234v1||||econ||||
274||||None||||Topologically Mapping the Macroeconomy||||arXiv.org||||2019/11/24||||Topologically Mapping the Macroeconomy||||Dlotko, Pawel || Rudkin, Simon || Qiu, Wanling||||https://arxiv.org/pdf/1911.10476||||1911.10476||||An understanding of the economic landscape in a world of ever increasing data necessitates representations of data that can inform policy, deepen understanding and guide future research. Topological Data Analysis offers a set of tools which deliver on all three calls. Abstract two-dimensional snapshots of multi-dimensional space readily capture non-monotonic relationships, inform of similarity between points of interest in parameter space, mapping such to outcomes. Specific examples show how some, but not all, countries have returned to Great Depression levels, and reappraise the links between real private capital growth and the performance of the economy. Theoretical and empirical expositions alike remind on the dangers of assuming monotonic relationships and discounting combinations of factors as determinants of outcomes; both dangers Topological Data Analysis addresses. Policy-makers can look at outcomes and target areas of the input space where such are not satisfactory, academics may additionally find evidence to motivate theoretical development, and practitioners can gain a rapid and robust base for decision making.||||@arxiv||||2019/11/24||||Topologically Mapping the Macroeconomy||||An understanding of the economic landscape in a world of ever increasing data necessitates representations of data that can inform policy, deepen understanding and guide future research....||||https://arxiv.org/abs/1911.10476v1||||econ||||
275||||None||||A path-based many-to-many assignment game to model Mobility-as-a-Service market networks||||arXiv.org||||2019/11/11||||A path-based many-to-many assignment game to model Mobility-as-a-Service market networks||||Pantelidis, Theodoros || Rasulkhani, Saeid || Chow, Joseph Y. J.||||https://arxiv.org/pdf/1911.04435||||1911.04435||||As Mobility as a Service (MaaS) systems become increasingly popular, travel is changing from unimodal trips to personalized services offered by a market of mobility operators. Traditional traffic assignment models ignore the interaction of different operators. However, a key characteristic of MaaS markets is that urban trip decisions depend on both user route decisions as well as operator service and pricing decisions. We adopt a new paradigm for traffic assignment in a MaaS network of multiple operators using the concept of stable matching to allocate costs and determine prices offered by operators corresponding to user route choices and operator service choices without resorting to nonconvex bilevel programming formulations. Unlike our prior work, the proposed model allows travelers to make multimodal, multi-operator trips, resulting in stable cost allocations between competing network operators to provide MaaS for users. Algorithms are proposed to generate stability conditions for the stable outcome pricing model. Extensive computational experiments demonstrate the use of the model, and effectiveness of the proposed algorithm, to handling pricing responses of MaaS operators in technological and capacity changes, government acquisition, consolidation, and firm entry, using the classic Sioux Falls network.||||@arxiv||||2019/11/11||||A path-based many-to-many assignment game to model...||||As Mobility as a Service (MaaS) systems become increasingly popular, travel is changing from unimodal trips to personalized services offered by a market of mobility operators. Traditional traffic...||||https://arxiv.org/abs/1911.04435v1||||cs||||
276||||None||||A Simple Estimator for Quantile Panel Data Models Using Smoothed Quantile Regressions||||arXiv.org||||2019/11/12||||A Simple Estimator for Quantile Panel Data Models Using Smoothed Quantile Regressions||||Chen, Liang || Huo, Yulong||||https://arxiv.org/pdf/1911.04729||||1911.04729||||Canay (2011)'s two-step estimator of quantile panel data models, due to its simple intuition and low computational cost, has been widely used in empirical studies in recent years. In this paper, we revisit the estimator of Canay (2011) and point out that in his asymptotic analysis the bias of his estimator due to the estimation of the fixed effects is mistakenly omitted, and that such omission will lead to invalid inference on the coefficients. To solve this problem, we propose a similar easy-to-implement estimator based on smoothed quantile regressions. The asymptotic distribution of the new estimator is established and the analytical expression of its asymptotic bias is derived. Based on these results, we show how to make asymptotically valid inference based on both analytical and split-panel jackknife bias corrections. Finally, finite sample simulations are used to support our theoretical analysis and to illustrate the importance of bias correction in quantile regressions for panel data.||||@arxiv||||2019/11/12||||A Simple Estimator for Quantile Panel Data Models Using Smoothed...||||Canay (2011)'s two-step estimator of quantile panel data models, due to its simple intuition and low computational cost, has been widely used in empirical studies in recent years. In this paper,...||||https://arxiv.org/abs/1911.04729v1||||econ||||
277||||None||||On the residues vectors of a rational class of complex functions. Application to autoregressive processes||||arXiv.org||||2019/07/12||||On the residues vectors of a rational class of complex functions. Application to autoregressive processes||||Scheidereiter, Guillermo Daniel || Faure, Omar Roberto||||https://arxiv.org/pdf/1907.05949||||1907.05949||||Complex functions have multiple uses in various fields of study, so analyze their characteristics it is of extensive interest to other sciences. This work begins with a particular class of rational functions of a complex variable; over this is deduced two elementals properties concerning the residues and is proposed one results which establishes one lower bound for the p-norm of the residues vector. Applications to the autoregressive processes are presented and the exemplifications are indicated in historical data of electric generation and econometric series.||||@arxiv||||2019/07/12||||On the residues vectors of a rational class of complex functions....||||Complex functions have multiple uses in various fields of study, so analyze their characteristics it is of extensive interest to other sciences. This work begins with a particular class of...||||https://arxiv.org/abs/1907.05949v1||||econ||||
278||||None||||A revenue allocation scheme based on pairwise comparisons||||arXiv.org||||2019/09/25||||A revenue allocation scheme based on pairwise comparisons||||Petróczy, Dóra Gréta || Csató, László||||https://arxiv.org/pdf/1909.12931||||1909.12931||||A model of sharing revenues among groups when group members are ranked several times is presented. The methodology is based on pairwise comparison matrices, allows for the use of any weighting method, and makes possible to tune the level of inequality. Our proposal is demonstrated on the example of Formula One prize money allocation among the constructors. We introduce an axiom called scale invariance, which requires the ranking of teams to be independent of the parameter controlling inequality. The eigenvector method is revealed to violate this condition in our dataset, while the row geometric mean method always satisfies it. The revenue allocation is not influenced by the arbitrary valuation given to the race prizes in the official points scoring system of Formula One.||||@arxiv||||2019/09/25||||A revenue allocation scheme based on pairwise comparisons||||A model of sharing revenues among groups when group members are ranked several times is presented. The methodology is based on pairwise comparison matrices, allows for the use of any weighting...||||https://arxiv.org/abs/1909.12931v1||||cs||||
279||||None||||A Perfect Specialization Model for Gravity Equation in Bilateral Trade based on Production Structure||||arXiv.org||||2018/03/27||||A Perfect Specialization Model for Gravity Equation in Bilateral Trade based on Production Structure||||Einian, Majid || Ravasan, Farshad Ranjbar||||https://arxiv.org/pdf/1803.09935||||1803.09935||||Although initially originated as a totally empirical relationship to explain the volume of trade between two partners, gravity equation has been the focus of several theoretic models that try to explain it. Specialization models are of great importance in providing a solid theoretic ground for gravity equation in bilateral trade. Some research papers try to improve specialization models by adding imperfect specialization to model, but we believe it is unnecessary complication. We provide a perfect specialization model based on the phenomenon we call tradability, which overcomes the problems with simpler initial. We provide empirical evidence using estimates on panel data of bilateral trade of 40 countries over 10 years that support the theoretical model. The empirical results have implied that tradability is the only reason for deviations of data from basic perfect specialization models.||||@arxiv||||2018/03/27||||A Perfect Specialization Model for Gravity Equation in Bilateral...||||Although initially originated as a totally empirical relationship to explain the volume of trade between two partners, gravity equation has been the focus of several theoretic models that try to...||||https://arxiv.org/abs/1803.09935v1||||econ||||
280||||None||||The Incidental Parameters Problem in Testing for Remaining Cross-section Correlation||||arXiv.org||||2019/10/28||||The Incidental Parameters Problem in Testing for Remaining Cross-section Correlation||||Juodis, Arturas || Reese, Simon||||https://arxiv.org/pdf/1810.03715||||1810.03715||||In this paper we consider the properties of the Pesaran (2004, 2015a) CD test for cross-section correlation when applied to residuals obtained from panel data models with many estimated parameters. We show that the presence of period-specific parameters leads the CD test statistic to diverge as length of the time dimension of the sample grows. This result holds even if cross-section dependence is correctly accounted for and hence constitutes an example of the Incidental Parameters Problem. The relevance of this problem is investigated both for the classical Time Fixed Effects estimator as well as the Common Correlated Effects estimator of Pesaran (2006). We suggest a weighted CD test statistic which re-establishes standard normal inference under the null hypothesis. Given the widespread use of the CD test statistic to test for remaining cross-section correlation, our results have far reaching implications for empirical researchers.||||@arxiv||||2018/10/08||||The Incidental Parameters Problem in Testing for Remaining...||||In this paper we consider the properties of the Pesaran (2004, 2015a) CD test for cross-section correlation when applied to residuals obtained from panel data models with many estimated...||||https://arxiv.org/abs/1810.03715v3||||econ||||
281||||None||||A Vine-copula extension for the HAR model||||arXiv.org||||2019/07/19||||A Vine-copula extension for the HAR model||||Magris, Martin||||https://arxiv.org/pdf/1907.08522||||1907.08522||||The heterogeneous autoregressive (HAR) model is revised by modeling the joint distribution of the four partial-volatility terms therein involved. Namely, today's, yesterday's, last week's and last month's volatility components. The joint distribution relies on a (C-) Vine copula construction, allowing to conveniently extract volatility forecasts based on the conditional expectation of today's volatility given its past terms. The proposed empirical application involves more than seven years of high-frequency transaction prices for ten stocks and evaluates the in-sample, out-of-sample and one-step-ahead forecast performance of our model for daily realized-kernel measures. The model proposed in this paper is shown to outperform the HAR counterpart under different models for marginal distributions, copula construction methods, and forecasting settings.||||@arxiv||||2019/07/19||||A Vine-copula extension for the HAR model||||The heterogeneous autoregressive (HAR) model is revised by modeling the joint distribution of the four partial-volatility terms therein involved. Namely, today's, yesterday's, last week's and last...||||https://arxiv.org/abs/1907.08522v1||||econ||||
282||||None||||A Game Theoretic Setting of Capitation Versus Fee-For-Service Payment Systems||||arXiv.org||||2019/10/01||||A Game Theoretic Setting of Capitation Versus Fee-For-Service Payment Systems||||Koenecke, Allison||||https://arxiv.org/pdf/1904.11604||||1904.11604||||We aim to determine whether a game-theoretic model between an insurer and a healthcare practice yields a predictive equilibrium that incentivizes either player to deviate from a fee-for-service to capitation payment system. Using United States data from various primary care surveys, we find that non-extreme equilibria (i.e., shares of patients, or shares of patient visits, seen under a fee-for-service payment system) can be derived from a Stackelberg game if insurers award a non-linear bonus to practices based on performance. Overall, both insurers and practices can be incentivized to embrace capitation payments somewhat, but potentially at the expense of practice performance.||||@arxiv||||2019/04/25||||A Game Theoretic Setting of Capitation Versus Fee-For-Service...||||We aim to determine whether a game-theoretic model between an insurer and a healthcare practice yields a predictive equilibrium that incentivizes either player to deviate from a fee-for-service to...||||https://arxiv.org/abs/1904.11604v2||||econ||||
283||||None||||Density Forecasts in Panel Data Models: A Semiparametric Bayesian Perspective||||arXiv.org||||2018/05/10||||Density Forecasts in Panel Data Models: A Semiparametric Bayesian Perspective||||Liu, Laura||||https://arxiv.org/pdf/1805.04178||||1805.04178||||This paper constructs individual-specific density forecasts for a panel of firms or households using a dynamic linear model with common and heterogeneous coefficients and cross-sectional heteroskedasticity. The panel considered in this paper features a large cross-sectional dimension N but short time series T. Due to the short T, traditional methods have difficulty in disentangling the heterogeneous parameters from the shocks, which contaminates the estimates of the heterogeneous parameters. To tackle this problem, I assume that there is an underlying distribution of heterogeneous parameters, model this distribution nonparametrically allowing for correlation between heterogeneous parameters and initial conditions as well as individual-specific regressors, and then estimate this distribution by pooling the information from the whole cross-section together. Theoretically, I prove that both the estimated common parameters and the estimated distribution of the heterogeneous parameters achieve posterior consistency, and that the density forecasts asymptotically converge to the oracle forecast. Methodologically, I develop a simulation-based posterior sampling algorithm specifically addressing the nonparametric density estimation of unobserved heterogeneous parameters. Monte Carlo simulations and an application to young firm dynamics demonstrate improvements in density forecasts relative to alternative approaches.||||@arxiv||||2018/05/10||||Density Forecasts in Panel Data Models: A Semiparametric Bayesian...||||This paper constructs individual-specific density forecasts for a panel of firms or households using a dynamic linear model with common and heterogeneous coefficients and cross-sectional...||||https://arxiv.org/abs/1805.04178v1||||econ||||
284||||None||||An Experimental Investigation of Preference Misrepresentation in the Residency Match||||arXiv.org||||2018/08/30||||An Experimental Investigation of Preference Misrepresentation in the Residency Match||||Rees-Jones, Alex || Skowronek, Samuel||||https://arxiv.org/pdf/1802.01990||||1802.01990||||The development and deployment of matching procedures that incentivize truthful preference reporting is considered one of the major successes of market design research. In this study, we test the degree to which these procedures succeed in eliminating preference misrepresentation. We administered an online experiment to 1,714 medical students immediately after their participation in the medical residency match--a leading field application of strategy-proof market design. When placed in an analogous, incentivized matching task, we find that 23% of participants misrepresent their preferences. We explore the factors that predict preference misrepresentation, including cognitive ability, strategic positioning, overconfidence, expectations, advice, and trust. We discuss the implications of this behavior for the design of allocation mechanisms and the social welfare in markets that use them.||||@arxiv||||2018/02/05||||An Experimental Investigation of Preference Misrepresentation in...||||The development and deployment of matching procedures that incentivize truthful preference reporting is considered one of the major successes of market design research. In this study, we test the...||||https://arxiv.org/abs/1802.01990v2||||econ||||
285||||None||||Testing for time-varying properties under misspecified conditional mean and variance||||arXiv.org||||2019/09/01||||Testing for time-varying properties under misspecified conditional mean and variance||||Maki, Daiki || Ota, Yasushi||||https://arxiv.org/pdf/1907.12107||||1907.12107||||This study examines statistical performance of tests for time-varying properties under misspecified conditional mean and variance. When we test for time-varying properties of the conditional mean in the case in which data have no time-varying mean but have time-varying variance, asymptotic tests have size distortions. This is improved by the use of a bootstrap method. Similarly, when we test for time-varying properties of the conditional variance in the case in which data have time-varying mean but no time-varying variance, asymptotic tests have large size distortions. This is not improved even by the use of bootstrap methods. We show that tests for time-varying properties of the conditional mean by the bootstrap are robust regardless of the time-varying variance model, whereas tests for time-varying properties of the conditional variance do not perform well in the presence of misspecified time-varying mean.||||@arxiv||||2019/07/28||||Testing for time-varying properties under misspecified conditional...||||This study examines statistical performance of tests for time-varying properties under misspecified conditional mean and variance. When we test for time-varying properties of the conditional mean...||||https://arxiv.org/abs/1907.12107v2||||econ||||
286||||None||||Identifiability and Estimation of Possibly Non-Invertible SVARMA Models: A New Parametrisation||||arXiv.org||||2020/02/11||||Identifiability and Estimation of Possibly Non-Invertible SVARMA Models: A New Parametrisation||||Funovits, Bernd||||https://arxiv.org/pdf/2002.04346||||2002.04346||||This paper deals with parameterisation, identifiability, and maximum likelihood (ML) estimation of possibly non-invertible structural vector autoregressive moving average (SVARMA) models driven by independent and non-Gaussian shocks. We introduce a new parameterisation of the MA polynomial matrix based on the Wiener-Hopf factorisation (WHF) and show that the model is identified in this parametrisation for a generic set in the parameter space (when certain just-identifying restrictions are imposed). When the SVARMA model is driven by Gaussian errors, neither the static shock transmission matrix, nor the location of the determinantal zeros of the MA polynomial matrix can be identified without imposing further identifying restrictions on the parameters. We characterise the classes of observational equivalence with respect to second moment information at different stages of the modelling process. Subsequently, cross-sectional and temporal independence and non-Gaussianity of the shocks is used to solve these identifiability problems and identify the true root location of the MA polynomial matrix as well as the static shock transmission matrix (up to permutation and scaling).Typically imposed identifying restrictions on the shock transmission matrix as well as on the determinantal root location are made testable. Furthermore, we provide low level conditions for asymptotic normality of the ML estimator. The estimation procedure is illustrated with various examples from the economic literature and implemented as R-package.||||@arxiv||||2020/02/11||||Identifiability and Estimation of Possibly Non-Invertible SVARMA...||||This paper deals with parameterisation, identifiability, and maximum likelihood (ML) estimation of possibly non-invertible structural vector autoregressive moving average (SVARMA) models driven by...||||https://arxiv.org/abs/2002.04346v1||||econ||||
287||||None||||Startups and Stanford University||||arXiv.org||||2017/11/02||||Startups and Stanford University||||Lebret, Hervé||||https://arxiv.org/pdf/1711.00644||||1711.00644||||Startups have become in less than 50 years a major component of innovation and economic growth. Silicon Valley has been the place where the startup phenomenon was the most obvious and Stanford University was a major component of that success. Companies such as Google, Yahoo, Sun Microsystems, Cisco, Hewlett Packard had very strong links with Stanford but even these vary famous success stories cannot fully describe the richness and diversity of the Stanford entrepreneurial activity. This report explores the dynamics of more than 5000 companies founded by Stanford University alumni and staff, through their value creation, their field of activities, their growth patterns and more. The report also explores some features of the founders of these companies such as their academic background or the number of years between their Stanford experience and their company creation.||||@arxiv||||2017/11/02||||Startups and Stanford University||||Startups have become in less than 50 years a major component of innovation and economic growth. Silicon Valley has been the place where the startup phenomenon was the most obvious and Stanford...||||https://arxiv.org/abs/1711.00644v1||||econ||||
288||||None||||The Role of Agricultural Sector Productivity in Economic Growth: The Case of Iran's Economic Development Plan||||arXiv.org||||2018/06/11||||The Role of Agricultural Sector Productivity in Economic Growth: The Case of Iran's Economic Development Plan||||Tahamipour, Morteza || Mahmoudi, Mina||||https://arxiv.org/pdf/1806.04235||||1806.04235||||This study provides the theoretical framework and empirical model for productivity growth evaluations in agricultural sector as one of the most important sectors in Iran's economic development plan. We use the Solow residual model to measure the productivity growth share in the value-added growth of the agricultural sector. Our time series data includes value-added per worker, employment, and capital in this sector. The results show that the average total factor productivity growth rate in the agricultural sector is -0.72% during 1991-2010. Also, during this period, the share of total factor productivity growth in the value-added growth is -19.6%, while it has been forecasted to be 33.8% in the fourth development plan. Considering the effective role of capital in the agricultural low productivity, we suggest applying productivity management plans (especially in regards of capital productivity) to achieve future growth goals.||||@arxiv||||2018/06/11||||The Role of Agricultural Sector Productivity in Economic Growth:...||||This study provides the theoretical framework and empirical model for productivity growth evaluations in agricultural sector as one of the most important sectors in Iran's economic development...||||https://arxiv.org/abs/1806.04235v1||||econ||||
289||||None||||Introducing shrinkage in heavy-tailed state space models to predict equity excess returns||||arXiv.org||||2019/07/26||||Introducing shrinkage in heavy-tailed state space models to predict equity excess returns||||Huber, Florian || Kastner, Gregor || Pfarrhofer, Michael||||https://arxiv.org/pdf/1805.12217||||1805.12217||||We forecast S&P 500 excess returns using a flexible Bayesian econometric state space model with non-Gaussian features at several levels. More precisely, we control for overparameterization via novel global-local shrinkage priors on the state innovation variances as well as the time-invariant part of the state space model. The shrinkage priors are complemented by heavy tailed state innovations that cater for potential large breaks in the latent states. Moreover, we allow for leptokurtic stochastic volatility in the observation equation. The empirical findings indicate that several variants of the proposed approach outperform typical competitors frequently used in the literature, both in terms of point and density forecasts.||||@arxiv||||2018/05/30||||Introducing shrinkage in heavy-tailed state space models to...||||We forecast S&P 500 excess returns using a flexible Bayesian econometric state space model with non-Gaussian features at several levels. More precisely, we control for overparameterization via...||||https://arxiv.org/abs/1805.12217v2||||econ||||
290||||None||||Puzzle of cooperation: One for all, all for one---von Neumann, Wald, Rawls, and Pareto||||arXiv.org||||2019/12/26||||Puzzle of cooperation: One for all, all for one---von Neumann, Wald, Rawls, and Pareto||||Ismail, Mehmet S.||||https://arxiv.org/pdf/1912.00211||||1912.00211||||Cooperation among individuals has been one of the most puzzling phonemena in economics and other social sciences. In this paper, I introduce a novel solution concept, dubbed the optimin criterion, which (to the best of my knowledge) is the first parameter-free concept that can selectively explain the direction of well-established non-Nash deviations towards cooperation in games, including the repeated prisoner's dilemma, the traveler's dilemma, the centipede game, and the repeated public goods game. Moreover, the optimin criterion generalizes and unifies results in various fields: It not only coincides with (i) Wald's statistical decision-making criterion when Nature is antagonistic, (ii) the core in cooperative games when the core is nonempty, though it exists even if the core is empty, but also generalizes (iii) Nash equilibrium in n-person constant-sum games, (iv) stable matchings in matching models, and (v) competitive equilibrium in the Arrow-Debreu economy. Moreover, every Nash equilibrium satisfies the optimin criterion in an auxiliary game.||||@arxiv||||2019/11/30||||Puzzle of cooperation: One for all, all for one---von Neumann,...||||Cooperation among individuals has been one of the most puzzling phonemena in economics and other social sciences. In this paper, I introduce a novel solution concept, dubbed the optimin criterion,...||||https://arxiv.org/abs/1912.00211v3||||econ||||
291||||None||||Relational Communication||||arXiv.org||||2019/01/17||||Relational Communication||||Kolotilin, Anton || Li, Hongyi||||https://arxiv.org/pdf/1901.05645||||1901.05645||||We study a communication game between an informed sender and an uninformed receiver with repeated interactions and voluntary transfers. Transfers motivate the receiver's decision-making and signal the sender's information. Although full separation can always be supported in equilibrium, partial or complete pooling is optimal if the receiver's decision-making is highly responsive to information. In this case, the receiver's decision-making is disciplined by pooling extreme states, where she is most tempted to defect. In characterizing optimal equilibria, we establish new results on monotone persuasion.||||@arxiv||||2019/01/17||||Relational Communication||||We study a communication game between an informed sender and an uninformed receiver with repeated interactions and voluntary transfers. Transfers motivate the receiver's decision-making and signal...||||https://arxiv.org/abs/1901.05645v1||||econ||||
292||||None||||Robust tests for ARCH in the presence of the misspecified conditional mean: A comparison of nonparametric approches||||arXiv.org||||2019/09/01||||Robust tests for ARCH in the presence of the misspecified conditional mean: A comparison of nonparametric approches||||Maki, Daiki || Ota, Yasushi||||https://arxiv.org/pdf/1907.12752||||1907.12752||||This study compares statistical properties of ARCH tests that are robust to the presence of the misspecified conditional mean. The approaches employed in this study are based on two nonparametric regressions for the conditional mean. First is the ARCH test using Nadayara-Watson kernel regression. Second is the ARCH test using the polynomial approximation regression. The two approaches do not require specification of the conditional mean and can adapt to various nonlinear models, which are unknown a priori. Accordingly, they are robust to misspecified conditional mean models. Simulation results show that ARCH tests based on the polynomial approximation regression approach have better statistical properties than ARCH tests using Nadayara-Watson kernel regression approach for various nonlinear models.||||@arxiv||||2019/07/30||||Robust tests for ARCH in the presence of the misspecified...||||This study compares statistical properties of ARCH tests that are robust to the presence of the misspecified conditional mean. The approaches employed in this study are based on two nonparametric...||||https://arxiv.org/abs/1907.12752v2||||econ||||
293||||None||||Measurement Errors as Bad Leverage Points||||arXiv.org||||2018/07/08||||Measurement Errors as Bad Leverage Points||||Blankmeyer, Eric||||https://arxiv.org/pdf/1807.02814||||1807.02814||||Errors-in-variables is a long-standing, difficult issue in linear regression; and progress depends in part on new identifying assumptions. I characterize measurement error as bad-leverage points and assume that fewer than half the sample observations are heavily contaminated, in which case a high-breakdown robust estimator may be able to isolate and down weight or discard the problematic data. In simulations of simple and multiple regression where eiv affects 25% of the data and R-squared is mediocre, certain high-breakdown estimators have small bias and reliable confidence intervals.||||@arxiv||||2018/07/08||||Measurement Errors as Bad Leverage Points||||Errors-in-variables is a long-standing, difficult issue in linear regression; and progress depends in part on new identifying assumptions. I characterize measurement error as bad-leverage points...||||https://arxiv.org/abs/1807.02814v1||||econ||||
294||||None||||Food Supply Chain and Business Model Innovation||||arXiv.org||||2020/01/12||||Food Supply Chain and Business Model Innovation||||Nosratabadi, Saeed || Mosavi, Amirhosein || Lakner, Zoltan||||https://arxiv.org/pdf/2001.03982||||2001.03982||||This paper investigates the contribution of business model innovations in improvement of food supply chains. Through a systematic literature review, the notable business model innovations in the food industry are identified, surveyed, and evaluated. Findings reveal that the innovations in value proposition, value creation processes, and value delivery processes of business models are the successful strategies proposed in food industry. It is further disclosed that rural female entrepreneurs, social movements, and also urban conditions are the most important driving forces inducing the farmers to reconsider their business models. In addition, the new technologies and environmental factors are the secondary contributors in business model innovation for the food processors. It is concluded that digitalization has disruptively changed the food distributors models. E-commerce models and internet of things are reported as the essential factors imposing the retailers to innovate their business models. Furthermore, the consumption demand and the product quality are two main factors affecting the business models of all the firms operating in the food supply chain regardless of their positions in the chain. The findings of the current study provide an insight into the food industry to design a sustainable business model to bridge the gap between food supply and food demand.||||@arxiv||||2020/01/12||||Food Supply Chain and Business Model Innovation||||This paper investigates the contribution of business model innovations in improvement of food supply chains. Through a systematic literature review, the notable business model innovations in the...||||https://arxiv.org/abs/2001.03982v1||||econ||||
295||||None||||Improving Point and Interval Estimates of Monotone Functions by Rearrangement||||arXiv.org||||2008/11/07||||Improving Point and Interval Estimates of Monotone Functions by Rearrangement||||Chernozhukov, Victor || Fernandez-Val, Ivan || Galichon, Alfred||||https://arxiv.org/pdf/0806.4730||||0806.4730||||Suppose that a target function is monotonic, namely, weakly increasing, and an available original estimate of this target function is not weakly increasing. Rearrangements, univariate and multivariate, transform the original estimate to a monotonic estimate that always lies closer in common metrics to the target function. Furthermore, suppose an original simultaneous confidence interval, which covers the target function with probability at least $1-α$, is defined by an upper and lower end-point functions that are not weakly increasing. Then the rearranged confidence interval, defined by the rearranged upper and lower end-point functions, is shorter in length in common norms than the original interval and also covers the target function with probability at least $1-α$. We demonstrate the utility of the improved point and interval estimates with an age-height growth chart example.||||@arxiv||||2008/06/29||||Improving Point and Interval Estimates of Monotone Functions by...||||Suppose that a target function is monotonic, namely, weakly increasing, and an available original estimate of this target function is not weakly increasing. Rearrangements, univariate and...||||https://arxiv.org/abs/0806.4730v3||||econ||||
296||||None||||Bayesian Elicitation||||arXiv.org||||2020/01/26||||Bayesian Elicitation||||Whitmeyer, Mark||||https://arxiv.org/pdf/1902.00976||||1902.00976||||How can a receiver design an information structure in order to elicit information from a sender? We study how a decision-maker can acquire more information from an agent by reducing her own ability to observe what the agent transmits. Intuitively, when the two parties' preferences are not perfectly aligned, this garbling relaxes the sender's concern that the receiver will use her information to the sender's disadvantage. We characterize the optimal information structure for the receiver. The main result is that under broad conditions, the receiver can do just as well as if she could commit to a rule mapping the sender's message to actions: information design is just as good as full commitment. Similarly, we show that these conditions guarantee that ex ante information acquisition always benefits the receiver, even though this learning might actually lower the receiver's expected payoff in the absence of garbling. We illustrate these effects in a range of economically relevant examples.||||@arxiv||||2019/02/03||||Bayesian Elicitation||||How can a receiver design an information structure in order to elicit information from a sender? We study how a decision-maker can acquire more information from an agent by reducing her own...||||https://arxiv.org/abs/1902.00976v2||||econ||||
297||||None||||Identification of semiparametric discrete outcome models with bounded covariates||||arXiv.org||||2018/11/13||||Identification of semiparametric discrete outcome models with bounded covariates||||Kashaev, Nail||||https://arxiv.org/pdf/1811.05555||||1811.05555||||Identification of discrete outcome models is often established by using special covariates that have full support. This paper shows how these identification results can be extended to a large class of commonly used semiparametric discrete outcome models when all covariates are bounded. I apply the proposed methodology to multinomial choice models, bundles models, and finite games of complete information.||||@arxiv||||2018/11/13||||Identification of semiparametric discrete outcome models with...||||Identification of discrete outcome models is often established by using special covariates that have full support. This paper shows how these identification results can be extended to a large...||||https://arxiv.org/abs/1811.05555v1||||econ||||
298||||None||||Comparing School Choice and College Admission Mechanisms By Their Immunity to Strategic Admissions||||arXiv.org||||2020/01/25||||Comparing School Choice and College Admission Mechanisms By Their Immunity to Strategic Admissions||||Bonkoungou, Somouaoga || Nesterov, Alexander S.||||https://arxiv.org/pdf/2001.06166||||2001.06166||||Recently dozens of school districts and college admissions systems around the world have reformed their admission rules. As a main motivation for these reforms the policymakers cited strategic flaws of the rules: students had strong incentives to game the system, which caused dramatic consequences for non-strategic students. However, almost none of the new rules were strategy-proof. We explain this puzzle. We show that after the reforms the rules became more immune to strategic admissions: each student received a smaller set of schools that he can get in using a strategy, weakening incentives to manipulate. Simultaneously, the admission to each school became strategy-proof to a larger set of students, making the schools more available for non-strategic students. We also show that the existing explanation of the puzzle due to Pathak and Sönmez (2013) is incomplete.||||@arxiv||||2020/01/17||||Comparing School Choice and College Admission Mechanisms By Their...||||Recently dozens of school districts and college admissions systems around the world have reformed their admission rules. As a main motivation for these reforms the policymakers cited strategic...||||https://arxiv.org/abs/2001.06166v2||||econ||||
299||||None||||Modal Regression using Kernel Density Estimation: a Review||||arXiv.org||||2017/12/07||||Modal Regression using Kernel Density Estimation: a Review||||Chen, Yen-Chi||||https://arxiv.org/pdf/1710.07004||||1710.07004||||We review recent advances in modal regression studies using kernel density estimation. Modal regression is an alternative approach for investigating relationship between a response variable and its covariates. Specifically, modal regression summarizes the interactions between the response variable and covariates using the conditional mode or local modes. We first describe the underlying model of modal regression and its estimators based on kernel density estimation. We then review the asymptotic properties of the estimators and strategies for choosing the smoothing bandwidth. We also discuss useful algorithms and similar alternative approaches for modal regression, and propose future direction in this field.||||@arxiv||||2017/10/19||||Modal Regression using Kernel Density Estimation: a Review||||We review recent advances in modal regression studies using kernel density estimation. Modal regression is an alternative approach for investigating relationship between a response variable and...||||https://arxiv.org/abs/1710.07004v2||||econ||||
300||||None||||Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments||||arXiv.org||||2019/06/06||||Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments||||Syrgkanis, Vasilis || Lei, Victor || Oprescu, Miruna || Hei, Maggie || Battocchi, Keith || Lewis, Greg||||https://arxiv.org/pdf/1905.10176||||1905.10176||||We consider the estimation of heterogeneous treatment effects with arbitrary machine learning methods in the presence of unobserved confounders with the aid of a valid instrument. Such settings arise in A/B tests with an intent-to-treat structure, where the experimenter randomizes over which user will receive a recommendation to take an action, and we are interested in the effect of the downstream action. We develop a statistical learning approach to the estimation of heterogeneous effects, reducing the problem to the minimization of an appropriate loss function that depends on a set of auxiliary models (each corresponding to a separate prediction task). The reduction enables the use of all recent algorithmic advances (e.g. neural nets, forests). We show that the estimated effect model is robust to estimation errors in the auxiliary models, by showing that the loss satisfies a Neyman orthogonality criterion. Our approach can be used to estimate projections of the true effect model on simpler hypothesis spaces. When these spaces are parametric, then the parameter estimates are asymptotically normal, which enables construction of confidence sets. We applied our method to estimate the effect of membership on downstream webpage engagement on TripAdvisor, using as an instrument an intent-to-treat A/B test among 4 million TripAdvisor users, where some users received an easier membership sign-up process. We also validate our method on synthetic data and on public datasets for the effects of schooling on income.||||@arxiv||||2019/05/24||||Machine Learning Estimation of Heterogeneous Treatment Effects...||||We consider the estimation of heterogeneous treatment effects with arbitrary machine learning methods in the presence of unobserved confounders with the aid of a valid instrument. Such settings...||||https://arxiv.org/abs/1905.10176v3||||cs||||
301||||None||||Geometrically stopped Markovian random growth processes and Pareto tails||||arXiv.org||||2020/01/01||||Geometrically stopped Markovian random growth processes and Pareto tails||||Beare, Brendan K. || Toda, Alexis Akira||||https://arxiv.org/pdf/1712.01431||||1712.01431||||Many empirical studies document power law behavior in size distributions of economic interest such as cities, firms, income, and wealth. One mechanism for generating such behavior combines independent and identically distributed Gaussian additive shocks to log-size with a geometric age distribution. We generalize this mechanism by allowing the shocks to be non-Gaussian (but light-tailed) and dependent upon a Markov state variable. Our main results provide sharp bounds on tail probabilities, a simple equation determining Pareto exponents, and comparative statics. We present two applications: we show that (i) the tails of the wealth distribution in a heterogeneous-agent dynamic general equilibrium model with idiosyncratic investment risk are Paretian, and (ii) a random growth model for the population dynamics of Japanese municipalities is consistent with the observed Pareto exponent but only after allowing for Markovian dynamics.||||@arxiv||||2017/12/05||||Geometrically stopped Markovian random growth processes and Pareto tails||||Many empirical studies document power law behavior in size distributions of economic interest such as cities, firms, income, and wealth. One mechanism for generating such behavior combines...||||https://arxiv.org/abs/1712.01431v3||||econ||||
302||||None||||Forecasting Across Time Series Databases using Recurrent Neural Networks on Groups of Similar Series: A Clustering Approach||||arXiv.org||||2018/09/12||||Forecasting Across Time Series Databases using Recurrent Neural Networks on Groups of Similar Series: A Clustering Approach||||Bandara, Kasun || Bergmeir, Christoph || Smyl, Slawek||||https://arxiv.org/pdf/1710.03222||||1710.03222||||With the advent of Big Data, nowadays in many applications databases containing large quantities of similar time series are available. Forecasting time series in these domains with traditional univariate forecasting procedures leaves great potentials for producing accurate forecasts untapped. Recurrent neural networks (RNNs), and in particular Long Short-Term Memory (LSTM) networks, have proven recently that they are able to outperform state-of-the-art univariate time series forecasting methods in this context when trained across all available time series. However, if the time series database is heterogeneous, accuracy may degenerate, so that on the way towards fully automatic forecasting methods in this space, a notion of similarity between the time series needs to be built into the methods. To this end, we present a prediction model that can be used with different types of RNN models on subgroups of similar time series, which are identified by time series clustering techniques. We assess our proposed methodology using LSTM networks, a widely popular RNN variant. Our method achieves competitive results on benchmarking datasets under competition evaluation procedures. In particular, in terms of mean sMAPE accuracy, it consistently outperforms the baseline LSTM model and outperforms all other methods on the CIF2016 forecasting competition dataset.||||@arxiv||||2017/10/09||||Forecasting Across Time Series Databases using Recurrent Neural...||||With the advent of Big Data, nowadays in many applications databases containing large quantities of similar time series are available. Forecasting time series in these domains with traditional...||||https://arxiv.org/abs/1710.03222v2||||cs||||
303||||None||||Functional Sequential Treatment Allocation with Covariates||||arXiv.org||||2020/01/29||||Functional Sequential Treatment Allocation with Covariates||||Kock, Anders Bredahl || Preinerstorfer, David || Veliyev, Bezirgen||||https://arxiv.org/pdf/2001.10996||||2001.10996||||We consider a multi-armed bandit problem with covariates. Given a realization of the covariate vector, instead of targeting the treatment with highest conditional expectation, the decision maker targets the treatment which maximizes a general functional of the conditional potential outcome distribution, e.g., a conditional quantile, trimmed mean, or a socio-economic functional such as an inequality, welfare or poverty measure. We develop expected regret lower bounds for this problem, and construct a near minimax optimal assignment policy.||||@arxiv||||2020/01/29||||Functional Sequential Treatment Allocation with Covariates||||We consider a multi-armed bandit problem with covariates. Given a realization of the covariate vector, instead of targeting the treatment with highest conditional expectation, the decision maker...||||https://arxiv.org/abs/2001.10996v1||||cs||||
304||||None||||Systemic Risk Clustering of China Internet Financial Based on t-SNE Machine Learning Algorithm||||arXiv.org||||2019/08/30||||Systemic Risk Clustering of China Internet Financial Based on t-SNE Machine Learning Algorithm||||Chuanmin, Mi || Runjie, Xu || Qingtong, Lin||||https://arxiv.org/pdf/1909.03808||||1909.03808||||With the rapid development of Internet finance, a large number of studies have shown that Internet financial platforms have different financial systemic risk characteristics when they are subject to macroeconomic shocks or fragile internal crisis. From the perspective of regional development of Internet finance, this paper uses t-SNE machine learning algorithm to obtain data mining of China's Internet finance development index involving 31 provinces and 335 cities and regions. The conclusion of the peak and thick tail characteristics, then proposed three classification risks of Internet financial systemic risk, providing more regionally targeted recommendations for the systematic risk of Internet finance.||||@arxiv||||2019/08/30||||Systemic Risk Clustering of China Internet Financial Based on...||||With the rapid development of Internet finance, a large number of studies have shown that Internet financial platforms have different financial systemic risk characteristics when they are subject...||||https://arxiv.org/abs/1909.03808v1||||econ||||
305||||None||||Evaluating Effects of Tuition Fees: Lasso for the Case of Germany||||arXiv.org||||2019/09/18||||Evaluating Effects of Tuition Fees: Lasso for the Case of Germany||||Görgen, Konstantin || Schienle, Melanie||||https://arxiv.org/pdf/1909.08299||||1909.08299||||We study the effect of the introduction of university tuition fees on the enrollment behavior of students in Germany. For this, an appropriate Lasso-technique is crucial in order to identify the magnitude and significance of the effect due to potentially many relevant controlling factors and only a short time frame where fees existed. We show that a post-double selection strategy combined with stability selection determines a significant negative impact of fees on student enrollment and identifies relevant variables. This is in contrast to previous empirical studies and a plain linear panel regression which cannot detect any effect of tuition fees in this case. In our study, we explicitly deal with data challenges in the response variable in a transparent way and provide respective robust results. Moreover, we control for spatial cross-effects capturing the heterogeneity in the introduction scheme of fees across federal states ("Bundesländer"), which can set their own educational policy. We also confirm the validity of our Lasso approach in a comprehensive simulation study.||||@arxiv||||2019/09/18||||Evaluating Effects of Tuition Fees: Lasso for the Case of Germany||||We study the effect of the introduction of university tuition fees on the enrollment behavior of students in Germany. For this, an appropriate Lasso-technique is crucial in order to identify the...||||https://arxiv.org/abs/1909.08299v1||||econ||||
306||||None||||Estimating Treatment Effects using Multiple Surrogates: The Role of the Surrogate Score and the Surrogate Index||||arXiv.org||||2016/06/04||||Estimating Treatment Effects using Multiple Surrogates: The Role of the Surrogate Score and the Surrogate Index||||Athey, Susan || Chetty, Raj || Imbens, Guido || Kang, Hyunseung||||https://arxiv.org/pdf/1603.09326||||1603.09326||||Estimating the long-term effects of treatments is of interest in many fields. A common challenge in estimating such treatment effects is that long-term outcomes are unobserved in the time frame needed to make policy decisions. One approach to overcome this missing data problem is to analyze treatments effects on an intermediate outcome, often called a statistical surrogate, if it satisfies the condition that treatment and outcome are independent conditional on the statistical surrogate. The validity of the surrogacy condition is often controversial. Here we exploit that fact that in modern datasets, researchers often observe a large number, possibly hundreds or thousands, of intermediate outcomes, thought to lie on or close to the causal chain between the treatment and the long-term outcome of interest. Even if none of the individual proxies satisfies the statistical surrogacy criterion by itself, using multiple proxies can be useful in causal inference. We focus primarily on a setting with two samples, an experimental sample containing data about the treatment indicator and the surrogates and an observational sample containing information about the surrogates and the primary outcome. We state assumptions under which the average treatment effect be identified and estimated with a high-dimensional vector of proxies that collectively satisfy the surrogacy assumption, and derive the bias from violations of the surrogacy assumption, and show that even if the primary outcome is also observed in the experimental sample, there is still information to be gained from using surrogates.||||@arxiv||||2016/03/30||||Estimating Treatment Effects using Multiple Surrogates: The Role...||||Estimating the long-term effects of treatments is of interest in many fields. A common challenge in estimating such treatment effects is that long-term outcomes are unobserved in the time frame...||||https://arxiv.org/abs/1603.09326v2||||econ||||
307||||None||||Machine-learned patterns suggest that diversification drives economic development||||arXiv.org||||2018/12/09||||Machine-learned patterns suggest that diversification drives economic development||||Brummitt, Charles D. || Gomez-Lievano, Andres || Hausmann, Ricardo || Bonds, Matthew H.||||https://arxiv.org/pdf/1812.03534||||1812.03534||||We develop a machine-learning-based method, Principal Smooth-Dynamics Analysis (PriSDA), to identify patterns in economic development and to automate the development of new theory of economic dynamics. Traditionally, economic growth is modeled with a few aggregate quantities derived from simplified theoretical models. Here, PriSDA identifies important quantities. Applied to 55 years of data on countries' exports, PriSDA finds that what most distinguishes countries' export baskets is their diversity, with extra weight assigned to more sophisticated products. The weights are consistent with previous measures of product complexity in the literature. The second dimension of variation is a proficiency in machinery relative to agriculture. PriSDA then couples these quantities with per-capita income and infers the dynamics of the system over time. According to PriSDA, the pattern of economic development of countries is dominated by a tendency toward increased diversification. Moreover, economies appear to become richer after they diversify (i.e., diversity precedes growth). The model predicts that middle-income countries with diverse export baskets will grow the fastest in the coming decades, and that countries will converge onto intermediate levels of income and specialization. PriSDA is generalizable and may illuminate dynamics of elusive quantities such as diversity and complexity in other natural and social systems.||||@arxiv||||2018/12/09||||Machine-learned patterns suggest that diversification drives...||||We develop a machine-learning-based method, Principal Smooth-Dynamics Analysis (PriSDA), to identify patterns in economic development and to automate the development of new theory of economic...||||https://arxiv.org/abs/1812.03534v1||||econ||||
308||||None||||Asymptotic results under multiway clustering||||arXiv.org||||2018/08/03||||Asymptotic results under multiway clustering||||Davezies, Laurent || D'Haultfoeuille, Xavier || Guyonvarch, Yannick||||https://arxiv.org/pdf/1807.07925||||1807.07925||||If multiway cluster-robust standard errors are used routinely in applied economics, surprisingly few theoretical results justify this practice. This paper aims to fill this gap. We first prove, under nearly the same conditions as with i.i.d. data, the weak convergence of empirical processes under multiway clustering. This result implies central limit theorems for sample averages but is also key for showing the asymptotic normality of nonlinear estimators such as GMM estimators. We then establish consistency of various asymptotic variance estimators, including that of Cameron et al. (2011) but also a new estimator that is positive by construction. Next, we show the general consistency, for linear and nonlinear estimators, of the pigeonhole bootstrap, a resampling scheme adapted to multiway clustering. Monte Carlo simulations suggest that inference based on our two preferred methods may be accurate even with very few clusters, and significantly improve upon inference based on Cameron et al. (2011).||||@arxiv||||2018/07/20||||Asymptotic results under multiway clustering||||If multiway cluster-robust standard errors are used routinely in applied economics, surprisingly few theoretical results justify this practice. This paper aims to fill this gap. We first prove,...||||https://arxiv.org/abs/1807.07925v2||||econ||||
309||||None||||Model Selection Techniques -- An Overview||||arXiv.org||||2018/10/22||||Model Selection Techniques -- An Overview||||Ding, Jie || Tarokh, Vahid || Yang, Yuhong||||https://arxiv.org/pdf/1810.09583||||1810.09583||||In the era of big data, analysts usually explore various statistical models or machine learning methods for observed data in order to facilitate scientific discoveries or gain predictive power. Whatever data and fitting procedures are employed, a crucial step is to select the most appropriate model or method from a set of candidates. Model selection is a key ingredient in data analysis for reliable and reproducible statistical inference or prediction, and thus central to scientific studies in fields such as ecology, economics, engineering, finance, political science, biology, and epidemiology. There has been a long history of model selection techniques that arise from researches in statistics, information theory, and signal processing. A considerable number of methods have been proposed, following different philosophies and exhibiting varying performances. The purpose of this article is to bring a comprehensive overview of them, in terms of their motivation, large sample performance, and applicability. We provide integrated and practically relevant discussions on theoretical properties of state-of- the-art model selection approaches. We also share our thoughts on some controversial views on the practice of model selection.||||@arxiv||||2018/10/22||||Model Selection Techniques -- An Overview||||In the era of big data, analysts usually explore various statistical models or machine learning methods for observed data in order to facilitate scientific discoveries or gain predictive power....||||https://arxiv.org/abs/1810.09583v1||||cs||||
310||||None||||Resource Abundance and Life Expectancy||||arXiv.org||||2017/12/31||||Resource Abundance and Life Expectancy||||Sanginabadi, Bahram||||https://arxiv.org/pdf/1801.00369||||1801.00369||||This paper investigates the impacts of major natural resource discoveries since 1960 on life expectancy in the nations that they were resource poor prior to the discoveries. Previous literature explains the relation between nations wealth and life expectancy, but it has been silent about the impacts of resource discoveries on life expectancy. We attempt to fill this gap in this study. An important advantage of this study is that as the previous researchers argued resource discovery could be an exogenous variable. We use longitudinal data from 1960 to 2014 and we apply three modern empirical methods including Difference-in-Differences, Event studies, and Synthetic Control approach, to investigate the main question of the research which is 'how resource discoveries affect life expectancy?'. The findings show that resource discoveries in Ecuador, Yemen, Oman, and Equatorial Guinea have positive and significant impacts on life expectancy, but the effects for the European countries are mostly negative.||||@arxiv||||2017/12/31||||Resource Abundance and Life Expectancy||||This paper investigates the impacts of major natural resource discoveries since 1960 on life expectancy in the nations that they were resource poor prior to the discoveries. Previous literature...||||https://arxiv.org/abs/1801.00369v1||||econ||||
311||||None||||Investor Experiences and International Capital Flows||||arXiv.org||||2020/01/21||||Investor Experiences and International Capital Flows||||Malmendier, Ulrike || Pouzo, Demian || Vanasco, Victoria||||https://arxiv.org/pdf/2001.07790||||2001.07790||||We propose a novel explanation for classic international macro puzzles regarding capital flows and portfolio investment, which builds on modern macro-finance models of experience-based belief formation. Individual experiences of past macroeconomic outcomes have been shown to exert a long-lasting influence on beliefs about future realizations, and to explain domestic stock-market investment. We argue that experience effects can explain the tendency of investors to hold an over proportional fraction of their equity wealth in domestic stocks (home bias), to invest in domestic equity markets in periods of domestic crises (retrenchment), and to withdraw capital from foreign equity markets in periods of foreign crises (fickleness). Experience-based learning generates additional implications regarding the strength of these puzzles in times of higher or lower economic activity and depending on the demographic composition of market participants. We test and confirm these predictions in the data.||||@arxiv||||2020/01/21||||Investor Experiences and International Capital Flows||||We propose a novel explanation for classic international macro puzzles regarding capital flows and portfolio investment, which builds on modern macro-finance models of experience-based belief...||||https://arxiv.org/abs/2001.07790v1||||econ||||
312||||None||||State Drug Policy Effectiveness: Comparative Policy Analysis of Drug Overdose Mortality||||arXiv.org||||2019/11/18||||State Drug Policy Effectiveness: Comparative Policy Analysis of Drug Overdose Mortality||||Olson, Jarrod || Chen, Po-Hsu Allen || White, Marissa || Brennan, Nicole || Gong, Ning||||https://arxiv.org/pdf/1909.01936||||1909.01936||||Opioid overdose rates have reached an epidemic level and state-level policy innovations have followed suit in an effort to prevent overdose deaths. State-level drug law is a set of policies that may reinforce or undermine each other, and analysts have a limited set of tools for handling the policy collinearity using statistical methods. This paper uses a machine learning method called hierarchical clustering to empirically generate "policy bundles" by grouping states with similar sets of policies in force at a given time together for analysis in a 50-state, 10-year interrupted time series regression with drug overdose deaths as the dependent variable. Policy clusters were generated from 138 binomial variables observed by state and year from the Prescription Drug Abuse Policy System. Clustering reduced the policies to a set of 10 bundles. The approach allows for ranking of the relative effect of different bundles and is a tool to recommend those most likely to succeed. This study shows that a set of policies balancing Medication Assisted Treatment, Naloxone Access, Good Samaritan Laws, Medication Assisted Treatment, Prescription Drug Monitoring Programs and legalization of medical marijuana leads to a reduced number of overdose deaths, but not until its second year in force.||||@arxiv||||2019/09/03||||State Drug Policy Effectiveness: Comparative Policy Analysis of...||||Opioid overdose rates have reached an epidemic level and state-level policy innovations have followed suit in an effort to prevent overdose deaths. State-level drug law is a set of policies that...||||https://arxiv.org/abs/1909.01936v2||||cs||||
313||||None||||Universal Basic Income: The Last Bullet in the Darkness||||arXiv.org||||2019/10/12||||Universal Basic Income: The Last Bullet in the Darkness||||Rasoolinejad, Mohammad||||https://arxiv.org/pdf/1910.05658||||1910.05658||||Universal Basic Income (UBI) has recently been gaining traction. Arguments exist on both sides in favor of and against it. Like any other financial tool, UBI can be useful if used with discretion. This paper seeks to clarify how UBI affects the economy, including how it can be beneficial. The key point is to regulate the rate of UBI based on the inflation rate. This should be done by an independent institution from the executive branch of the government. If implemented correctly, UBI can add a powerful tool to the Federal Reserve toolkit. UBI can be used to reintroduce inflation to the countries which suffer long-lasting deflationary environment. UBI has the potential to decrease the wealth disparity, decrease the national debt, increase productivity, and increase comparative advantage of the economy. UBI also can substitute the current welfare systems because of its transparency and efficiency. This article focuses more on the United States, but similar ideas can be implemented in other developed nations.||||@arxiv||||2019/10/12||||Universal Basic Income: The Last Bullet in the Darkness||||Universal Basic Income (UBI) has recently been gaining traction. Arguments exist on both sides in favor of and against it. Like any other financial tool, UBI can be useful if used with discretion....||||https://arxiv.org/abs/1910.05658v1||||econ||||
314||||None||||Measuring the Demand Effects of Formal and Informal Communication : Evidence from Online Markets for Illicit Drugs||||arXiv.org||||2018/02/24||||Measuring the Demand Effects of Formal and Informal Communication : Evidence from Online Markets for Illicit Drugs||||Armona, Luis||||https://arxiv.org/pdf/1802.08778||||1802.08778||||I present evidence that communication between marketplace participants is an important influence on market demand. I find that consumer demand is approximately equally influenced by communication on both formal and informal networks- namely, product reviews and community forums. In addition, I find empirical evidence of a vendor's ability to commit to disclosure dampening the effect of communication on demand. I also find that product demand is more responsive to average customer sentiment as the number of messages grows, as may be expected in a Bayesian updating framework.||||@arxiv||||2018/02/24||||Measuring the Demand Effects of Formal and Informal Communication...||||I present evidence that communication between marketplace participants is an important influence on market demand. I find that consumer demand is approximately equally influenced by communication...||||https://arxiv.org/abs/1802.08778v1||||econ||||
315||||None||||The ABC of Simulation Estimation with Auxiliary Statistics||||arXiv.org||||2017/10/11||||The ABC of Simulation Estimation with Auxiliary Statistics||||Forneron, Jean-Jacques || Ng, Serena||||https://arxiv.org/pdf/1501.01265||||1501.01265||||The frequentist method of simulated minimum distance (SMD) is widely used in economics to estimate complex models with an intractable likelihood. In other disciplines, a Bayesian approach known as Approximate Bayesian Computation (ABC) is far more popular. This paper connects these two seemingly related approaches to likelihood-free estimation by means of a Reverse Sampler that uses both optimization and importance weighting to target the posterior distribution. Its hybrid features enable us to analyze an ABC estimate from the perspective of SMD. We show that an ideal ABC estimate can be obtained as a weighted average of a sequence of SMD modes, each being the minimizer of the deviations between the data and the model. This contrasts with the SMD, which is the mode of the average deviations. Using stochastic expansions, we provide a general characterization of frequentist estimators and those based on Bayesian computations including Laplace-type estimators. Their differences are illustrated using analytical examples and a simulation study of the dynamic panel model.||||@arxiv||||2015/01/06||||The ABC of Simulation Estimation with Auxiliary Statistics||||The frequentist method of simulated minimum distance (SMD) is widely used in economics to estimate complex models with an intractable likelihood. In other disciplines, a Bayesian approach known as...||||https://arxiv.org/abs/1501.01265v4||||econ||||
316||||None||||EMU and ECB Conflicts||||arXiv.org||||2018/07/21||||EMU and ECB Conflicts||||Mackenzie, William||||https://arxiv.org/pdf/1807.08097||||1807.08097||||In dynamical framework the conflict between government and the central bank according to the exchange Rate of payment of fixed rates and fixed rates of fixed income (EMU) convergence criteria such that the public debt / GDP ratio The method consists of calculating private public debt management in a public debt management system purpose there is no mechanism to allow naturally for this adjustment.||||@arxiv||||2018/07/21||||EMU and ECB Conflicts||||In dynamical framework the conflict between government and the central bank according to the exchange Rate of payment of fixed rates and fixed rates of fixed income (EMU) convergence criteria such...||||https://arxiv.org/abs/1807.08097v1||||econ||||
317||||None||||A Model of Competing Narratives||||arXiv.org||||2018/11/10||||A Model of Competing Narratives||||Eliaz, Kfir || Spiegler, Ran||||https://arxiv.org/pdf/1811.04232||||1811.04232||||We formalize the argument that political disagreements can be traced to a "clash of narratives". Drawing on the "Bayesian Networks" literature, we model a narrative as a causal model that maps actions into consequences, weaving a selection of other random variables into the story. An equilibrium is defined as a probability distribution over narrative-policy pairs that maximizes a representative agent's anticipatory utility, capturing the idea that public opinion favors hopeful narratives. Our equilibrium analysis sheds light on the structure of prevailing narratives, the variables they involve, the policies they sustain and their contribution to political polarization.||||@arxiv||||2018/11/10||||A Model of Competing Narratives||||We formalize the argument that political disagreements can be traced to a "clash of narratives". Drawing on the "Bayesian Networks" literature, we model a narrative as a causal model that maps...||||https://arxiv.org/abs/1811.04232v1||||econ||||
318||||None||||The Category of Node-and-Choice Forms, with Subcategories for Choice-Sequence Forms and Choice-Set Forms||||arXiv.org||||2019/04/27||||The Category of Node-and-Choice Forms, with Subcategories for Choice-Sequence Forms and Choice-Set Forms||||Streufert, Peter A.||||https://arxiv.org/pdf/1904.12085||||1904.12085||||The literature specifies extensive-form games in many styles, and eventually I hope to formally translate games across those styles. Toward that end, this paper defines $\mathbf{NCF}$, the category of node-and-choice forms. The category's objects are extensive forms in essentially any style, and the category's isomorphisms are made to accord with the literature's small handful of ad hoc style equivalences.   Further, this paper develops two full subcategories: $\mathbf{CsqF}$ for forms whose nodes are choice-sequences, and $\mathbf{CsetF}$ for forms whose nodes are choice-sets. I show that $\mathbf{NCF}$ is "isomorphically enclosed" in $\mathbf{CsqF}$ in the sense that each $\mathbf{NCF}$ form is isomorphic to a $\mathbf{CsqF}$ form. Similarly, I show that $\mathbf{CsqF_{\tilde a}}$ is isomorphically enclosed in $\mathbf{CsetF}$ in the sense that each $\mathbf{CsqF}$ form with no-absentmindedness is isomorphic to a $\mathbf{CsetF}$ form. The converses are found to be almost immediate, and the resulting equivalences unify and simplify two ad hoc style equivalences in Kline and Luckraz 2016 and Streufert 2019.   Aside from the larger agenda, this paper already makes three practical contributions. Style equivalences are made easier to derive by [1] a natural concept of isomorphic invariance and [2] the composability of isomorphic enclosures. In addition, [3] some new consequences of equivalence are systematically deduced.||||@arxiv||||2019/04/27||||The Category of Node-and-Choice Forms, with Subcategories for...||||The literature specifies extensive-form games in many styles, and eventually I hope to formally translate games across those styles. Toward that end, this paper defines $\mathbf{NCF}$, the...||||https://arxiv.org/abs/1904.12085v1||||cs||||
319||||None||||Invest or Exit? Optimal Decisions in the Face of a Declining Profit Stream||||arXiv.org||||2019/01/06||||Invest or Exit? Optimal Decisions in the Face of a Declining Profit Stream||||Kwon, H. Dharma||||https://arxiv.org/pdf/1901.01486||||1901.01486||||Even in the face of deteriorating and highly volatile demand, firms often invest in, rather than discard, aging technologies. In order to study this phenomenon, we model the firm's profit stream as a Brownian motion with negative drift. At each point in time, the firm can continue operations, or it can stop and exit the project. In addition, there is a one-time option to make an investment which boosts the project's profit rate. Using stochastic analysis, we show that the optimal policy always exists and that it is characterized by three thresholds. There are investment and exit thresholds before investment, and there is a threshold for exit after investment. We also effect a comparative statics analysis of the thresholds with respect to the drift and the volatility of the Brownian motion. When the profit boost upon investment is sufficiently large, we find a novel result: the investment threshold decreases in volatility.||||@arxiv||||2019/01/06||||Invest or Exit? Optimal Decisions in the Face of a Declining Profit Stream||||Even in the face of deteriorating and highly volatile demand, firms often invest in, rather than discard, aging technologies. In order to study this phenomenon, we model the firm's profit stream...||||https://arxiv.org/abs/1901.01486v1||||econ||||
320||||None||||Regression to the Mean's Impact on the Synthetic Control Method: Bias and Sensitivity Analysis||||arXiv.org||||2019/09/10||||Regression to the Mean's Impact on the Synthetic Control Method: Bias and Sensitivity Analysis||||Illenberger, Nicholas || Small, Dylan S. || Shaw, Pamela A.||||https://arxiv.org/pdf/1909.04706||||1909.04706||||To make informed policy recommendations from observational data, we must be able to discern true treatment effects from random noise and effects due to confounding. Difference-in-Difference techniques which match treated units to control units based on pre-treatment outcomes, such as the synthetic control approach have been presented as principled methods to account for confounding. However, we show that use of synthetic controls or other matching procedures can introduce regression to the mean (RTM) bias into estimates of the average treatment effect on the treated. Through simulations, we show RTM bias can lead to inflated type I error rates as well as decreased power in typical policy evaluation settings. Further, we provide a novel correction for RTM bias which can reduce bias and attain appropriate type I error rates. This correction can be used to perform a sensitivity analysis which determines how results may be affected by RTM. We use our proposed correction and sensitivity analysis to reanalyze data concerning the effects of California's Proposition 99, a large-scale tobacco control program, on statewide smoking rates.||||@arxiv||||2019/09/10||||Regression to the Mean's Impact on the Synthetic Control...||||To make informed policy recommendations from observational data, we must be able to discern true treatment effects from random noise and effects due to confounding. Difference-in-Difference...||||https://arxiv.org/abs/1909.04706v1||||econ||||
321||||None||||How to avoid the zero-power trap in testing for correlation||||arXiv.org||||2018/12/27||||How to avoid the zero-power trap in testing for correlation||||Preinerstorfer, David||||https://arxiv.org/pdf/1812.10752||||1812.10752||||In testing for correlation of the errors in regression models the power of tests can be very low for strongly correlated errors. This counterintuitive phenomenon has become known as the "zero-power trap". Despite a considerable amount of literature devoted to this problem, mainly focusing on its detection, a convincing solution has not yet been found. In this article we first discuss theoretical results concerning the occurrence of the zero-power trap phenomenon. Then, we suggest and compare three ways to avoid it. Given an initial test that suffers from the zero-power trap, the method we recommend for practice leads to a modified test whose power converges to one as the correlation gets very strong. Furthermore, the modified test has approximately the same power function as the initial test, and thus approximately preserves all of its optimality properties. We also provide some numerical illustrations in the context of testing for network generated correlation.||||@arxiv||||2018/12/27||||How to avoid the zero-power trap in testing for correlation||||In testing for correlation of the errors in regression models the power of tests can be very low for strongly correlated errors. This counterintuitive phenomenon has become known as the...||||https://arxiv.org/abs/1812.10752v1||||econ||||
322||||None||||A General Method for Demand Inversion||||arXiv.org||||2018/02/27||||A General Method for Demand Inversion||||Li, Lixiong||||https://arxiv.org/pdf/1802.04444||||1802.04444||||This paper describes a numerical method to solve for mean product qualities which equates the real market share to the market share predicted by a discrete choice model. The method covers a general class of discrete choice model, including the pure characteristics model in Berry and Pakes(2007) and the random coefficient logit model in Berry et al.(1995) (hereafter BLP). The method transforms the original market share inversion problem to an unconstrained convex minimization problem, so that any convex programming algorithm can be used to solve the inversion. Moreover, such results also imply that the computational complexity of inverting a demand model should be no more than that of a convex programming problem. In simulation examples, I show the method outperforms the contraction mapping algorithm in BLP. I also find the method remains robust in pure characteristics models with near-zero market shares.||||@arxiv||||2018/02/13||||A General Method for Demand Inversion||||This paper describes a numerical method to solve for mean product qualities which equates the real market share to the market share predicted by a discrete choice model. The method covers a...||||https://arxiv.org/abs/1802.04444v3||||econ||||
323||||None||||Suboptimal Provision of Privacy and Statistical Accuracy When They are Public Goods||||arXiv.org||||2019/06/21||||Suboptimal Provision of Privacy and Statistical Accuracy When They are Public Goods||||Abowd, John M. || Schmutte, Ian M. || Sexton, William || Vilhuber, Lars||||https://arxiv.org/pdf/1906.09353||||1906.09353||||With vast databases at their disposal, private tech companies can compete with public statistical agencies to provide population statistics. However, private companies face different incentives to provide high-quality statistics and to protect the privacy of the people whose data are used. When both privacy protection and statistical accuracy are public goods, private providers tend to produce at least one suboptimally, but it is not clear which. We model a firm that publishes statistics under a guarantee of differential privacy. We prove that provision by the private firm results in inefficiently low data quality in this framework.||||@arxiv||||2019/06/21||||Suboptimal Provision of Privacy and Statistical Accuracy When They...||||With vast databases at their disposal, private tech companies can compete with public statistical agencies to provide population statistics. However, private companies face different incentives to...||||https://arxiv.org/abs/1906.09353v1||||cs||||
324||||None||||Dynamic and granular loss reserving with copulae||||arXiv.org||||2018/01/05||||Dynamic and granular loss reserving with copulae||||Maciak, Matúš || Okhrin, Ostap || Pešta, Michal||||https://arxiv.org/pdf/1801.01792||||1801.01792||||An intensive research sprang up for stochastic methods in insurance during the past years. To meet all future claims rising from policies, it is requisite to quantify the outstanding loss liabilities. Loss reserving methods based on aggregated data from run-off triangles are predominantly used to calculate the claims reserves. Conventional reserving techniques have some disadvantages: loss of information from the policy and the claim's development due to the aggregation, zero or negative cells in the triangle; usually small number of observations in the triangle; only few observations for recent accident years; and sensitivity to the most recent paid claims.   To overcome these dilemmas, granular loss reserving methods for individual claim-by-claim data will be derived. Reserves' estimation is a crucial part of the risk valuation process, which is now a front burner in economics. Since there is a growing demand for prediction of total reserves for different types of claims or even multiple lines of business, a time-varying copula framework for granular reserving will be established.||||@arxiv||||2018/01/05||||Dynamic and granular loss reserving with copulae||||An intensive research sprang up for stochastic methods in insurance during the past years. To meet all future claims rising from policies, it is requisite to quantify the outstanding loss...||||https://arxiv.org/abs/1801.01792v1||||econ||||
325||||None||||Wasserstein Index Generation Model: Automatic Generation of Time-series Index with Application to Economic Policy Uncertainty||||arXiv.org||||2019/11/25||||Wasserstein Index Generation Model: Automatic Generation of Time-series Index with Application to Economic Policy Uncertainty||||Xie, Fangzhou||||https://arxiv.org/pdf/1908.04369||||1908.04369||||I propose a novel method, the Wasserstein Index Generation model (WIG), to generate a public sentiment index automatically. To test the model`s effectiveness, an application to generate Economic Policy Uncertainty (EPU) index is showcased.||||@arxiv||||2019/08/12||||Wasserstein Index Generation Model: Automatic Generation of...||||I propose a novel method, the Wasserstein Index Generation model (WIG), to generate a public sentiment index automatically. To test the model`s effectiveness, an application to generate Economic...||||https://arxiv.org/abs/1908.04369v4||||cs||||
326||||None||||Buy-Online-and-Pick-up-in-Store in Omnichannel Retailing||||arXiv.org||||2019/09/05||||Buy-Online-and-Pick-up-in-Store in Omnichannel Retailing||||Kusuda, Yasuyuki||||https://arxiv.org/pdf/1909.00822||||1909.00822||||In this paper, we extend the model of Gao and Su (2016) and consider an omnichannel strategy in which inventory can be replenished when a retailer sells only in physical stores. With "buy-online-and-pick-up-in-store" (BOPS) having been introduced, consumers can choose to buy directly online, buy from a retailer using BOPS, or go directly to a store to make purchases without using BOPS. The retailer is able to select the inventory level to maximize the probability of inventory availability at the store. Furthermore, the retailer can incur an additional cost to reduce the BOPS ordering lead time, which results in a lowered hassle cost for consumers who use BOPS. In conclusion, we found that there are two types of equilibrium: that in which all consumers go directly to the store without using BOPS and that in which all consumers use BOPS.||||@arxiv||||2019/09/02||||Buy-Online-and-Pick-up-in-Store in Omnichannel Retailing||||In this paper, we extend the model of Gao and Su (2016) and consider an omnichannel strategy in which inventory can be replenished when a retailer sells only in physical stores. With...||||https://arxiv.org/abs/1909.00822v2||||econ||||
327||||None||||A Uniform Bound of the Operator Norm of Random Element Matrices and Operator Norm Minimizing Estimation||||arXiv.org||||2019/05/03||||A Uniform Bound of the Operator Norm of Random Element Matrices and Operator Norm Minimizing Estimation||||Moon, Hyungsik Roger||||https://arxiv.org/pdf/1905.01096||||1905.01096||||In this paper, we derive a uniform stochastic bound of the operator norm (or equivalently, the largest singular value) of random matrices whose elements are indexed by parameters. As an application, we propose a new estimator that minimizes the operator norm of the matrix that consists of the moment functions. We show the consistency of the estimator.||||@arxiv||||2019/05/03||||A Uniform Bound of the Operator Norm of Random Element Matrices...||||In this paper, we derive a uniform stochastic bound of the operator norm (or equivalently, the largest singular value) of random matrices whose elements are indexed by parameters. As an...||||https://arxiv.org/abs/1905.01096v1||||econ||||
328||||None||||Toy Model for Large Non-Symmetric Random Matrices||||arXiv.org||||2010/04/26||||Toy Model for Large Non-Symmetric Random Matrices||||Snarska, Małgorzata||||https://arxiv.org/pdf/1004.4522||||1004.4522||||Non-symmetric rectangular correlation matrices occur in many problems in economics. We test the method of extracting statistically meaningful correlations between input and output variables of large dimensionality and build a toy model for artificially included correlations in large random time series.The results are then applied to analysis of polish macroeconomic data and can be used as an alternative to classical cointegration approach.||||@arxiv||||2010/04/26||||Toy Model for Large Non-Symmetric Random Matrices||||Non-symmetric rectangular correlation matrices occur in many problems in economics. We test the method of extracting statistically meaningful correlations between input and output variables of...||||https://arxiv.org/abs/1004.4522v1||||econ||||
329||||None||||Heterogeneous structural breaks in panel data models||||arXiv.org||||2018/11/24||||Heterogeneous structural breaks in panel data models||||Okui, Ryo || Wang, Wendun||||https://arxiv.org/pdf/1801.04672||||1801.04672||||This paper develops a new model and estimation procedure for panel data that allows us to identify heterogeneous structural breaks. We model individual heterogeneity using a grouped pattern. For each group, we allow common structural breaks in the coefficients. However, the number, timing, and size of these breaks can differ across groups. We develop a hybrid estimation procedure of the grouped fixed effects approach and adaptive group fused Lasso. We show that our method can consistently identify the latent group structure, detect structural breaks, and estimate the regression parameters. Monte Carlo results demonstrate the good performance of the proposed method in finite samples. An empirical application to the relationship between income and democracy illustrates the importance of considering heterogeneous structural breaks.||||@arxiv||||2018/01/15||||Heterogeneous structural breaks in panel data models||||This paper develops a new model and estimation procedure for panel data that allows us to identify heterogeneous structural breaks. We model individual heterogeneity using a grouped pattern. For...||||https://arxiv.org/abs/1801.04672v2||||econ||||
330||||None||||Hospitality Students' Perceptions towards Working in Hotels: a case study of the faculty of tourism and hotels in Alexandria University||||arXiv.org||||2018/07/22||||Hospitality Students' Perceptions towards Working in Hotels: a case study of the faculty of tourism and hotels in Alexandria University||||El-Houshy, Sayed||||https://arxiv.org/pdf/1807.09660||||1807.09660||||The tourism and hospitality industry worldwide has been confronted with the problem of attracting and retaining quality employees. If today's students are to become the effective practitioners of tomorrow, it is fundamental to understand their perceptions of tourism employment. Therefore, this research aims at investigating the perceptions of hospitality students at the Faculty of Tourism in Alexandria University towards the industry as a career choice. A self-administrated questionnaire was developed to rate the importance of 20 factors in influencing career choice, and the extent to which hospitality as a career offers these factors. From the results, it is clear that students generally do not believe that the hospitality career will offer them the factors they found important. However, most of respondents (70.6%) indicated that they would work in the industry after graduation. Finally, a set of specific remedial actions that hospitality stakeholders could initiate to improve the perceptions of hospitality career are discussed.||||@arxiv||||2018/07/22||||Hospitality Students' Perceptions towards Working in Hotels: a...||||The tourism and hospitality industry worldwide has been confronted with the problem of attracting and retaining quality employees. If today's students are to become the effective practitioners of...||||https://arxiv.org/abs/1807.09660v1||||econ||||
331||||None||||Pursuing More Sustainable Energy Consumption by Analyzing Sectoral Direct and Indirect Energy Use in Malaysia: An Input-Output Analysis||||arXiv.org||||2020/01/07||||Pursuing More Sustainable Energy Consumption by Analyzing Sectoral Direct and Indirect Energy Use in Malaysia: An Input-Output Analysis||||Harun, Mukaramah||||https://arxiv.org/pdf/2001.02508||||2001.02508||||Malaysia is experiencing ever increasing domestic energy consumption. This study is an attempt at analyzing the changes in sectoral energy intensities in Malaysia for the period 1995 to 2011. The study quantifies the sectoral total, direct, and indirect energy intensities to track the sectors that are responsible for the increasing energy consumption. The energy input-output model which is a frontier method for examining resource embodiments in goods and services on a sectoral scale that is popular among scholars has been applied in this study.||||@arxiv||||2020/01/07||||Pursuing More Sustainable Energy Consumption by Analyzing Sectoral...||||Malaysia is experiencing ever increasing domestic energy consumption. This study is an attempt at analyzing the changes in sectoral energy intensities in Malaysia for the period 1995 to 2011. The...||||https://arxiv.org/abs/2001.02508v1||||econ||||
332||||None||||Locally Robust Semiparametric Estimation||||arXiv.org||||2018/05/31||||Locally Robust Semiparametric Estimation||||Chernozhukov, Victor || Escanciano, Juan Carlos || Ichimura, Hidehiko || Newey, Whitney K. || Robins, James M.||||https://arxiv.org/pdf/1608.00033||||1608.00033||||We give a general construction of debiased/locally robust/orthogonal (LR) moment functions for GMM, where the derivative with respect to first step nonparametric estimation is zero and equivalently first step estimation has no effect on the influence function. This construction consists of adding an estimator of the influence function adjustment term for first step nonparametric estimation to identifying or original moment conditions. We also give numerical methods for estimating LR moment functions that do not require an explicit formula for the adjustment term.   LR moment conditions have reduced bias and so are important when the first step is machine learning. We derive LR moment conditions for dynamic discrete choice based on first step machine learning estimators of conditional choice probabilities. We provide simple and general asymptotic theory for LR estimators based on sample splitting. This theory uses the additive decomposition of LR moment conditions into an identifying condition and a first step influence adjustment. Our conditions require only mean square consistency and a few (generally either one or two) readily interpretable rate conditions.   LR moment functions have the advantage of being less sensitive to first step estimation. Some LR moment functions are also doubly robust meaning they hold if one first step is incorrect. We give novel classes of doubly robust moment functions and characterize double robustness. For doubly robust estimators our asymptotic theory only requires one rate condition.||||@arxiv||||2016/07/29||||Locally Robust Semiparametric Estimation||||We give a general construction of debiased/locally robust/orthogonal (LR) moment functions for GMM, where the derivative with respect to first step nonparametric estimation is zero and...||||https://arxiv.org/abs/1608.00033v2||||econ||||
333||||None||||Coalition-structured governance improves cooperation to provide public goods||||arXiv.org||||2019/10/24||||Coalition-structured governance improves cooperation to provide public goods||||Vasconcelos, Vítor V. || Hannam, Phillip M. || Levin, Simon A. || Pacheco, Jorge M.||||https://arxiv.org/pdf/1910.11337||||1910.11337||||While the benefits of common and public goods are shared, they tend to be scarce when contributions are provided voluntarily. Failure to cooperate in the provision or preservation of these goods is fundamental to sustainability challenges, ranging from local fisheries to global climate change. In the real world, such cooperative dilemmas occur in multiple interactions with complex strategic interests and frequently without full information. We argue that voluntary cooperation enabled across multiple coalitions (akin to polycentricity) not only facilitates greater generation of non-excludable public goods, but may also allow evolution toward a more cooperative, stable, and inclusive approach to governance. Contrary to any previous study, we show that these merits of multi-coalition governance are far more general than the singular examples occurring in the literature, and are robust under diverse conditions of excludability, congestability of the non-excludable public good, and arbitrary shapes of the return-to-contribution function. We first confirm the intuition that a single coalition without enforcement and with players pursuing their self-interest without knowledge of returns to contribution is prone to cooperative failure. Next, we demonstrate that the same pessimistic model but with a multi-coalition structure of governance experiences relatively higher cooperation by enabling recognition of marginal gains of cooperation in the game at stake. In the absence of enforcement, public-goods regimes that evolve through a proliferation of voluntary cooperative forums can maintain and increase cooperation more successfully than singular, inclusive regimes.||||@arxiv||||2019/10/24||||Coalition-structured governance improves cooperation to provide...||||While the benefits of common and public goods are shared, they tend to be scarce when contributions are provided voluntarily. Failure to cooperate in the provision or preservation of these goods...||||https://arxiv.org/abs/1910.11337v1||||econ||||
334||||None||||Indirect Inference for Locally Stationary Models||||arXiv.org||||2019/06/05||||Indirect Inference for Locally Stationary Models||||Frazier, David || Koo, Bonsoo||||https://arxiv.org/pdf/1906.01768||||1906.01768||||We propose the use of indirect inference estimation for inference in locally stationary models. We develop a local indirect inference algorithm and establish the asymptotic properties of the proposed estimator. Due to the nonparametric nature of the model under study, the resulting estimators display nonparametric rates of convergence and behavior. We validate our methodology via simulation studies in the confines of a locally stationary moving average model and a locally stationary multiplicative stochastic volatility model. An application of the methodology gives evidence of non-linear, time-varying volatility for monthly returns on the Fama-French portfolios.||||@arxiv||||2019/06/05||||Indirect Inference for Locally Stationary Models||||We propose the use of indirect inference estimation for inference in locally stationary models. We develop a local indirect inference algorithm and establish the asymptotic properties of the...||||https://arxiv.org/abs/1906.01768v1||||econ||||
335||||None||||lpdensity: Local Polynomial Density Estimation and Inference||||arXiv.org||||2019/06/15||||lpdensity: Local Polynomial Density Estimation and Inference||||Cattaneo, Matias D. || Jansson, Michael || Ma, Xinwei||||https://arxiv.org/pdf/1906.06529||||1906.06529||||Density estimation and inference methods are widely used in empirical work. When the data has compact support, as all empirical applications de facto do, conventional kernel-based density estimators are inapplicable near or at the boundary because of their well known boundary bias. Alternative smoothing methods are available to handle boundary points in density estimation, but they all require additional tuning parameter choices or other typically ad hoc modifications depending on the evaluation point and/or approach considered. This article discusses the R and Stata package lpdensity implementing a novel local polynomial density estimator proposed in Cattaneo, Jansson and Ma (2019), which is boundary adaptive, fully data-driven and automatic, and requires only the choice of one tuning parameter. The methods implemented also cover local polynomial estimation of the cumulative distribution function and density derivatives, as well as several other theoretical and methodological results. In addition to point estimation and graphical procedures, the package offers consistent variance estimators, mean squared error optimal bandwidth selection, and robust bias-corrected inference. A comparison with several other density estimation packages and functions available in R using a Monte Carlo experiment is provided.||||@arxiv||||2019/06/15||||lpdensity: Local Polynomial Density Estimation and Inference||||Density estimation and inference methods are widely used in empirical work. When the data has compact support, as all empirical applications de facto do, conventional kernel-based density...||||https://arxiv.org/abs/1906.06529v1||||econ||||
336||||None||||Does Non-Farm Income Improve The Poverty and Income Inequality Among Agricultural Household In Rural Kedah?||||arXiv.org||||2020/01/07||||Does Non-Farm Income Improve The Poverty and Income Inequality Among Agricultural Household In Rural Kedah?||||Mata, Siti Hadijah Che || Jalil, Ahmad Zafarullah Abdul || Harun, Mukaramah||||https://arxiv.org/pdf/2001.03487||||2001.03487||||This paper used a primary data collected through a surveys among farmers in rural Kedah to examine the effect of non farm income on poverty and income inequality. This paper employed two method, for the first objective which is to examine the impact of non farm income to poverty, we used poverty decomposition techniques - Foster, greer and Thorbecke (FGT) as has been done by Adams (2004). For the second objective, which is to examine the impact of non farm income to income inequality, we used Gini decomposition techniques.||||@arxiv||||2020/01/07||||Does Non-Farm Income Improve The Poverty and Income Inequality...||||This paper used a primary data collected through a surveys among farmers in rural Kedah to examine the effect of non farm income on poverty and income inequality. This paper employed two method,...||||https://arxiv.org/abs/2001.03487v1||||econ||||
337||||None||||The perils of automated fitting of datasets: the case of a wind turbine cost model||||arXiv.org||||2019/05/21||||The perils of automated fitting of datasets: the case of a wind turbine cost model||||Klöckl, Claude || Gruber, Katharina || Regner, Peter || Wehrle, Sebastian || Schmidt, Johannes||||https://arxiv.org/pdf/1905.08870||||1905.08870||||Rinne et al. conduct an interesting analysis of the impact of wind turbine technology and land-use on wind power potentials, which allows profound insights into each factors contribution to overall potentials. The paper presents a detailed model of site-specific wind turbine investment cost (i.e. road- and grid access costs) complemented by a model used to estimate site-independent costs. We believe that propose a cutting edge model of site-specific investment costs. However, the site-independent cost model is flawed in our opinion. This flaw most likely does not impact the results presented in the paper, although we expect a considerable generalization error. Thus the application of the wind turbine cost model in other contexts may lead to unreasonable results. More generally, the derivation of the wind turbine cost model serves as an example of how applications of automated regression analysis can go wrong.||||@arxiv||||2019/05/21||||The perils of automated fitting of datasets: the case of a wind...||||Rinne et al. conduct an interesting analysis of the impact of wind turbine technology and land-use on wind power potentials, which allows profound insights into each factors contribution to...||||https://arxiv.org/abs/1905.08870v1||||cs||||
338||||None||||Allowance prices in the EU ETS -- fundamental price drivers and the recent upward trend||||arXiv.org||||2019/11/20||||Allowance prices in the EU ETS -- fundamental price drivers and the recent upward trend||||Friedrich, Marina || Pahle, Michael||||https://arxiv.org/pdf/1906.10572||||1906.10572||||In 2017 allowance prices in the EU Emissions Trading Scheme (ETS) have started to rally from persistently low levels in previous years. Market observers attribute this development to the ETS reform, naming anticipation of the tightening of allowance supply through the Market Stability Reserve (MSR) and speculative buying as main price drivers. The former suggests an increasing role of fundamentals, while the latter could give rise to investor overreaction and mispricing. Analyzing the price run up using the anticipative dating algorithm by Phillips et al. (2015), we obtain first empirical results that detect explosive behavior in prices from February 2018 on. Such behavior could reflect an adaption process, or be an indicator for market exuberance during the inflationary phase of a bubble. Testing for the former, we neither find that abatement-related fundamentals display equally explosive behavior, nor a change in coefficients using a complementary time-varying non-parametrical regression model. Finally, we examine the two potential explanations from a theoretical angle to shed light on candidate underlying mechanisms. We conclude that no explanation can be ruled out, but further research is needed to increase confidence in either.||||@arxiv||||2019/06/25||||Allowance prices in the EU ETS -- fundamental price drivers and...||||In 2017 allowance prices in the EU Emissions Trading Scheme (ETS) have started to rally from persistently low levels in previous years. Market observers attribute this development to the ETS...||||https://arxiv.org/abs/1906.10572v4||||econ||||
339||||None||||Are Bitcoins price predictable? Evidence from machine learning techniques using technical indicators||||arXiv.org||||2019/09/03||||Are Bitcoins price predictable? Evidence from machine learning techniques using technical indicators||||Gyamerah, Samuel Asante||||https://arxiv.org/pdf/1909.01268||||1909.01268||||The uncertainties in future Bitcoin price make it difficult to accurately predict the price of Bitcoin. Accurately predicting the price for Bitcoin is therefore important for decision-making process of investors and market players in the cryptocurrency market. Using historical data from 01/01/2012 to 16/08/2019, machine learning techniques (Generalized linear model via penalized maximum likelihood, random forest, support vector regression with linear kernel, and stacking ensemble) were used to forecast the price of Bitcoin. The prediction models employed key and high dimensional technical indicators as the predictors. The performance of these techniques were evaluated using mean absolute percentage error (MAPE), root mean square error (RMSE), mean absolute error (MAE), and coefficient of determination (R-squared). The performance metrics revealed that the stacking ensemble model with two base learner (random forest and generalized linear model via penalized maximum likelihood) and support vector regression with linear kernel as meta-learner was the optimal model for forecasting Bitcoin price. The MAPE, RMSE, MAE, and R-squared values for the stacking ensemble model were 0.0191%, 15.5331 USD, 124.5508 USD, and 0.9967 respectively. These values show a high degree of reliability in predicting the price of Bitcoin using the stacking ensemble model. Accurately predicting the future price of Bitcoin will yield significant returns for investors and market players in the cryptocurrency market.||||@arxiv||||2019/09/03||||Are Bitcoins price predictable? Evidence from machine learning...||||The uncertainties in future Bitcoin price make it difficult to accurately predict the price of Bitcoin. Accurately predicting the price for Bitcoin is therefore important for decision-making...||||https://arxiv.org/abs/1909.01268v1||||econ||||
340||||None||||Mining the Automotive Industry: A Network Analysis of Corporate Positioning and Technological Trends||||arXiv.org||||2020/01/08||||Mining the Automotive Industry: A Network Analysis of Corporate Positioning and Technological Trends||||Stoehr, Niklas || Braesemann, Fabian || Frommelt, Michael || Zhou, Shi||||https://arxiv.org/pdf/1912.10097||||1912.10097||||The digital transformation is driving revolutionary innovations and new market entrants threaten established sectors of the economy such as the automotive industry. Following the need for monitoring shifting industries, we present a network-centred analysis of car manufacturer web pages. Solely exploiting publicly-available information, we construct large networks from web pages and hyperlinks. The network properties disclose the internal corporate positioning of the three largest automotive manufacturers, Toyota, Volkswagen and Hyundai with respect to innovative trends and their international outlook. We tag web pages concerned with topics like e-mobility and environment or autonomous driving, and investigate their relevance in the network. Sentiment analysis on individual web pages uncovers a relationship between page linking and use of positive language, particularly with respect to innovative trends. Web pages of the same country domain form clusters of different size in the network that reveal strong correlations with sales market orientation. Our approach maintains the web content's hierarchical structure imposed by the web page networks. It, thus, presents a method to reveal hierarchical structures of unstructured text content obtained from web scraping. It is highly transparent, reproducible and data driven, and could be used to gain complementary insights into innovative strategies of firms and competitive landscapes, which would not be detectable by the analysis of web content alone.||||@arxiv||||2019/12/20||||Mining the Automotive Industry: A Network Analysis of Corporate...||||The digital transformation is driving revolutionary innovations and new market entrants threaten established sectors of the economy such as the automotive industry. Following the need for...||||https://arxiv.org/abs/1912.10097v2||||cs||||
341||||None||||Identification and Estimation of Spillover Effects in Randomized Experiments||||arXiv.org||||2019/06/07||||Identification and Estimation of Spillover Effects in Randomized Experiments||||Vazquez-Bare, Gonzalo||||https://arxiv.org/pdf/1711.02745||||1711.02745||||See manuscript for full abstract.||||@arxiv||||2017/11/07||||Identification and Estimation of Spillover Effects in Randomized...||||See manuscript for full abstract.||||https://arxiv.org/abs/1711.02745v4||||econ||||
342||||None||||Predicting bubble bursts in oil prices using mixed causal-noncausal models||||arXiv.org||||2019/11/25||||Predicting bubble bursts in oil prices using mixed causal-noncausal models||||Hecq, Alain || Voisin, Elisa||||https://arxiv.org/pdf/1911.10916||||1911.10916||||This paper investigates oil price series using mixed causal-noncausal autoregressive (MAR) models, namely dynamic processes that depend not only on their lags but also on their leads. MAR models have been successfully implemented on commodity prices as they allow to generate nonlinear features such as speculative bubbles. We estimate the probabilities that bubbles in oil price series burst once the series enter an explosive phase. To do so we first evaluate how to adequately detrend nonstationary oil price series while preserving the bubble patterns observed in the raw data. The impact of different filters on the identification of MAR models as well as on forecasting bubble events is investigated using Monte Carlo simulations. We illustrate our findings on WTI and Brent monthly series.||||@arxiv||||2019/11/25||||Predicting bubble bursts in oil prices using mixed causal-noncausal models||||This paper investigates oil price series using mixed causal-noncausal autoregressive (MAR) models, namely dynamic processes that depend not only on their lags but also on their leads. MAR models...||||https://arxiv.org/abs/1911.10916v1||||econ||||
343||||None||||Externalities in Knowledge Production: Evidence from a Randomized Field Experiment||||arXiv.org||||2019/03/02||||Externalities in Knowledge Production: Evidence from a Randomized Field Experiment||||Hinnosaar, Marit || Hinnosaar, Toomas || Kummer, Michael || Slivko, Olga||||https://arxiv.org/pdf/1903.01861||||1903.01861||||Are there positive or negative externalities in knowledge production? Do current contributions to knowledge production increase or decrease the future growth of knowledge? We use a randomized field experiment, which added relevant content to some pages in Wikipedia while leaving similar pages unchanged. We find that the addition of content has a negligible impact on the subsequent long-run growth of content. Our results have implications for information seeding and incentivizing contributions, implying that additional content does not generate sizable externalities by inspiring nor discouraging future contributions.||||@arxiv||||2019/03/02||||Externalities in Knowledge Production: Evidence from a Randomized...||||Are there positive or negative externalities in knowledge production? Do current contributions to knowledge production increase or decrease the future growth of knowledge? We use a randomized...||||https://arxiv.org/abs/1903.01861v1||||cs||||
344||||None||||Influencing factors that determine the usage of the crowd-shipping services||||arXiv.org||||2019/02/22||||Influencing factors that determine the usage of the crowd-shipping services||||Le, Tho V. || Ukkusuri, Satish V.||||https://arxiv.org/pdf/1902.08681||||1902.08681||||The objective of this study is to understand how senders choose shipping services for different products, given the availability of both emerging crowd-shipping (CS) and traditional carriers in a logistics market. Using data collected from a US survey, Random Utility Maximization (RUM) and Random Regret Minimization (RRM) models have been employed to reveal factors that influence the diversity of decisions made by senders. Shipping costs, along with additional real-time services such as courier reputations, tracking info, e-notifications, and customized delivery time and location, have been found to have remarkable impacts on senders' choices. Interestingly, potential senders were willing to pay more to ship grocery items such as food, beverages, and medicines by CS services. Moreover, the real-time services have low elasticities, meaning that only a slight change in those services will lead to a change in sender-behavior. Finally, data-science techniques were used to assess the performance of the RUM and RRM models and found to have similar accuracies. The findings from this research will help logistics firms address potential market segments, prepare service configurations to fulfill senders' expectations, and develop effective business operations strategies.||||@arxiv||||2019/02/22||||Influencing factors that determine the usage of the crowd-shipping services||||The objective of this study is to understand how senders choose shipping services for different products, given the availability of both emerging crowd-shipping (CS) and traditional carriers in a...||||https://arxiv.org/abs/1902.08681v1||||econ||||
345||||None||||Volatility Models Applied to Geophysics and High Frequency Financial Market Data||||arXiv.org||||2019/01/26||||Volatility Models Applied to Geophysics and High Frequency Financial Market Data||||Mariani, Maria C || Bhuiyan, Md Al Masum || Tweneboah, Osei K || Gonzalez-Huizar, Hector || Florescu, Ionut||||https://arxiv.org/pdf/1901.09145||||1901.09145||||This work is devoted to the study of modeling geophysical and financial time series. A class of volatility models with time-varying parameters is presented to forecast the volatility of time series in a stationary environment. The modeling of stationary time series with consistent properties facilitates prediction with much certainty. Using the GARCH and stochastic volatility model, we forecast one-step-ahead suggested volatility with +/- 2 standard prediction errors, which is enacted via Maximum Likelihood Estimation. We compare the stochastic volatility model relying on the filtering technique as used in the conditional volatility with the GARCH model. We conclude that the stochastic volatility is a better forecasting tool than GARCH (1, 1), since it is less conditioned by autoregressive past information.||||@arxiv||||2019/01/26||||Volatility Models Applied to Geophysics and High Frequency...||||This work is devoted to the study of modeling geophysical and financial time series. A class of volatility models with time-varying parameters is presented to forecast the volatility of time...||||https://arxiv.org/abs/1901.09145v1||||econ||||
346||||None||||Mesoscale impact of trader psychology on stock markets: a multi-agent AI approach||||arXiv.org||||2019/10/10||||Mesoscale impact of trader psychology on stock markets: a multi-agent AI approach||||Lussange, J. || Palminteri, S. || Bourgeois-Gironde, S. || Gutkin, B.||||https://arxiv.org/pdf/1910.10099||||1910.10099||||Recent advances in the fields of machine learning and neurofinance have yielded new exciting research perspectives in practical inference of behavioural economy in financial markets and microstructure study. We here present the latest results from a recently published stock market simulator built around a multi-agent system architecture, in which each agent is an autonomous investor trading stocks by reinforcement learning (RL) via a centralised double-auction limit order book. The RL framework allows for the implementation of specific behavioural and cognitive traits known to trader psychology, and thus to study the impact of these traits on the whole stock market at the mesoscale. More precisely, we narrowed our agent design to three such psychological biases known to have a direct correspondence with RL theory, namely delay discounting, greed, and fear. We compared ensuing simulated data to real stock market data over the past decade or so, and find that market stability benefits from larger populations of agents prone to delay discounting and most astonishingly, to greed.||||@arxiv||||2019/10/10||||Mesoscale impact of trader psychology on stock markets: a...||||Recent advances in the fields of machine learning and neurofinance have yielded new exciting research perspectives in practical inference of behavioural economy in financial markets and...||||https://arxiv.org/abs/1910.10099v1||||econ||||
347||||None||||Predicting Auction Price of Vehicle License Plate with Deep Residual Learning||||arXiv.org||||2019/10/08||||Predicting Auction Price of Vehicle License Plate with Deep Residual Learning||||Chow, Vinci||||https://arxiv.org/pdf/1910.04879||||1910.04879||||Due to superstition, license plates with desirable combinations of characters are highly sought after in China, fetching prices that can reach into the millions in government-held auctions. Despite the high stakes involved, there has been essentially no attempt to provide price estimates for license plates. We present an end-to-end neural network model that simultaneously predict the auction price, gives the distribution of prices and produces latent feature vectors. While both types of neural network architectures we consider outperform simpler machine learning methods, convolutional networks outperform recurrent networks for comparable training time or model complexity. The resulting model powers our online price estimator and search engine.||||@arxiv||||2019/10/08||||Predicting Auction Price of Vehicle License Plate with Deep...||||Due to superstition, license plates with desirable combinations of characters are highly sought after in China, fetching prices that can reach into the millions in government-held auctions....||||https://arxiv.org/abs/1910.04879v1||||cs||||
348||||None||||How fragile are information cascades?||||arXiv.org||||2018/02/21||||How fragile are information cascades?||||Peres, Yuval || Racz, Miklos Z. || Sly, Allan || Stuhl, Izabella||||https://arxiv.org/pdf/1711.04024||||1711.04024||||It is well known that sequential decision making may lead to information cascades. That is, when agents make decisions based on their private information, as well as observing the actions of those before them, then it might be rational to ignore their private signal and imitate the action of previous individuals. If the individuals are choosing between a right and a wrong state, and the initial actions are wrong, then the whole cascade will be wrong. This issue is due to the fact that cascades can be based on very little information.   We show that if agents occasionally disregard the actions of others and base their action only on their private information, then wrong cascades can be avoided. Moreover, we study the optimal asymptotic rate at which the error probability at time $t$ can go to zero. The optimal policy is for the player at time $t$ to follow their private information with probability $p_{t} = c/t$, leading to a learning rate of $c'/t$, where the constants $c$ and $c'$ are explicit.||||@arxiv||||2017/11/10||||How fragile are information cascades?||||It is well known that sequential decision making may lead to information cascades. That is, when agents make decisions based on their private information, as well as observing the actions of those...||||https://arxiv.org/abs/1711.04024v2||||cs||||
349||||None||||Modelling transfer profits as externalities in a cooperative game-theoretic model of natural gas networks||||arXiv.org||||2019/02/01||||Modelling transfer profits as externalities in a cooperative game-theoretic model of natural gas networks||||Csercsik, Dávid || Hubert, Franz || Sziklai, Balázs R. || Kóczy, László Á.||||https://arxiv.org/pdf/1901.11435||||1901.11435||||Existing cooperative game theoretic studies of bargaining power in gas pipeline systems are based on the so called characteristic function form (CFF). This approach is potentially misleading if some pipelines fall under regulated third party access (TPA). TPA, which is by now the norm in the EU, obliges the owner of a pipeline to transport gas for others, provided they pay a regulated transport fee. From a game theoretic perspective, this institutional setting creates so called "externalities," the description of which requires partition function form (PFF) games. In this paper we propose a method to compute payoffs, reflecting the power structure, for a pipeline system with regulated TPA. The method is based on an iterative flow mechanism to determine gas flows and transport fees for individual players and uses the recursive core and the minimal claim function to convert the PPF game back into a CFF game, which can be solved by standard methods. We illustrate the approach with a simple stylized numerical example of the gas network in Central Eastern Europe with a focus on Ukraine's power index as a major transit country.||||@arxiv||||2019/01/31||||Modelling transfer profits as externalities in a cooperative...||||Existing cooperative game theoretic studies of bargaining power in gas pipeline systems are based on the so called characteristic function form (CFF). This approach is potentially misleading if...||||https://arxiv.org/abs/1901.11435v2||||econ||||
350||||None||||Obvious Manipulations in Cake-Cutting||||arXiv.org||||2019/10/14||||Obvious Manipulations in Cake-Cutting||||Ortega, Josue || Segal-Halevi, Erel||||https://arxiv.org/pdf/1908.02988||||1908.02988||||In cake-cutting, strategy-proofness is a very costly requirement in terms of fairness: for n=2 it implies a dictatorial allocation, whereas for n > 2 it requires that one agent receives no cake. We show that a weaker version of this property recently suggested by Troyan and Morril, called non-obvious manipulability, is compatible with the strong fairness property of proportionality, which guarantees that each agent receives 1/n of the cake. Both properties are satisfied by the leftmost leaves mechanism, an adaptation of the Dubins - Spanier moving knife procedure. Most other classical proportional mechanisms in literature are obviously manipulable, including the original moving knife mechanism. Non-obvious manipulability explains why leftmost leaves is manipulated less often in practice than other proportional mechanisms.||||@arxiv||||2019/08/08||||Obvious Manipulations in Cake-Cutting||||In cake-cutting, strategy-proofness is a very costly requirement in terms of fairness: for n=2 it implies a dictatorial allocation, whereas for n > 2 it requires that one agent receives no cake....||||https://arxiv.org/abs/1908.02988v2||||cs||||
351||||None||||An Inattention Model for Traveler Behavior with e-Coupons||||arXiv.org||||2018/12/28||||An Inattention Model for Traveler Behavior with e-Coupons||||Qiu, Han||||https://arxiv.org/pdf/1901.05070||||1901.05070||||In this study, we consider traveler coupon redemption behavior from the perspective of an urban mobility service. Assuming traveler behavior is in accordance with the principle of utility maximization, we first formulate a baseline dynamical model for traveler's expected future trip sequence under the framework of Markov decision processes and from which we derive approximations of the optimal coupon redemption policy. However, we find that this baseline model cannot explain perfectly observed coupon redemption behavior of traveler for a car-sharing service. To resolve this deviation from utility-maximizing behavior, we suggest a hypothesis that travelers may not be aware of all coupons available to them. Based on this hypothesis, we formulate an inattention model on unawareness, which is complementary to the existing models of inattention, and incorporate it into the baseline model. Estimation results show that the proposed model better explains the coupon redemption dataset than the baseline model. We also conduct a simulation experiment to quantify the negative impact of unawareness on coupons' promotional effects. These results can be used by mobility service operators to design effective coupon distribution schemes in practice.||||@arxiv||||2018/12/28||||An Inattention Model for Traveler Behavior with e-Coupons||||In this study, we consider traveler coupon redemption behavior from the perspective of an urban mobility service. Assuming traveler behavior is in accordance with the principle of utility...||||https://arxiv.org/abs/1901.05070v1||||econ||||
352||||None||||The Economic Complexity of US Metropolitan Areas||||arXiv.org||||2019/01/23||||The Economic Complexity of US Metropolitan Areas||||Fritz, Benedikt S. L. || Manduca, Robert A.||||https://arxiv.org/pdf/1901.08112||||1901.08112||||We calculate measures of economic complexity for US metropolitan areas for the years 2007-2015 based on industry employment data. We show that the concept of economic complexity translates well from the cross-country to the regional setting, and is able to incorporate local as well as traded industries. The largest cities and the Northeast of the US have the highest average complexity, while traded industries are more complex than local-serving ones on average, but with some exceptions. On average, regions with higher complexity have a higher income per capita, but those regions also were more affected by the financial crisis. Finally, economic complexity is a significant predictor of within-decreases in income per capita and population. Our findings highlight the importance of subnational regions, and particularly metropolitan areas, as units of economic geography.||||@arxiv||||2019/01/23||||The Economic Complexity of US Metropolitan Areas||||We calculate measures of economic complexity for US metropolitan areas for the years 2007-2015 based on industry employment data. We show that the concept of economic complexity translates well...||||https://arxiv.org/abs/1901.08112v1||||econ||||
353||||None||||Modeling and Prediction of Iran's Steel Consumption Based on Economic Activity Using Support Vector Machines||||arXiv.org||||2019/12/05||||Modeling and Prediction of Iran's Steel Consumption Based on Economic Activity Using Support Vector Machines||||Kamalzadeh, Hossein || Sobhan, Saeid Nassim || Boskabadi, Azam || Hatami, Mohsen || Gharehyakheh, Amin||||https://arxiv.org/pdf/1912.02373||||1912.02373||||The steel industry has great impacts on the economy and the environment of both developed and underdeveloped countries. The importance of this industry and these impacts have led many researchers to investigate the relationship between a country's steel consumption and its economic activity resulting in the so-called intensity of use model. This paper investigates the validity of the intensity of use model for the case of Iran's steel consumption and extends this hypothesis by using the indexes of economic activity to model the steel consumption. We use the proposed model to train support vector machines and predict the future values for Iran's steel consumption. The paper provides detailed correlation tests for the factors used in the model to check for their relationships with the steel consumption. The results indicate that Iran's steel consumption is strongly correlated with its economic activity following the same pattern as the economy has been in the last four decades.||||@arxiv||||2019/12/05||||Modeling and Prediction of Iran's Steel Consumption Based on...||||The steel industry has great impacts on the economy and the environment of both developed and underdeveloped countries. The importance of this industry and these impacts have led many researchers...||||https://arxiv.org/abs/1912.02373v1||||cs||||
354||||None||||Dynamic Programming with State-Dependent Discounting||||arXiv.org||||2019/09/02||||Dynamic Programming with State-Dependent Discounting||||Stachurski, John || Zhang, Junnan||||https://arxiv.org/pdf/1908.08800||||1908.08800||||This paper extends the core results of discrete time infinite horizon dynamic programming theory to the case of state-dependent discounting. The traditional constant-discount condition requires that the discount factor of the controller is strictly less than one. Here we replace the constant factor with a discount factor process and require, in essence, that the process is strictly less than one on average in the long run. We prove that, under this condition, the standard optimality results can be recovered, including Bellman's principle of optimality, convergence of value function iteration and convergence of policy function iteration. We also show that the condition cannot be weakened in many standard settings. The dynamic programming framework considered in the paper is general enough to contain features such as recursive preferences. Several applications are discussed.||||@arxiv||||2019/08/23||||Dynamic Programming with State-Dependent Discounting||||This paper extends the core results of discrete time infinite horizon dynamic programming theory to the case of state-dependent discounting. The traditional constant-discount condition requires...||||https://arxiv.org/abs/1908.08800v2||||econ||||
355||||None||||Equilibrium in Production Chains with Multiple Upstream Partners||||arXiv.org||||2019/08/22||||Equilibrium in Production Chains with Multiple Upstream Partners||||Yu, Meng || Zhang, Junnan||||https://arxiv.org/pdf/1908.08208||||1908.08208||||In this paper, we extend and improve the production chain model introduced by Kikuchi et al. (2018). Utilizing the theory of monotone concave operators, we prove the existence, uniqueness, and global stability of equilibrium price, hence improving their results on production networks with multiple upstream partners. We propose an algorithm for computing the equilibrium price function that is more than ten times faster than successive evaluations of the operator. The model is then generalized to a stochastic setting that offers richer implications for the distribution of firms in a production network.||||@arxiv||||2019/08/22||||Equilibrium in Production Chains with Multiple Upstream Partners||||In this paper, we extend and improve the production chain model introduced by Kikuchi et al. (2018). Utilizing the theory of monotone concave operators, we prove the existence, uniqueness, and...||||https://arxiv.org/abs/1908.08208v1||||econ||||
356||||None||||Almost Sure Uniqueness of a Global Minimum Without Convexity||||arXiv.org||||2019/02/19||||Almost Sure Uniqueness of a Global Minimum Without Convexity||||Cox, Gregory||||https://arxiv.org/pdf/1803.02415||||1803.02415||||This paper establishes the argmin of a random objective function to be unique almost surely. This paper first formulates a general result that proves almost sure uniqueness without convexity of the objective function. The general result is then applied to a variety of applications in statistics. Four applications are discussed, including uniqueness of M-estimators, both classical likelihood and penalized likelihood estimators, and two applications of the argmin theorem, threshold regression and weak identification.||||@arxiv||||2018/03/06||||Almost Sure Uniqueness of a Global Minimum Without Convexity||||This paper establishes the argmin of a random objective function to be unique almost surely. This paper first formulates a general result that proves almost sure uniqueness without convexity of...||||https://arxiv.org/abs/1803.02415v3||||econ||||
357||||None||||Heterogeneous Earnings Effects of the Job Corps by Gender Earnings: A Translated Quantile Approach||||arXiv.org||||2019/08/23||||Heterogeneous Earnings Effects of the Job Corps by Gender Earnings: A Translated Quantile Approach||||Strittmatter, Anthony||||https://arxiv.org/pdf/1908.08721||||1908.08721||||Several studies of the Job Corps tend to nd more positive earnings effects for males than for females. This effect heterogeneity favouring males contrasts with the results of the majority of other training programmes' evaluations. Applying the translated quantile approach of Bitler, Hoynes, and Domina (2014), I investigate a potential mechanism behind the surprising findings for the Job Corps. My results provide suggestive evidence that the effect of heterogeneity by gender operates through existing gender earnings inequality rather than Job Corps trainability differences.||||@arxiv||||2019/08/23||||Heterogeneous Earnings Effects of the Job Corps by Gender...||||Several studies of the Job Corps tend to nd more positive earnings effects for males than for females. This effect heterogeneity favouring males contrasts with the results of the majority of other...||||https://arxiv.org/abs/1908.08721v1||||econ||||
358||||None||||Non-Asymptotic Inference in a Class of Optimization Problems||||arXiv.org||||2019/07/03||||Non-Asymptotic Inference in a Class of Optimization Problems||||Horowitz, Joel || Lee, Sokbae||||https://arxiv.org/pdf/1905.06491||||1905.06491||||This paper describes a method for carrying out non-asymptotic inference on partially identified parameters that are solutions to a class of optimization problems. The optimization problems arise in applications in which grouped data are used for estimation of a model's structural parameters. The parameters are characterized by restrictions that involve the population means of observed random variables in addition to the structural parameters of interest. Inference consists of finding confidence intervals for the structural parameters. Our method is non-asymptotic in the sense that it provides a finite-sample bound on the difference between the true and nominal probabilities with which a confidence interval contains the true but unknown value of a parameter. We contrast our method with an alternative non-asymptotic method based on the median-of-means estimator of Minsker (2015). The results of Monte Carlo experiments and an empirical example illustrate the usefulness of our method.||||@arxiv||||2019/05/16||||Non-Asymptotic Inference in a Class of Optimization Problems||||This paper describes a method for carrying out non-asymptotic inference on partially identified parameters that are solutions to a class of optimization problems. The optimization problems arise...||||https://arxiv.org/abs/1905.06491v2||||econ||||
359||||None||||A Regularized Factor-augmented Vector Autoregressive Model||||arXiv.org||||2019/12/12||||A Regularized Factor-augmented Vector Autoregressive Model||||Daniele, Maurizio || Schnaitmann, Julie||||https://arxiv.org/pdf/1912.06049||||1912.06049||||We propose a regularized factor-augmented vector autoregressive (FAVAR) model that allows for sparsity in the factor loadings. In this framework, factors may only load on a subset of variables which simplifies the factor identification and their economic interpretation. We identify the factors in a data-driven manner without imposing specific relations between the unobserved factors and the underlying time series. Using our approach, the effects of structural shocks can be investigated on economically meaningful factors and on all observed time series included in the FAVAR model. We prove consistency for the estimators of the factor loadings, the covariance matrix of the idiosyncratic component, the factors, as well as the autoregressive parameters in the dynamic model. In an empirical application, we investigate the effects of a monetary policy shock on a broad range of economically relevant variables. We identify this shock using a joint identification of the factor model and the structural innovations in the VAR model. We find impulse response functions which are in line with economic rationale, both on the factor aggregates and observed time series level.||||@arxiv||||2019/12/12||||A Regularized Factor-augmented Vector Autoregressive Model||||We propose a regularized factor-augmented vector autoregressive (FAVAR) model that allows for sparsity in the factor loadings. In this framework, factors may only load on a subset of variables...||||https://arxiv.org/abs/1912.06049v1||||econ||||
360||||None||||Discrete Time Dynamic Programming with Recursive Preferences: Optimality and Applications||||arXiv.org||||2019/01/15||||Discrete Time Dynamic Programming with Recursive Preferences: Optimality and Applications||||Ren, Guanlong || Stachurski, John||||https://arxiv.org/pdf/1812.05748||||1812.05748||||This paper provides an alternative approach to the theory of dynamic programming, designed to accommodate the kinds of recursive preference specifications that have become popular in economic and financial analysis, while still supporting traditional additively separable rewards. The approach exploits the theory of monotone convex operators, which turns out to be well suited to dynamic maximization. The intuition is that convexity is preserved under maximization, so convexity properties found in preferences extend naturally to the Bellman operator.||||@arxiv||||2018/12/14||||Discrete Time Dynamic Programming with Recursive Preferences:...||||This paper provides an alternative approach to the theory of dynamic programming, designed to accommodate the kinds of recursive preference specifications that have become popular in economic and...||||https://arxiv.org/abs/1812.05748v3||||econ||||
361||||None||||Accounting for Unobservable Heterogeneity in Cross Section Using Spatial First Differences||||arXiv.org||||2019/08/21||||Accounting for Unobservable Heterogeneity in Cross Section Using Spatial First Differences||||Druckenmiller, Hannah || Hsiang, Solomon||||https://arxiv.org/pdf/1810.07216||||1810.07216||||We develop a cross-sectional research design to identify causal effects in the presence of unobservable heterogeneity without instruments. When units are dense in physical space, it may be sufficient to regress the "spatial first differences" (SFD) of the outcome on the treatment and omit all covariates. The identifying assumptions of SFD are similar in mathematical structure and plausibility to other quasi-experimental designs. We use SFD to obtain new estimates for the effects of time-invariant geographic factors, soil and climate, on long-run agricultural productivities --- relationships crucial for economic decisions, such as land management and climate policy, but notoriously confounded by unobservables.||||@arxiv||||2018/10/16||||Accounting for Unobservable Heterogeneity in Cross Section Using...||||We develop a cross-sectional research design to identify causal effects in the presence of unobservable heterogeneity without instruments. When units are dense in physical space, it may be...||||https://arxiv.org/abs/1810.07216v2||||econ||||
362||||None||||A Bootstrap Test for the Existence of Moments for GARCH Processes||||arXiv.org||||2019/07/10||||A Bootstrap Test for the Existence of Moments for GARCH Processes||||Heinemann, Alexander||||https://arxiv.org/pdf/1902.01808||||1902.01808||||This paper studies the joint inference on conditional volatility parameters and the innovation moments by means of bootstrap to test for the existence of moments for GARCH(p,q) processes. We propose a residual bootstrap to mimic the joint distribution of the quasi-maximum likelihood estimators and the empirical moments of the residuals and also prove its validity. A bootstrap-based test for the existence of moments is proposed, which provides asymptotically correctly-sized tests without losing its consistency property. It is simple to implement and extends to other GARCH-type settings. A simulation study demonstrates the test's size and power properties in finite samples and an empirical application illustrates the testing approach.||||@arxiv||||2019/02/05||||A Bootstrap Test for the Existence of Moments for GARCH Processes||||This paper studies the joint inference on conditional volatility parameters and the innovation moments by means of bootstrap to test for the existence of moments for GARCH(p,q) processes. We...||||https://arxiv.org/abs/1902.01808v3||||econ||||
363||||None||||The Core of an Economy with an Endogenous Social Division of Labour||||arXiv.org||||2018/09/05||||The Core of an Economy with an Endogenous Social Division of Labour||||Gilles, Robert P.||||https://arxiv.org/pdf/1809.01470||||1809.01470||||This paper considers the core of a competitive market economy with an endogenous social division of labour. The theory is founded on the notion of a "consumer-producer", who consumes as well as produces commodities. First, we show that the Core of such an economy with an endogenous social division of labour can be founded on deviations of coalitions of arbitrary size, extending the seminal insights of Vind and Schmeidler for pure exchange economies. Furthermore, we establish the equivalence between the Core and the set of competitive equilibria for continuum economies with an endogenous social division of labour. Our analysis also concludes that self-organisation in a social division of labour can be incorporated into the Edgeworthian barter process directly. This is formulated as a Core equivalence result stated for a Structured Core concept based on renegotiations among fully specialised economic agents, i.e., coalitions that use only fully developed internal divisions of labour. Our approach bridges the gap between standard economies with social production and coalition production economies. Therefore, a more straightforward and natural interpretation of coalitional improvement and the Core can be developed than for coalition production economies.||||@arxiv||||2018/09/05||||The Core of an Economy with an Endogenous Social Division of Labour||||This paper considers the core of a competitive market economy with an endogenous social division of labour. The theory is founded on the notion of a "consumer-producer", who consumes as well as...||||https://arxiv.org/abs/1809.01470v1||||econ||||
364||||None||||Lee-Carter method for forecasting mortality for Peruvian Population||||arXiv.org||||2018/11/23||||Lee-Carter method for forecasting mortality for Peruvian Population||||Cerda-Hernández, J. || Sikov, A.||||https://arxiv.org/pdf/1811.09622||||1811.09622||||In this article, we have modeled mortality rates of Peruvian female and male populations during the period of 1950-2017 using the Lee-Carter (LC) model. The stochastic mortality model was introduced by Lee and Carter (1992) and has been used by many authors for fitting and forecasting the human mortality rates. The Singular Value Decomposition (SVD) approach is used for estimation of the parameters of the LC model. Utilizing the best fitted auto regressive integrated moving average (ARIMA) model we forecast the values of the time dependent parameter of the LC model for the next thirty years. The forecasted values of life expectancy at different age group with $95\%$ confidence intervals are also reported for the next thirty years. In this research we use the data, obtained from the Peruvian National Institute of Statistics (INEI).||||@arxiv||||2018/11/23||||Lee-Carter method for forecasting mortality for Peruvian Population||||In this article, we have modeled mortality rates of Peruvian female and male populations during the period of 1950-2017 using the Lee-Carter (LC) model. The stochastic mortality model was...||||https://arxiv.org/abs/1811.09622v1||||econ||||
365||||None||||In search of a new economic model determined by logistic growth||||arXiv.org||||2018/10/22||||In search of a new economic model determined by logistic growth||||Smirnov, Roman G. || Wang, Kunpeng||||https://arxiv.org/pdf/1711.02625||||1711.02625||||In this paper we extend the work by Ryuzo Sato devoted to the development of economic growth models within the framework of the Lie group theory. We propose a new growth model based on the assumption of logistic growth in factors. It is employed to derive new production functions and introduce a new notion of wage share. In the process it is shown that the new functions compare reasonably well against relevant economic data. The corresponding problem of maximization of profit under conditions of perfect competition is solved with the aid of one of these functions. In addition, it is explained in reasonably rigorous mathematical terms why Bowley's law no longer holds true in post-1960 data.||||@arxiv||||2017/11/07||||In search of a new economic model determined by logistic growth||||In this paper we extend the work by Ryuzo Sato devoted to the development of economic growth models within the framework of the Lie group theory. We propose a new growth model based on the...||||https://arxiv.org/abs/1711.02625v5||||econ||||
366||||None||||Forecasting the US GDP Components in the short run||||arXiv.org||||2019/06/19||||Forecasting the US GDP Components in the short run||||Jokubaitis, Saulius || Celov, Dmitrij||||https://arxiv.org/pdf/1906.07992||||1906.07992||||The aim of this paper is to estimate short-term forecasts of the US GDP components by expenditure approach sooner than they are officially released by the national institutions of statistics. For this reason, nowcasts along with 1- and 2-quarter forecasts are estimated by using available monthly information, officially released with a considerably smaller delay. The high-dimensionality problem of the monthly dataset used is solved by assuming sparse structures for the choice of leading indicators, capable of adequately explaining the dynamics of the GDP components. Variable selection and the estimation of the forecasts is performed by using the LASSO method, together with some of its popular modifications. Additionally, a modification of the LASSO is proposed, combining the methods of LASSO and principal components, in order to further improve the forecasting performance. Forecast accuracy of the models is evaluated by conducting pseudo-real-time forecasting exercises for four components of the GDP over the sample of 2005-2015, and compared with the benchmark ARMA models. The main results suggest that LASSO is able to outperform ARMA models when forecasting the GDP components and to identify leading explanatory variables. The proposed modification of the LASSO in some cases show further improvement in forecast accuracy.||||@arxiv||||2019/06/19||||Forecasting the US GDP Components in the short run||||The aim of this paper is to estimate short-term forecasts of the US GDP components by expenditure approach sooner than they are officially released by the national institutions of statistics. For...||||https://arxiv.org/abs/1906.07992v1||||econ||||
367||||None||||Bounds On Treatment Effects On Transitions||||arXiv.org||||2017/09/26||||Bounds On Treatment Effects On Transitions||||Vikström, Johan || Ridder, Geert || Weidner, Martin||||https://arxiv.org/pdf/1709.08981||||1709.08981||||This paper considers the identification of treatment effects on conditional transition probabilities. We show that even under random assignment only the instantaneous average treatment effect is point identified. Since treated and control units drop out at different rates, randomization only ensures the comparability of treatment and controls at the time of randomization, so that long-run average treatment effects are not point identified. Instead we derive informative bounds on these average treatment effects. Our bounds do not impose (semi)parametric restrictions, for example, proportional hazards. We also explore various assumptions such as monotone treatment response, common shocks and positively correlated outcomes that tighten the bounds.||||@arxiv||||2017/09/26||||Bounds On Treatment Effects On Transitions||||This paper considers the identification of treatment effects on conditional transition probabilities. We show that even under random assignment only the instantaneous average treatment effect is...||||https://arxiv.org/abs/1709.08981v1||||econ||||
369||||None||||Economically rational sample-size choice and irreproducibility||||arXiv.org||||2019/10/02||||Economically rational sample-size choice and irreproducibility||||Braganza, Oliver||||https://arxiv.org/pdf/1908.08702||||1908.08702||||Several systematic studies have suggested that a large fraction of published research is not reproducible. One probable reason for low reproducibility is insufficient sample size, resulting in low power and low positive predictive value. It has been suggested that insufficient sample-size choice is driven by a combination of scientific competition and 'positive publication bias'. Here we formalize this intuition in a simple model, in which scientists choose economically rational sample sizes, balancing the cost of experimentation with income from publication. Specifically, assuming that a scientist's income derives only from 'positive' findings (positive publication bias) and that individual samples cost a fixed amount, allows to leverage basic statistical formulas into an economic optimality prediction. We find that if effects have i) low base probability, ii) small effect size or iii) low grant income per publication, then the rational (economically optimal) sample size is small. Furthermore, for plausible distributions of these parameters we find a robust emergence of a bimodal distribution of obtained statistical power and low overall reproducibility rates, matching empirical findings. Overall, the model describes a simple mechanism explaining both the prevalence and the persistence of small sample sizes. It suggests economic rationality, or economic pressures, as a principal driver of irreproducibility.||||@arxiv||||2019/08/23||||Economically rational sample-size choice and irreproducibility||||Several systematic studies have suggested that a large fraction of published research is not reproducible. One probable reason for low reproducibility is insufficient sample size, resulting in low...||||https://arxiv.org/abs/1908.08702v3||||cs||||
370||||None||||Estimation of the size of informal employment based on administrative records with non-ignorable selection mechanism||||arXiv.org||||2019/06/26||||Estimation of the size of informal employment based on administrative records with non-ignorable selection mechanism||||Beręsewicz, Maciej || Nikulin, Dagmara||||https://arxiv.org/pdf/1906.10957||||1906.10957||||In this study we used company level administrative data from the National Labour Inspectorate and The Polish Social Insurance Institution in order to estimate the prevalence of informal employment in Poland. Since the selection mechanism is non-ignorable we employed a generalization of Heckman's sample selection model assuming non-Gaussian correlation of errors and clustering by incorporation of random effects. We found that 5.7% (4.6%, 7.1%; 95% CI) of registered enterprises in Poland, to some extent, take advantage of the informal labour force. Our study exemplifies a new approach to measuring informal employment, which can be implemented in other countries. It also contributes to the existing literature by providing, to the best of our knowledge, the first estimates of informal employment at the level of companies based solely on administrative data.||||@arxiv||||2019/06/26||||Estimation of the size of informal employment based on...||||In this study we used company level administrative data from the National Labour Inspectorate and The Polish Social Insurance Institution in order to estimate the prevalence of informal employment...||||https://arxiv.org/abs/1906.10957v1||||econ||||
371||||None||||Influence of High-Speed Railway System on Inter-city Travel Behavior in Vietnam||||arXiv.org||||2018/12/11||||Influence of High-Speed Railway System on Inter-city Travel Behavior in Vietnam||||Le, Tho V. || Zhang, Junyi || Chikaraishi, Makoto || Fujiwara, Akimasa||||https://arxiv.org/pdf/1812.04184||||1812.04184||||To analyze the influence of introducing the High-Speed Railway (HSR) system on business and non-business travel behavior, this study develops an integrated inter-city travel demand model to represent trip generations, destination choice, and travel mode choice behavior. The accessibility calculated from the RP/SP (Revealed Preference/Stated Preference) combined nested logit model of destination and mode choices is used as an explanatory variable in the trip frequency models. One of the important findings is that additional travel would be induced by introducing HSR. Our simulation analyses also reveal that HSR and conventional airlines will be the main modes for middle distances and long distances, respectively. The development of zones may highly influence the destination choices for business purposes, while prices of HSR and Low-Cost Carriers affect choices for non-business purposes. Finally, the research reveals that people on non-business trips are more sensitive to changes in travel time, travel cost and regional attributes than people on business trips.||||@arxiv||||2018/12/11||||Influence of High-Speed Railway System on Inter-city Travel...||||To analyze the influence of introducing the High-Speed Railway (HSR) system on business and non-business travel behavior, this study develops an integrated inter-city travel demand model to...||||https://arxiv.org/abs/1812.04184v1||||econ||||
372||||None||||Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous Treatment Effects||||arXiv.org||||2018/04/16||||Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous Treatment Effects||||Abraham, Sarah || Sun, Liyang||||https://arxiv.org/pdf/1804.05785||||1804.05785||||Event studies are frequently used to estimate average treatment effects on the treated (ATT). In estimating the ATT, researchers commonly use fixed effects models that implicitly assume constant treatment effects across cohorts. We show that this is not an innocuous assumption. In fixed effect models where the sole regressor is treatment status, the OLS coefficient is a non-convex average of the heterogeneous cohort-specific ATTs. When regressors containing lags and leads of treatment are added, the OLS coefficient corresponding to a given lead or lag picks up spurious terms consisting of treatment effects from other periods. Therefore, estimates from these commonly used models are not causally interpretable. We propose alternative estimators that identify certain convex averages of the cohort-specific ATTs, hence allowing for causal interpretation even under heterogeneous treatment effects. To illustrate the empirical content of our results, we show that the fixed effects estimators and our proposed estimators differ substantially in an application to the economic consequences of hospitalization.||||@arxiv||||2018/04/16||||Estimating Dynamic Treatment Effects in Event Studies with...||||Event studies are frequently used to estimate average treatment effects on the treated (ATT). In estimating the ATT, researchers commonly use fixed effects models that implicitly assume constant...||||https://arxiv.org/abs/1804.05785v1||||econ||||
373||||None||||A Dynamic Analysis of Nash Equilibria in Search Models with Fiat Money||||arXiv.org||||2018/05/12||||A Dynamic Analysis of Nash Equilibria in Search Models with Fiat Money||||Bonetto, Federico || Iacopetta, Maurizio||||https://arxiv.org/pdf/1805.04733||||1805.04733||||We study the rise in the acceptability fiat money in a Kiyotaki-Wright economy by developing a method that can determine dynamic Nash equilibria for a class of search models with genuine heterogenous agents. We also address open issues regarding the stability properties of pure strategies equilibria and the presence of multiple equilibria. Experiments illustrate the liquidity conditions that favor the transition from partial to full acceptance of fiat money, and the effects of inflationary shocks on production, liquidity, and trade.||||@arxiv||||2018/05/12||||A Dynamic Analysis of Nash Equilibria in Search Models with Fiat Money||||We study the rise in the acceptability fiat money in a Kiyotaki-Wright economy by developing a method that can determine dynamic Nash equilibria for a class of search models with genuine...||||https://arxiv.org/abs/1805.04733v1||||econ||||
374||||None||||Sharp Bounds for the Marginal Treatment Effect with Sample Selection||||arXiv.org||||2019/04/17||||Sharp Bounds for the Marginal Treatment Effect with Sample Selection||||Possebom, Vitor||||https://arxiv.org/pdf/1904.08522||||1904.08522||||I analyze treatment effects in situations when agents endogenously select into the treatment group and into the observed sample. As a theoretical contribution, I propose pointwise sharp bounds for the marginal treatment effect (MTE) of interest within the always-observed subpopulation under monotonicity assumptions. Moreover, I impose an extra mean dominance assumption to tighten the previous bounds. I further discuss how to identify those bounds when the support of the propensity score is either continuous or discrete. Using these results, I estimate bounds for the MTE of the Job Corps Training Program on hourly wages for the always-employed subpopulation and find that it is decreasing in the likelihood of attending the program within the Non-Hispanic group. For example, the Average Treatment Effect on the Treated is between \$.33 and \$.99 while the Average Treatment Effect on the Untreated is between \$.71 and \$3.00.||||@arxiv||||2019/04/17||||Sharp Bounds for the Marginal Treatment Effect with Sample Selection||||I analyze treatment effects in situations when agents endogenously select into the treatment group and into the observed sample. As a theoretical contribution, I propose pointwise sharp bounds for...||||https://arxiv.org/abs/1904.08522v1||||econ||||
375||||None||||Robust Productivity Analysis: An application to German FADN data||||arXiv.org||||2019/02/13||||Robust Productivity Analysis: An application to German FADN data||||Kloss, Mathias || Kirschstein, Thomas || Liebscher, Steffen || Petrick, Martin||||https://arxiv.org/pdf/1902.00678||||1902.00678||||Sources of bias in empirical studies can be separated in those coming from the modelling domain (e.g. multicollinearity) and those coming from outliers. We propose a two-step approach to counter both issues. First, by decontaminating data with a multivariate outlier detection procedure and second, by consistently estimating parameters of the production function. We apply this approach to a panel of German field crop data. Results show that the decontamination procedure detects multivariate outliers. In general, multivariate outlier control delivers more reasonable results with a higher precision in the estimation of some parameters and seems to mitigate the effects of multicollinearity.||||@arxiv||||2019/02/02||||Robust Productivity Analysis: An application to German FADN data||||Sources of bias in empirical studies can be separated in those coming from the modelling domain (e.g. multicollinearity) and those coming from outliers. We propose a two-step approach to counter...||||https://arxiv.org/abs/1902.00678v2||||econ||||
376||||None||||Deep Learning for Predicting Asset Returns||||arXiv.org||||2018/04/26||||Deep Learning for Predicting Asset Returns||||Feng, Guanhao || He, Jingyu || Polson, Nicholas G.||||https://arxiv.org/pdf/1804.09314||||1804.09314||||Deep learning searches for nonlinear factors for predicting asset returns. Predictability is achieved via multiple layers of composite factors as opposed to additive ones. Viewed in this way, asset pricing studies can be revisited using multi-layer deep learners, such as rectified linear units (ReLU) or long-short-term-memory (LSTM) for time-series effects. State-of-the-art algorithms including stochastic gradient descent (SGD), TensorFlow and dropout design provide imple- mentation and efficient factor exploration. To illustrate our methodology, we revisit the equity market risk premium dataset of Welch and Goyal (2008). We find the existence of nonlinear factors which explain predictability of returns, in particular at the extremes of the characteristic space. Finally, we conclude with directions for future research.||||@arxiv||||2018/04/25||||Deep Learning for Predicting Asset Returns||||Deep learning searches for nonlinear factors for predicting asset returns. Predictability is achieved via multiple layers of composite factors as opposed to additive ones. Viewed in this way,...||||https://arxiv.org/abs/1804.09314v2||||cs||||
377||||None||||Inference for First-Price Auctions with Guerre, Perrigne, and Vuong's Estimator||||arXiv.org||||2019/03/15||||Inference for First-Price Auctions with Guerre, Perrigne, and Vuong's Estimator||||Ma, Jun || Marmer, Vadim || Shneyerov, Artyom||||https://arxiv.org/pdf/1903.06401||||1903.06401||||We consider inference on the probability density of valuations in the first-price sealed-bid auctions model within the independent private value paradigm. We show the asymptotic normality of the two-step nonparametric estimator of Guerre, Perrigne, and Vuong (2000) (GPV), and propose an easily implementable and consistent estimator of the asymptotic variance. We prove the validity of the pointwise percentile bootstrap confidence intervals based on the GPV estimator. Lastly, we use the intermediate Gaussian approximation approach to construct bootstrap-based asymptotically valid uniform confidence bands for the density of the valuations.||||@arxiv||||2019/03/15||||Inference for First-Price Auctions with Guerre, Perrigne, and...||||We consider inference on the probability density of valuations in the first-price sealed-bid auctions model within the independent private value paradigm. We show the asymptotic normality of the...||||https://arxiv.org/abs/1903.06401v1||||econ||||
378||||None||||Predicting Patent Citations to measure Economic Impact of Scholarly Research||||arXiv.org||||2019/06/07||||Predicting Patent Citations to measure Economic Impact of Scholarly Research||||Shaikh, Abdul Rahman || Alhoori, Hamed||||https://arxiv.org/pdf/1906.08244||||1906.08244||||A crucial goal of funding research and development has always been to advance economic development. On this basis, a consider-able body of research undertaken with the purpose of determining what exactly constitutes economic impact and how to accurately measure that impact has been published. Numerous indicators have been used to measure economic impact, although no single indicator has been widely adapted. Based on patent data collected from Altmetric we predict patent citations through various social media features using several classification models. Patents citing a research paper implies the potential it has for direct application inits field. These predictions can be utilized by researchers in deter-mining the practical applications for their work when applying for patents.||||@arxiv||||2019/06/07||||Predicting Patent Citations to measure Economic Impact of...||||A crucial goal of funding research and development has always been to advance economic development. On this basis, a consider-able body of research undertaken with the purpose of determining what...||||https://arxiv.org/abs/1906.08244v1||||cs||||
379||||None||||Strategic Formation and Reliability of Supply Chain Networks||||arXiv.org||||2020/01/14||||Strategic Formation and Reliability of Supply Chain Networks||||Amelkin, Victor || Vohra, Rakesh||||https://arxiv.org/pdf/1909.08021||||1909.08021||||Supply chains are the backbone of the global economy. Disruptions to them can be costly. Centrally managed supply chains invest in ensuring their resilience. Decentralized supply chains, however, must rely upon the self-interest of their individual components to maintain the resilience of the entire chain.   We examine the incentives that independent self-interested agents have in forming a resilient supply chain network in the face of production disruptions and competition. In our model, competing suppliers are subject to yield uncertainty (they deliver less than ordered) and congestion (lead time uncertainty or, "soft" supply caps). Competing retailers must decide which suppliers to link to based on both price and reliability. In the presence of yield uncertainty only, the resulting supply chain networks are sparse. Retailers concentrate their links on a single supplier, counter to the idea that they should mitigate yield uncertainty by diversifying their supply base. This happens because retailers benefit from supply variance. It suggests that competition will amplify output uncertainty. When congestion is included as well, the resulting networks are denser and resemble the bipartite expander graphs that have been proposed in the supply chain literature, thereby, providing the first example of endogenous formation of resilient supply chain networks, without resilience being explicitly encoded in payoffs. Finally, we show that a supplier's investments in improved yield can make it worse off. This happens because high production output saturates the market, which, in turn lowers prices and profits for participants.||||@arxiv||||2019/09/17||||Strategic Formation and Reliability of Supply Chain Networks||||Supply chains are the backbone of the global economy. Disruptions to them can be costly. Centrally managed supply chains invest in ensuring their resilience. Decentralized supply chains, however,...||||https://arxiv.org/abs/1909.08021v2||||cs||||
380||||None||||Estimating the Welfare Effects of School Vouchers||||arXiv.org||||2020/01/31||||Estimating the Welfare Effects of School Vouchers||||Kamat, Vishal || Norris, Samuel||||https://arxiv.org/pdf/2002.00103||||2002.00103||||We analyze the welfare effects of voucher provision in the DC Opportunity Scholarship Program (OSP), a school voucher program in Washington, DC, that randomly allocated vouchers to students. To do so, we develop new discrete choice tools to show how to use data with random allocation of school vouchers to characterize what we can learn about the welfare benefits of providing a voucher of a given amount, as measured by the average willingness to pay for that voucher, and these benefits net of the costs of providing that voucher. A novel feature of our tools is that they allow specifying the relationship of the demand for the various schools with respect to prices to be entirely nonparametric or to be parameterized in a flexible manner, both of which do not necessarily imply that the welfare parameters are point identified. Applying our tools to the OSP data, we find that provision of the status-quo as well as a wide range of counterfactual voucher amounts has a positive net average benefit. We find these positive results arise due to the presence of many low-tuition schools in the program, removing these schools from the program can result in a negative net average benefit.||||@arxiv||||2020/01/31||||Estimating the Welfare Effects of School Vouchers||||We analyze the welfare effects of voucher provision in the DC Opportunity Scholarship Program (OSP), a school voucher program in Washington, DC, that randomly allocated vouchers to students. To do...||||https://arxiv.org/abs/2002.00103v1||||econ||||
381||||None||||Optimizing Execution Cost Using Stochastic Control||||arXiv.org||||2019/09/24||||Optimizing Execution Cost Using Stochastic Control||||Bansal, Akshay || Mukherjee, Diganta||||https://arxiv.org/pdf/1909.10762||||1909.10762||||We devise an optimal allocation strategy for the execution of a predefined number of stocks in a given time frame using the technique of discrete-time Stochastic Control Theory for a defined market model. This market structure allows an instant execution of the market orders and has been analyzed based on the assumption of discretized geometric movement of the stock prices. We consider two different cost functions where the first function involves just the fiscal cost while the cost function of the second kind incorporates the risks of non-strategic constrained investments along with fiscal costs. Precisely, the strategic development of constrained execution of K stocks within a stipulated time frame of T units is established mathematically using a well-defined stochastic behaviour of stock prices and the same is compared with some of the commonly-used execution strategies using the historical stock price data.||||@arxiv||||2019/09/24||||Optimizing Execution Cost Using Stochastic Control||||We devise an optimal allocation strategy for the execution of a predefined number of stocks in a given time frame using the technique of discrete-time Stochastic Control Theory for a defined...||||https://arxiv.org/abs/1909.10762v1||||econ||||
382||||None||||Sparse Bayesian time-varying covariance estimation in many dimensions||||arXiv.org||||2017/11/11||||Sparse Bayesian time-varying covariance estimation in many dimensions||||Kastner, Gregor||||https://arxiv.org/pdf/1608.08468||||1608.08468||||We address the curse of dimensionality in dynamic covariance estimation by modeling the underlying co-volatility dynamics of a time series vector through latent time-varying stochastic factors. The use of a global-local shrinkage prior for the elements of the factor loadings matrix pulls loadings on superfluous factors towards zero. To demonstrate the merits of the proposed framework, the model is applied to simulated data as well as to daily log-returns of 300 S&P 500 members. Our approach yields precise correlation estimates, strong implied minimum variance portfolio performance and superior forecasting accuracy in terms of log predictive scores when compared to typical benchmarks.||||@arxiv||||2016/08/30||||Sparse Bayesian time-varying covariance estimation in many dimensions||||We address the curse of dimensionality in dynamic covariance estimation by modeling the underlying co-volatility dynamics of a time series vector through latent time-varying stochastic factors....||||https://arxiv.org/abs/1608.08468v3||||econ||||
383||||None||||Semi-parametric dynamic contextual pricing||||arXiv.org||||2019/08/11||||Semi-parametric dynamic contextual pricing||||Shah, Virag || Blanchet, Jose || Johari, Ramesh||||https://arxiv.org/pdf/1901.02045||||1901.02045||||Motivated by the application of real-time pricing in e-commerce platforms, we consider the problem of revenue-maximization in a setting where the seller can leverage contextual information describing the customer's history and the product's type to predict her valuation of the product. However, her true valuation is unobservable to the seller, only binary outcome in the form of success-failure of a transaction is observed. Unlike in usual contextual bandit settings, the optimal price/arm given a covariate in our setting is sensitive to the detailed characteristics of the residual uncertainty distribution. We develop a semi-parametric model in which the residual distribution is non-parametric and provide the first algorithm which learns both regression parameters and residual distribution with $\tilde O(\sqrt{n})$ regret. We empirically test a scalable implementation of our algorithm and observe good performance.||||@arxiv||||2019/01/07||||Semi-parametric dynamic contextual pricing||||Motivated by the application of real-time pricing in e-commerce platforms, we consider the problem of revenue-maximization in a setting where the seller can leverage contextual information...||||https://arxiv.org/abs/1901.02045v4||||cs||||
384||||None||||The Impact of Renewable Energy Forecasts on Intraday Electricity Prices||||arXiv.org||||2019/11/18||||The Impact of Renewable Energy Forecasts on Intraday Electricity Prices||||Kulakov, Sergei || Ziel, Florian||||https://arxiv.org/pdf/1903.09641||||1903.09641||||In this paper we study the impact of errors in wind and solar power forecasts on intraday electricity prices. We develop a novel econometric model which is based on day-ahead wholesale auction curves data and errors in wind and solar power forecasts. The model shifts day-ahead supply curves to calculate intraday prices. We apply our model to the German EPEX SPOT SE data. Our model outperforms both linear and non-linear benchmarks. Our study allows us to conclude that errors in renewable energy forecasts exert a non-linear impact on intraday prices. We demonstrate that additional wind and solar power capacities induce non-linear changes in the intraday price volatility. Finally, we comment on economical and policy implications of our findings.||||@arxiv||||2019/03/22||||The Impact of Renewable Energy Forecasts on Intraday Electricity Prices||||In this paper we study the impact of errors in wind and solar power forecasts on intraday electricity prices. We develop a novel econometric model which is based on day-ahead wholesale auction...||||https://arxiv.org/abs/1903.09641v2||||econ||||
385||||None||||Forecast Encompassing Tests for the Expected Shortfall||||arXiv.org||||2019/09/21||||Forecast Encompassing Tests for the Expected Shortfall||||Dimitriadis, Timo || Schnaitmann, Julie||||https://arxiv.org/pdf/1908.04569||||1908.04569||||We introduce new forecast encompassing tests for the risk measure Expected Shortfall (ES). The ES currently receives much attention through its introduction into the Basel III Accords, which stipulate its use as the primary market risk measure for the international banking regulation. We utilize joint loss functions for the pair ES and Value at Risk to set up three ES encompassing test variants. The tests are built on misspecification robust asymptotic theory and we verify the finite sample properties of the tests in an extensive simulation study. We use the encompassing tests to illustrate the potential of forecast combination methods for different financial assets.||||@arxiv||||2019/08/13||||Forecast Encompassing Tests for the Expected Shortfall||||We introduce new forecast encompassing tests for the risk measure Expected Shortfall (ES). The ES currently receives much attention through its introduction into the Basel III Accords, which...||||https://arxiv.org/abs/1908.04569v2||||econ||||
386||||None||||Inference for VARs Identified with Sign Restrictions||||arXiv.org||||2018/02/07||||Inference for VARs Identified with Sign Restrictions||||Granziera, Eleonora || Moon, Hyungsik Roger || Schorfheide, Frank||||https://arxiv.org/pdf/1709.10196||||1709.10196||||There is a fast growing literature that set-identifies structural vector autoregressions (SVARs) by imposing sign restrictions on the responses of a subset of the endogenous variables to a particular structural shock (sign-restricted SVARs). Most methods that have been used to construct pointwise coverage bands for impulse responses of sign-restricted SVARs are justified only from a Bayesian perspective. This paper demonstrates how to formulate the inference problem for sign-restricted SVARs within a moment-inequality framework. In particular, it develops methods of constructing confidence bands for impulse response functions of sign-restricted SVARs that are valid from a frequentist perspective. The paper also provides a comparison of frequentist and Bayesian coverage bands in the context of an empirical application - the former can be substantially wider than the latter.||||@arxiv||||2017/09/28||||Inference for VARs Identified with Sign Restrictions||||There is a fast growing literature that set-identifies structural vector autoregressions (SVARs) by imposing sign restrictions on the responses of a subset of the endogenous variables to a...||||https://arxiv.org/abs/1709.10196v2||||econ||||
387||||None||||The Impact of Age on Nationality Bias: Evidence from Ski Jumping||||arXiv.org||||2018/08/11||||The Impact of Age on Nationality Bias: Evidence from Ski Jumping||||Schneemann, Sandra || Scholten, Hendrik || Deutscher, Christian||||https://arxiv.org/pdf/1808.03804||||1808.03804||||This empirical research explores the impact of age on nationality bias. World Cup competition data suggest that judges of professional ski jumping competitions prefer jumpers of their own nationality and exhibit this preference by rewarding them with better marks. Furthermore, the current study reveals that this nationality bias is diminished among younger judges, in accordance with the reported lower levels of national discrimination among younger generations. Globalisation and its effect in reducing class-based thinking may explain this reduced bias in judgment of others.||||@arxiv||||2018/08/11||||The Impact of Age on Nationality Bias: Evidence from Ski Jumping||||This empirical research explores the impact of age on nationality bias. World Cup competition data suggest that judges of professional ski jumping competitions prefer jumpers of their own...||||https://arxiv.org/abs/1808.03804v1||||econ||||
388||||None||||Designing An Industrial Policy For Developing Countries: A New Approach||||arXiv.org||||2019/01/14||||Designing An Industrial Policy For Developing Countries: A New Approach||||Haeri, Ali || Arabmazar, Abbas||||https://arxiv.org/pdf/1901.04265||||1901.04265||||In this study, the prevalent methodology for design of the industrial policy in developing countries was critically assessed, and it was shown that the mechanism and content of classical method is fundamentally contradictory to the goals and components of the endogenous growth theories. This study, by proposing a new approach, along settling Schumpeter's economic growth theory as a policy framework, designed the process of entering, analyzing and processing data as the mechanism of the industrial policy in order to provide "theoretical consistency" and "technical and Statistical requirements" for targeting the growth stimulant factor effectively.||||@arxiv||||2019/01/14||||Designing An Industrial Policy For Developing Countries: A New Approach||||In this study, the prevalent methodology for design of the industrial policy in developing countries was critically assessed, and it was shown that the mechanism and content of classical method is...||||https://arxiv.org/abs/1901.04265v1||||econ||||
389||||None||||Isotonic regression discontinuity designs||||arXiv.org||||2019/12/11||||Isotonic regression discontinuity designs||||Babii, Andrii || Kumar, Rohit||||https://arxiv.org/pdf/1908.05752||||1908.05752||||In isotonic regression discontinuity designs, the average outcome and the treatment assignment probability are monotone in the running variable. We introduce novel nonparametric estimators for sharp and fuzzy designs based on the isotonic regression which is robust to the inference after the model selection problem. The large sample distributions of introduced estimators are driven by scaled Brownian motions originating from zero and moving in opposite directions. Since these distributions are not pivotal, we also introduce a novel trimmed wild bootstrap procedure, which does not require additional nonparametric smoothing, typically needed in such settings, and show its consistency. We illustrate our approach on the well-known dataset of Lee (2008), estimating the incumbency effect in the U.S. House elections.||||@arxiv||||2019/08/15||||Isotonic regression discontinuity designs||||In isotonic regression discontinuity designs, the average outcome and the treatment assignment probability are monotone in the running variable. We introduce novel nonparametric estimators for...||||https://arxiv.org/abs/1908.05752v4||||econ||||
390||||None||||Rate-Optimal Estimation of the Intercept in a Semiparametric Sample-Selection Model||||arXiv.org||||2018/09/25||||Rate-Optimal Estimation of the Intercept in a Semiparametric Sample-Selection Model||||Goh, Chuan||||https://arxiv.org/pdf/1710.01423||||1710.01423||||This paper presents a new estimator of the intercept of a linear regression model in cases where the outcome varaible is observed subject to a selection rule. The intercept is often in this context of inherent interest; for example, in a program evaluation context, the difference between the intercepts in outcome equations for participants and non-participants can be interpreted as the difference in average outcomes of participants and their counterfactual average outcomes if they had chosen not to participate. The new estimator can under mild conditions exhibit a rate of convergence in probability equal to $n^{-p/(2p+1)}$, where $p\ge 2$ is an integer that indexes the strength of certain smoothness assumptions. This rate of convergence is shown in this context to be the optimal rate of convergence for estimation of the intercept parameter in terms of a minimax criterion. The new estimator, unlike other proposals in the literature, is under mild conditions consistent and asymptotically normal with a rate of convergence that is the same regardless of the degree to which selection depends on unobservables in the outcome equation. Simulation evidence and an empirical example are included.||||@arxiv||||2017/10/04||||Rate-Optimal Estimation of the Intercept in a Semiparametric...||||This paper presents a new estimator of the intercept of a linear regression model in cases where the outcome varaible is observed subject to a selection rule. The intercept is often in this...||||https://arxiv.org/abs/1710.01423v3||||econ||||
392||||None||||Transmission of Macroeconomic Shocks to Risk Parameters: Their uses in Stress Testing||||arXiv.org||||2019/05/17||||Transmission of Macroeconomic Shocks to Risk Parameters: Their uses in Stress Testing||||Rojas, Helder || Dias, David||||https://arxiv.org/pdf/1809.07401||||1809.07401||||In this paper, we are interested in evaluating the resilience of financial portfolios under extreme economic conditions. Therefore, we use empirical measures to characterize the transmission process of macroeconomic shocks to risk parameters. We propose the use of an extensive family of models, called General Transfer Function Models, which condense well the characteristics of the transmission described by the impact measures. The procedure for estimating the parameters of these models is described employing the Bayesian approach and using the prior information provided by the impact measures. In addition, we illustrate the use of the estimated models from the credit risk data of a portfolio.||||@arxiv||||2018/09/19||||Transmission of Macroeconomic Shocks to Risk Parameters: Their...||||In this paper, we are interested in evaluating the resilience of financial portfolios under extreme economic conditions. Therefore, we use empirical measures to characterize the transmission...||||https://arxiv.org/abs/1809.07401v3||||econ||||
393||||None||||Addictive Auctions: using lucky-draw and gambling addiction to increase participation during auctioning||||arXiv.org||||2019/06/07||||Addictive Auctions: using lucky-draw and gambling addiction to increase participation during auctioning||||Kumar, Ravin||||https://arxiv.org/pdf/1906.03237||||1906.03237||||Auction theories are believed to provide a better selling opportunity for the resources to be allocated. Various organizations have taken measures to increase trust among participants towards their auction system, but trust alone cannot ensure a high level of participation. We propose a new type of auction system which takes advantage of lucky draw and gambling addictions to increase the engagement level of candidates in an auction. Our system makes use of security features present in existing auction systems for ensuring fairness and maintaining trust among participants.||||@arxiv||||2019/06/07||||Addictive Auctions: using lucky-draw and gambling addiction to...||||Auction theories are believed to provide a better selling opportunity for the resources to be allocated. Various organizations have taken measures to increase trust among participants towards...||||https://arxiv.org/abs/1906.03237v1||||econ||||
394||||None||||Large Dimensional Latent Factor Modeling with Missing Observations and Applications to Causal Inference||||arXiv.org||||2019/11/22||||Large Dimensional Latent Factor Modeling with Missing Observations and Applications to Causal Inference||||Xiong, Ruoxuan || Pelger, Markus||||https://arxiv.org/pdf/1910.08273||||1910.08273||||This paper develops the inferential theory for latent factor models estimated from large dimensional panel data with missing observations. We estimate a latent factor model by applying principal component analysis to an adjusted covariance matrix estimated from partially observed panel data. We derive the asymptotic distribution for the estimated factors, loadings and the imputed values under a general approximate factor model. The key application is to estimate counterfactual outcomes in causal inference from panel data. The unobserved control group is modeled as missing values, which are inferred from the latent factor model. The inferential theory for the imputed values allows us to test for individual treatment effects at any time. We apply our method to portfolio investment strategies and find that around 14% of their average returns are significantly reduced by the academic publication of these strategies.||||@arxiv||||2019/10/18||||Large Dimensional Latent Factor Modeling with Missing Observations...||||This paper develops the inferential theory for latent factor models estimated from large dimensional panel data with missing observations. We estimate a latent factor model by applying principal...||||https://arxiv.org/abs/1910.08273v3||||econ||||
395||||None||||A Consistent Variance Estimator for 2SLS When Instruments Identify Different LATEs||||arXiv.org||||2018/06/05||||A Consistent Variance Estimator for 2SLS When Instruments Identify Different LATEs||||Lee, Seojeong||||https://arxiv.org/pdf/1806.01457||||1806.01457||||Under treatment effect heterogeneity, an instrument identifies the instrument-specific local average treatment effect (LATE). With multiple instruments, two-stage least squares (2SLS) estimand is a weighted average of different LATEs. What is often overlooked in the literature is that the postulated moment condition evaluated at the 2SLS estimand does not hold unless those LATEs are the same. If so, the conventional heteroskedasticity-robust variance estimator would be inconsistent, and 2SLS standard errors based on such estimators would be incorrect. I derive the correct asymptotic distribution, and propose a consistent asymptotic variance estimator by using the result of Hall and Inoue (2003, Journal of Econometrics) on misspecified moment condition models. This can be used to correctly calculate the standard errors regardless of whether there is more than one LATE or not.||||@arxiv||||2018/06/05||||A Consistent Variance Estimator for 2SLS When Instruments Identify...||||Under treatment effect heterogeneity, an instrument identifies the instrument-specific local average treatment effect (LATE). With multiple instruments, two-stage least squares (2SLS) estimand is...||||https://arxiv.org/abs/1806.01457v1||||econ||||
396||||None||||The Optimal Deterrence of Crime: A Focus on the Time Preference of DWI Offenders||||arXiv.org||||2019/09/20||||The Optimal Deterrence of Crime: A Focus on the Time Preference of DWI Offenders||||Wang, Yuqing || Pei, Yan Ru||||https://arxiv.org/pdf/1909.06509||||1909.06509||||We develop a general model for finding the optimal penal strategy based on the behavioral traits of the offenders. We focus on how the discount rate (level of time discounting) affects criminal propensity on the individual level, and how the aggregation of these effects influences criminal activities on the population level. The effects are aggregated based on the distribution of discount rate among the population. We study this distribution empirically through a survey with 207 participants, and we show that it follows zero-inflated exponential distribution. We quantify the effectiveness of the penal strategy as its net utility for the population, and show how this quantity can be maximized. When we apply the maximization procedure on the offense of impaired driving (DWI), we discover that the effectiveness of DWI deterrence depends critically on the amount of fine and prison condition.||||@arxiv||||2019/09/14||||The Optimal Deterrence of Crime: A Focus on the Time Preference of...||||We develop a general model for finding the optimal penal strategy based on the behavioral traits of the offenders. We focus on how the discount rate (level of time discounting) affects criminal...||||https://arxiv.org/abs/1909.06509v2||||econ||||
397||||None||||A Nonparametric Approach to Measure the Heterogeneous Spatial Association: Under Spatial Temporal Data||||arXiv.org||||2018/03/22||||A Nonparametric Approach to Measure the Heterogeneous Spatial Association: Under Spatial Temporal Data||||Yuan, Zihao||||https://arxiv.org/pdf/1803.02334||||1803.02334||||Spatial association and heterogeneity are two critical areas in the research about spatial analysis, geography, statistics and so on. Though large amounts of outstanding methods has been proposed and studied, there are few of them tend to study spatial association under heterogeneous environment. Additionally, most of the traditional methods are based on distance statistic and spatial weighted matrix. However, in some abstract spatial situations, distance statistic can not be applied since we can not even observe the geographical locations directly. Meanwhile, under these circumstances, due to invisibility of spatial positions, designing of weight matrix can not absolutely avoid subjectivity. In this paper, a new entropy-based method, which is data-driven and distribution-free, has been proposed to help us investigate spatial association while fully taking the fact that heterogeneity widely exist. Specifically, this method is not bounded with distance statistic or weight matrix. Asymmetrical dependence is adopted to reflect the heterogeneity in spatial association for each individual and the whole discussion in this paper is performed on spatio-temporal data with only assuming stationary m-dependent over time.||||@arxiv||||2018/03/06||||A Nonparametric Approach to Measure the Heterogeneous Spatial...||||Spatial association and heterogeneity are two critical areas in the research about spatial analysis, geography, statistics and so on. Though large amounts of outstanding methods has been proposed...||||https://arxiv.org/abs/1803.02334v2||||econ||||
398||||None||||Online reviews can predict long-term returns of individual stocks||||arXiv.org||||2019/04/30||||Online reviews can predict long-term returns of individual stocks||||Wu, Junran || Xu, Ke || Zhao, Jichang||||https://arxiv.org/pdf/1905.03189||||1905.03189||||Online reviews are feedback voluntarily posted by consumers about their consumption experiences. This feedback indicates customer attitudes such as affection, awareness and faith towards a brand or a firm and demonstrates inherent connections with a company's future sales, cash flow and stock pricing. However, the predicting power of online reviews for long-term returns on stocks, especially at the individual level, has received little research attention, making a comprehensive exploration necessary to resolve existing debates. In this paper, which is based exclusively on online reviews, a methodology framework for predicting long-term returns of individual stocks with competent performance is established. Specifically, 6,246 features of 13 categories inferred from more than 18 million product reviews are selected to build the prediction models. With the best classifier selected from cross-validation tests, a satisfactory increase in accuracy, 13.94%, was achieved compared to the cutting-edge solution with 10 technical indicators being features, representing an 18.28% improvement relative to the random value. The robustness of our model is further evaluated and testified in realistic scenarios. It is thus confirmed for the first time that long-term returns of individual stocks can be predicted by online reviews. This study provides new opportunities for investors with respect to long-term investments in individual stocks.||||@arxiv||||2019/04/30||||Online reviews can predict long-term returns of individual stocks||||Online reviews are feedback voluntarily posted by consumers about their consumption experiences. This feedback indicates customer attitudes such as affection, awareness and faith towards a brand...||||https://arxiv.org/abs/1905.03189v1||||cs||||
399||||None||||Measuring the Input Rank in Global Supply Networks||||arXiv.org||||2020/01/22||||Measuring the Input Rank in Global Supply Networks||||Rungi, Armando || Fattorini, Loredana || Huremovic, Kenan||||https://arxiv.org/pdf/2001.08003||||2001.08003||||In this paper, we introduce the Input Rank as a measure to study the organization of global supply networks at the firm level. We model the case of a firm that needs assessing the technological relevance of each direct and indirect supplier on a network-like production function with labor and intermediate inputs. In our framework, an input is technologically more relevant if a shock on that upstream market can hit harder the marginal costs of a downstream buyer, considering the topology of the supply structure. A higher labor intensity at each stage buffers the transmission of upstream shocks in the network. In addition, we provide for the possibility that producers have limited knowledge of inputs in the supply network, hence they can underestimate the relevance of more distant inputs. After applications, the Input Rank returns a matrix of technological centralities that order any direct or indirect input for a representative firm in any output industry. We compute the Input Rank on U.S. and world input-output tables. Finally, we test how it correlates with choices of vertical integration made by 20,489 U.S. parent companies controlling 154,836 affiliates worldwide. We find that a higher Input Rank is positively associated with higher odds that that input is vertically integrated, relatively more when final demand is elastic. A supplier's Input Rank remains a significant predictor of a firm's decision to integrate even after controlling for the relative positions on upstreamness(downstreamness) segments.||||@arxiv||||2020/01/22||||Measuring the Input Rank in Global Supply Networks||||In this paper, we introduce the Input Rank as a measure to study the organization of global supply networks at the firm level. We model the case of a firm that needs assessing the technological...||||https://arxiv.org/abs/2001.08003v1||||econ||||
400||||None||||Does Better Governance Guarantee Less Corruption? Evidence of Loss in Effectiveness of the Rule of Law||||arXiv.org||||2019/02/01||||Does Better Governance Guarantee Less Corruption? Evidence of Loss in Effectiveness of the Rule of Law||||Guerrero, Omar A. || Castañeda, Gonzalo||||https://arxiv.org/pdf/1902.00428||||1902.00428||||Corruption is an endemic societal problem with profound implications in the development of nations. In combating this issue, cross-national evidence supporting the effectiveness of the rule of law seems at odds with poorly realized outcomes from reforms inspired in such literature. This paper provides an explanation for such contradiction. By taking a computational approach, we develop two methodological novelties into the empirical study of corruption: (1) generating large within-country variation by means of simulation (instead of cross-national data pooling), and (2) accounting for interactions between covariates through a spillover network. The latter (the network), seems responsible for a significant reduction in the effectiveness of the rule of law; especially among the least developed countries. We also find that effectiveness can be boosted by improving complementary policy issues that may lie beyond the governance agenda. Moreover, our simulations suggest that improvements to the rule of law are a necessary yet not sufficient condition to curve corruption.||||@arxiv||||2019/02/01||||Does Better Governance Guarantee Less Corruption? Evidence of Loss...||||Corruption is an endemic societal problem with profound implications in the development of nations. In combating this issue, cross-national evidence supporting the effectiveness of the rule of law...||||https://arxiv.org/abs/1902.00428v1||||econ||||
401||||None||||Dimensional Analysis in Economics: A Study of the Neoclassical Economic Growth Model||||arXiv.org||||2018/02/28||||Dimensional Analysis in Economics: A Study of the Neoclassical Economic Growth Model||||Texocotitla, Miguel Alvarez || Hernandez, M. David Alvarez || Hernandez, Shani Alvarez||||https://arxiv.org/pdf/1802.10528||||1802.10528||||The fundamental purpose of the present research article is to introduce the basic principles of Dimensional Analysis in the context of the neoclassical economic theory, in order to apply such principles to the fundamental relations that underlay most models of economic growth. In particular, basic instruments from Dimensional Analysis are used to evaluate the analytical consistency of the Neoclassical economic growth model. The analysis shows that an adjustment to the model is required in such a way that the principle of dimensional homogeneity is satisfied.||||@arxiv||||2018/02/28||||Dimensional Analysis in Economics: A Study of the Neoclassical...||||The fundamental purpose of the present research article is to introduce the basic principles of Dimensional Analysis in the context of the neoclassical economic theory, in order to apply such...||||https://arxiv.org/abs/1802.10528v1||||econ||||
402||||None||||Multiple Benefits through Smart Home Energy Management Solutions -- A Simulation-Based Case Study of a Single-Family House in Algeria and Germany||||arXiv.org||||2019/04/25||||Multiple Benefits through Smart Home Energy Management Solutions -- A Simulation-Based Case Study of a Single-Family House in Algeria and Germany||||Ringel, Marc || Laidi, Roufaida || Djenouri, Djamel||||https://arxiv.org/pdf/1904.11496||||1904.11496||||From both global and local perspectives, there are strong reasons to promote energy efficiency. These reasons have prompted leaders in the European Union (EU) and countries of the Middle East and North Africa (MENA) to adopt policies to move their citizenry toward more efficient energy consumption. Energy efficiency policy is typically framed at the national, or transnational level. Policy makers then aim to incentivize microeconomic actors to align their decisions with macroeconomic policy. We suggest another path towards greater energy efficiency: Highlighting individual benefits at microeconomic level. By simulating lighting, heating and cooling operations in a model single-family home equipped with modest automation, we show that individual actors can be led to pursue energy efficiency out of enlightened self-interest. We apply simple-to-use, easily, scalable impact indicators that can be made available to homeowners and serve as intrinsic economic, environmental and social motivators for pursuing energy efficiency. The indicators reveal tangible homeowner benefits realizable under both the market-based pricing structure for energy in Germany and the state-subsidized pricing structure in Algeria. Benefits accrue under both the continental climate regime of Germany and the Mediterranean regime of Algeria, notably in the case that cooling energy needs are considered. Our findings show that smart home technology provides an attractive path for advancing energy efficiency goals. The indicators we assemble can help policy makers both to promote tangible benefits of energy efficiency to individual homeowners, and to identify those investments of public funds that best support individual pursuit of national and transnational energy goals.||||@arxiv||||2019/04/25||||Multiple Benefits through Smart Home Energy Management Solutions...||||From both global and local perspectives, there are strong reasons to promote energy efficiency. These reasons have prompted leaders in the European Union (EU) and countries of the Middle East and...||||https://arxiv.org/abs/1904.11496v1||||cs||||
403||||None||||Talent management - an etymological study||||arXiv.org||||2018/10/05||||Talent management - an etymological study||||Dimitrov, Kiril||||https://arxiv.org/pdf/1810.02615||||1810.02615||||The current article unveils and analyzes important shades of meaning for the widely discussed term talent management. It not only grounds the outlined perspectives in incremental formulation and elaboration of this construct, but also is oriented to exploring the underlying reasons for the social actors, proposing new nuances. Thus, a mind map and a fish-bone diagram are constructed to depict effectively and efficiently the current state of development for talent management and make easier the realizations of future research endeavours in this field.||||@arxiv||||2018/10/05||||Talent management - an etymological study||||The current article unveils and analyzes important shades of meaning for the widely discussed term talent management. It not only grounds the outlined perspectives in incremental formulation and...||||https://arxiv.org/abs/1810.02615v1||||econ||||
404||||None||||Improved Inference on the Rank of a Matrix||||arXiv.org||||2019/03/25||||Improved Inference on the Rank of a Matrix||||Chen, Qihui || Fang, Zheng||||https://arxiv.org/pdf/1812.02337||||1812.02337||||This paper develops a general framework for conducting inference on the rank of an unknown matrix $Π_0$. A defining feature of our setup is the null hypothesis of the form $\mathrm H_0: \mathrm{rank}(Π_0)\le r$. The problem is of first order importance because the previous literature focuses on $\mathrm H_0': \mathrm{rank}(Π_0)= r$ by implicitly assuming away $\mathrm{rank}(Π_0)<r$, which may lead to invalid rank tests due to over-rejections. In particular, we show that limiting distributions of test statistics under $\mathrm H_0'$ may not stochastically dominate those under $\mathrm{rank}(Π_0)<r$. A multiple test on the nulls $\mathrm{rank}(Π_0)=0,\ldots,r$, though valid, may be substantially conservative. We employ a testing statistic whose limiting distributions under $\mathrm H_0$ are highly nonstandard due to the inherent irregular natures of the problem, and then construct bootstrap critical values that deliver size control and improved power. Since our procedure relies on a tuning parameter, a two-step procedure is designed to mitigate concerns on this nuisance. We additionally argue that our setup is also important for estimation. We illustrate the empirical relevance of our results through testing identification in linear IV models that allows for clustered data and inference on sorting dimensions in a two-sided matching model with transferrable utility.||||@arxiv||||2018/12/06||||Improved Inference on the Rank of a Matrix||||This paper develops a general framework for conducting inference on the rank of an unknown matrix $Π_0$. A defining feature of our setup is the null hypothesis of the form $\mathrm H_0:...||||https://arxiv.org/abs/1812.02337v2||||econ||||
405||||None||||lassopack: Model selection and prediction with regularized regression in Stata||||arXiv.org||||2019/01/16||||lassopack: Model selection and prediction with regularized regression in Stata||||Ahrens, Achim || Hansen, Christian B. || Schaffer, Mark E.||||https://arxiv.org/pdf/1901.05397||||1901.05397||||This article introduces lassopack, a suite of programs for regularized regression in Stata. lassopack implements lasso, square-root lasso, elastic net, ridge regression, adaptive lasso and post-estimation OLS. The methods are suitable for the high-dimensional setting where the number of predictors $p$ may be large and possibly greater than the number of observations, $n$. We offer three different approaches for selecting the penalization (`tuning') parameters: information criteria (implemented in lasso2), $K$-fold cross-validation and $h$-step ahead rolling cross-validation for cross-section, panel and time-series data (cvlasso), and theory-driven (`rigorous') penalization for the lasso and square-root lasso for cross-section and panel data (rlasso). We discuss the theoretical framework and practical considerations for each approach. We also present Monte Carlo results to compare the performance of the penalization approaches.||||@arxiv||||2019/01/16||||lassopack: Model selection and prediction with regularized...||||This article introduces lassopack, a suite of programs for regularized regression in Stata. lassopack implements lasso, square-root lasso, elastic net, ridge regression, adaptive lasso and...||||https://arxiv.org/abs/1901.05397v1||||econ||||
406||||None||||Identification of and correction for publication bias||||arXiv.org||||2017/11/28||||Identification of and correction for publication bias||||Andrews, Isaiah || Kasy, Maximilian||||https://arxiv.org/pdf/1711.10527||||1711.10527||||Some empirical results are more likely to be published than others. Such selective publication leads to biased estimates and distorted inference. This paper proposes two approaches for identifying the conditional probability of publication as a function of a study's results, the first based on systematic replication studies and the second based on meta-studies. For known conditional publication probabilities, we propose median-unbiased estimators and associated confidence sets that correct for selective publication. We apply our methods to recent large-scale replication studies in experimental economics and psychology, and to meta-studies of the effects of minimum wages and de-worming programs.||||@arxiv||||2017/11/28||||Identification of and correction for publication bias||||Some empirical results are more likely to be published than others. Such selective publication leads to biased estimates and distorted inference. This paper proposes two approaches for identifying...||||https://arxiv.org/abs/1711.10527v1||||econ||||
407||||None||||Credit Risk: Simple Closed Form Approximate Maximum Likelihood Estimator||||arXiv.org||||2019/12/29||||Credit Risk: Simple Closed Form Approximate Maximum Likelihood Estimator||||Deo, Anand || Juneja, Sandeep||||https://arxiv.org/pdf/1912.12611||||1912.12611||||We consider discrete default intensity based and logit type reduced form models for conditional default probabilities for corporate loans where we develop simple closed form approximations to the maximum likelihood estimator (MLE) when the underlying covariates follow a stationary Gaussian process. In a practically reasonable asymptotic regime where the default probabilities are small, say 1-3% annually, the number of firms and the time period of data available is reasonably large, we rigorously show that the proposed estimator behaves similarly or slightly worse than the MLE when the underlying model is correctly specified. For more realistic case of model misspecification, both estimators are seen to be equally good, or equally bad. Further, beyond a point, both are more-or-less insensitive to increase in data. These conclusions are validated on empirical and simulated data. The proposed approximations should also have applications outside finance, where logit-type models are used and probabilities of interest are small.||||@arxiv||||2019/12/29||||Credit Risk: Simple Closed Form Approximate Maximum Likelihood Estimator||||We consider discrete default intensity based and logit type reduced form models for conditional default probabilities for corporate loans where we develop simple closed form approximations to the...||||https://arxiv.org/abs/1912.12611v1||||econ||||
408||||None||||Semiparametrically Point-Optimal Hybrid Rank Tests for Unit Roots||||arXiv.org||||2018/06/25||||Semiparametrically Point-Optimal Hybrid Rank Tests for Unit Roots||||Zhou, Bo || Akker, Ramon van den || Werker, Bas J. M.||||https://arxiv.org/pdf/1806.09304||||1806.09304||||We propose a new class of unit root tests that exploits invariance properties in the Locally Asymptotically Brownian Functional limit experiment associated to the unit root model. The invariance structures naturally suggest tests that are based on the ranks of the increments of the observations, their average, and an assumed reference density for the innovations. The tests are semiparametric in the sense that they are valid, i.e., have the correct (asymptotic) size, irrespective of the true innovation density. For a correctly specified reference density, our test is point-optimal and nearly efficient. For arbitrary reference densities, we establish a Chernoff-Savage type result, i.e., our test performs as well as commonly used tests under Gaussian innovations but has improved power under other, e.g., fat-tailed or skewed, innovation distributions. To avoid nonparametric estimation, we propose a simplified version of our test that exhibits the same asymptotic properties, except for the Chernoff-Savage result that we are only able to demonstrate by means of simulations.||||@arxiv||||2018/06/25||||Semiparametrically Point-Optimal Hybrid Rank Tests for Unit Roots||||We propose a new class of unit root tests that exploits invariance properties in the Locally Asymptotically Brownian Functional limit experiment associated to the unit root model. The invariance...||||https://arxiv.org/abs/1806.09304v1||||econ||||
409||||None||||Rebuttal of "On Nonparametric Identification of Treatment Effects in Duration Models"||||arXiv.org||||2019/07/20||||Rebuttal of "On Nonparametric Identification of Treatment Effects in Duration Models"||||Abbring, Jaap H. || Berg, Gerard J. van den||||https://arxiv.org/pdf/1907.09886||||1907.09886||||In their IZA Discussion Paper 10247, Johansson and Lee claim that the main result (Proposition 3) in Abbring and Van den Berg (2003b) does not hold. We show that their claim is incorrect. At a certain point within their line of reasoning, they make a rather basic error while transforming one random variable into another random variable, and this leads them to draw incorrect conclusions. As a result, their paper can be discarded.||||@arxiv||||2019/07/20||||Rebuttal of "On Nonparametric Identification of Treatment...||||In their IZA Discussion Paper 10247, Johansson and Lee claim that the main result (Proposition 3) in Abbring and Van den Berg (2003b) does not hold. We show that their claim is incorrect. At a...||||https://arxiv.org/abs/1907.09886v1||||econ||||
410||||None||||Quantifying the Coherence of Development Policy Priorities||||arXiv.org||||2019/02/01||||Quantifying the Coherence of Development Policy Priorities||||Guerrero, Omar A. || Castañeda, Gonzalo||||https://arxiv.org/pdf/1902.00430||||1902.00430||||Over the last 30 years, the concept of policy coherence for development has received especial attention among academics, practitioners and international organizations. However, its quantification and measurement remain elusive. To address this challenge, we develop a theoretical and empirical framework to measure the coherence of policy priorities for development. Our procedure takes into account the country-specific constraints that governments face when trying to reach specific development goals. Hence, we put forward a new definition of policy coherence where context-specific efficient resource allocations are employed as the baseline to construct an index. To demonstrate the usefulness and validity of our index, we analyze the cases of Mexico, Korea and Estonia, three developing countries that, arguably, joined the OECD with the aim of coherently establishing policies that could enable a catch-up process. We find that Korea shows significant signs of policy coherence, Estonia seems to be in the process of achieving it, and Mexico has unequivocally failed. Furthermore, our results highlight the limitations of assessing coherence in terms of naive benchmark comparisons using development-indicator data. Altogether, our framework sheds new light in a promising direction to develop bespoke analytic tools to meet the 2030 agenda.||||@arxiv||||2019/02/01||||Quantifying the Coherence of Development Policy Priorities||||Over the last 30 years, the concept of policy coherence for development has received especial attention among academics, practitioners and international organizations. However, its quantification...||||https://arxiv.org/abs/1902.00430v1||||econ||||
411||||None||||Population and trends in the global mean temperature||||arXiv.org||||2016/12/27||||Population and trends in the global mean temperature||||Tol, Richard S. J.||||https://arxiv.org/pdf/1612.09123||||1612.09123||||The Fisher Ideal index, developed to measure price inflation, is applied to define a population-weighted temperature trend. This method has the advantages that the trend is representative for the population distribution throughout the sample but without conflating the trend in the population distribution and the trend in the temperature. I show that the trend in the global area-weighted average surface air temperature is different in key details from the population-weighted trend. I extend the index to include urbanization and the urban heat island effect. This substantially changes the trend again. I further extend the index to include international migration, but this has a minor impact on the trend.||||@arxiv||||2016/12/27||||Population and trends in the global mean temperature||||The Fisher Ideal index, developed to measure price inflation, is applied to define a population-weighted temperature trend. This method has the advantages that the trend is representative for the...||||https://arxiv.org/abs/1612.09123v1||||econ||||
412||||None||||Predicting "Design Gaps" in the Market: Deep Consumer Choice Models under Probabilistic Design Constraints||||arXiv.org||||2018/12/28||||Predicting "Design Gaps" in the Market: Deep Consumer Choice Models under Probabilistic Design Constraints||||Burnap, Alex || Hauser, John||||https://arxiv.org/pdf/1812.11067||||1812.11067||||Predicting future successful designs and corresponding market opportunity is a fundamental goal of product design firms. There is accordingly a long history of quantitative approaches that aim to capture diverse consumer preferences, and then translate those preferences to corresponding "design gaps" in the market. We extend this work by developing a deep learning approach to predict design gaps in the market. These design gaps represent clusters of designs that do not yet exist, but are predicted to be both (1) highly preferred by consumers, and (2) feasible to build under engineering and manufacturing constraints. This approach is tested on the entire U.S. automotive market using of millions of real purchase data. We retroactively predict design gaps in the market, and compare predicted design gaps with actual known successful designs. Our preliminary results give evidence it may be possible to predict design gaps, suggesting this approach has promise for early identification of market opportunity.||||@arxiv||||2018/12/28||||Predicting "Design Gaps" in the Market: Deep Consumer...||||Predicting future successful designs and corresponding market opportunity is a fundamental goal of product design firms. There is accordingly a long history of quantitative approaches that aim to...||||https://arxiv.org/abs/1812.11067v1||||econ||||
413||||None||||The Calculus of Democratization and Development||||arXiv.org||||2017/12/12||||The Calculus of Democratization and Development||||Ferguson, Jacob||||https://arxiv.org/pdf/1712.04117||||1712.04117||||In accordance with "Democracy's Effect on Development: More Questions than Answers", we seek to carry out a study in following the description in the 'Questions for Further Study.' To that end, we studied 33 countries in the Sub-Saharan Africa region, who all went through an election which should signal a "step-up" for their democracy, one in which previously homogenous regimes transfer power to an opposition party that fairly won the election. After doing so, liberal-democracy indicators and democracy indicators were evaluated in the five years prior to and after the election took place, and over that ten-year period, we examine the data for trends. If we see positive or negative trends over this time horizon, we are able to conclude that it was the recent increase in the quality of their democracy which led to it. Having investigated examples of this in depth, there seem to be three main archetypes which drive the results. Countries with positive results to their democracy from the election have generally positive effects on their development, countries with more "plateau" like results also did well, but countries for whom the descent to authoritarianism was continued by this election found more negative results.||||@arxiv||||2017/12/12||||The Calculus of Democratization and Development||||In accordance with "Democracy's Effect on Development: More Questions than Answers", we seek to carry out a study in following the description in the 'Questions for Further Study.' To that end, we...||||https://arxiv.org/abs/1712.04117v1||||econ||||
414||||None||||Central Bank Communication and the Yield Curve: A Semi-Automatic Approach using Non-Negative Matrix Factorization||||arXiv.org||||2018/09/24||||Central Bank Communication and the Yield Curve: A Semi-Automatic Approach using Non-Negative Matrix Factorization||||Crayton, Ancil||||https://arxiv.org/pdf/1809.08718||||1809.08718||||Communication is now a standard tool in the central bank's monetary policy toolkit. Theoretically, communication provides the central bank an opportunity to guide public expectations, and it has been shown empirically that central bank communication can lead to financial market fluctuations. However, there has been little research into which dimensions or topics of information are most important in causing these fluctuations. We develop a semi-automatic methodology that summarizes the FOMC statements into its main themes, automatically selects the best model based on coherency, and assesses whether there is a significant impact of these themes on the shape of the U.S Treasury yield curve using topic modeling methods from the machine learning literature. Our findings suggest that the FOMC statements can be decomposed into three topics: (i) information related to the economic conditions and the mandates, (ii) information related to monetary policy tools and intermediate targets, and (iii) information related to financial markets and the financial crisis. We find that statements are most influential during the financial crisis and the effects are mostly present in the curvature of the yield curve through information related to the financial theme.||||@arxiv||||2018/09/24||||Central Bank Communication and the Yield Curve: A Semi-Automatic...||||Communication is now a standard tool in the central bank's monetary policy toolkit. Theoretically, communication provides the central bank an opportunity to guide public expectations, and it has...||||https://arxiv.org/abs/1809.08718v1||||cs||||
415||||None||||The Choice of When to Buy and When To Sell||||arXiv.org||||2019/12/05||||The Choice of When to Buy and When To Sell||||Glazer, Amihai || Hassin, Refael || Nowik, Irit||||https://arxiv.org/pdf/1912.02869||||1912.02869||||A consumer who wants to consume a good at a particular period may nevertheless attempt to buy it earlier if he is concerned that the good will otherwise be sold. We analyze the behavior of consumers in equilibrium and the price a profit-maximizing firm would charge. We show that a firm profits by not selling early. If, however, the firm is obligated to also offer the good early, then the firm may maximize profits by setting a price which induces consumers to all arrive early, or all arrive late, depending on the good's value to the customer.||||@arxiv||||2019/12/05||||The Choice of When to Buy and When To Sell||||A consumer who wants to consume a good at a particular period may nevertheless attempt to buy it earlier if he is concerned that the good will otherwise be sold. We analyze the behavior of...||||https://arxiv.org/abs/1912.02869v1||||econ||||
416||||None||||Artificial Intelligence and Big Data in Entrepreneurship: A New Era Has Begun||||arXiv.org||||2019/06/03||||Artificial Intelligence and Big Data in Entrepreneurship: A New Era Has Begun||||Obschonka, Martin || Audretsch, David B.||||https://arxiv.org/pdf/1906.00553||||1906.00553||||While the disruptive potential of artificial intelligence (AI) and Big Data has been receiving growing attention and concern in a variety of research and application fields over the last few years, it has not received much scrutiny in contemporary entrepreneurship research so far. Here we present some reflections and a collection of papers on the role of AI and Big Data for this emerging area in the study and application of entrepreneurship research. While being mindful of the potentially overwhelming nature of the rapid progress in machine intelligence and other Big Data technologies for contemporary structures in entrepreneurship research, we put an emphasis on the reciprocity of the co-evolving fields of entrepreneurship research and practice. How can AI and Big Data contribute to a productive transformation of the research field and the real-world phenomena (e.g., 'smart entrepreneurship')? We also discuss, however, ethical issues as well as challenges around a potential contradiction between entrepreneurial uncertainty and rule-driven AI rationality. The editorial gives researchers and practitioners orientation and showcases avenues and examples for concrete research in this field. At the same time, however, it is not unlikely that we will encounter unforeseeable and currently inexplicable developments in the field soon. We call on entrepreneurship scholars, educators, and practitioners to proactively prepare for future scenarios.||||@arxiv||||2019/06/03||||Artificial Intelligence and Big Data in Entrepreneurship: A New...||||While the disruptive potential of artificial intelligence (AI) and Big Data has been receiving growing attention and concern in a variety of research and application fields over the last few...||||https://arxiv.org/abs/1906.00553v1||||econ||||
417||||None||||Program Evaluation with Right-Censored Data||||arXiv.org||||2016/04/10||||Program Evaluation with Right-Censored Data||||Sant'Anna, Pedro H. C.||||https://arxiv.org/pdf/1604.02642||||1604.02642||||In a unified framework, we provide estimators and confidence bands for a variety of treatment effects when the outcome of interest, typically a duration, is subjected to right censoring. Our methodology accommodates average, distributional, and quantile treatment effects under different identifying assumptions including unconfoundedness, local treatment effects, and nonlinear differences-in-differences. The proposed estimators are easy to implement, have close-form representation, are fully data-driven upon estimation of nuisance parameters, and do not rely on parametric distributional assumptions, shape restrictions, or on restricting the potential treatment effect heterogeneity across different subpopulations. These treatment effects results are obtained as a consequence of more general results on two-step Kaplan-Meier estimators that are of independent interest: we provide conditions for applying (i) uniform law of large numbers, (ii) functional central limit theorems, and (iii) we prove the validity of the ordinary nonparametric bootstrap in a two-step estimation procedure where the outcome of interest may be randomly censored.||||@arxiv||||2016/04/10||||Program Evaluation with Right-Censored Data||||In a unified framework, we provide estimators and confidence bands for a variety of treatment effects when the outcome of interest, typically a duration, is subjected to right censoring. Our...||||https://arxiv.org/abs/1604.02642v1||||econ||||
418||||None||||Permutation inference with a finite number of heterogeneous clusters||||arXiv.org||||2019/07/01||||Permutation inference with a finite number of heterogeneous clusters||||Hagemann, Andreas||||https://arxiv.org/pdf/1907.01049||||1907.01049||||I introduce a simple permutation procedure to test conventional (non-sharp) hypotheses about the effect of a binary treatment in the presence of a finite number of large, heterogeneous clusters when the treatment effect is identified by comparisons across clusters. The procedure asymptotically controls size by applying a level-adjusted permutation test to a suitable statistic. The adjustments needed for most empirically relevant situations are tabulated in the paper. The adjusted permutation test is easy to implement in practice and performs well at conventional levels of significance with at least four treated clusters and a similar number of control clusters. It is particularly robust to situations where some clusters are much more variable than others. Examples and an empirical application are provided.||||@arxiv||||2019/07/01||||Permutation inference with a finite number of heterogeneous clusters||||I introduce a simple permutation procedure to test conventional (non-sharp) hypotheses about the effect of a binary treatment in the presence of a finite number of large, heterogeneous clusters...||||https://arxiv.org/abs/1907.01049v1||||econ||||
419||||None||||External Threats, Political Turnover and Fiscal Capacity||||arXiv.org||||2020/01/08||||External Threats, Political Turnover and Fiscal Capacity||||Galindo-Silva, Hector||||https://arxiv.org/pdf/2001.02322||||2001.02322||||In most of the recent literature on state capacity, the significance of wars in state-building assumes that threats from foreign countries generate common interests among domestic groups, leading to larger investments in state capacity. However, many countries that have suffered external conflicts don't experience increased unity. Instead, they face factional politics that often lead to destructive civil wars. This paper develops a theory of the impact of interstate conflicts on fiscal capacity in which fighting an external threat is not always a common-interest public good, and in which interstate conflicts can lead to civil wars. The theory identifies conditions under which an increased risk of external conflict decreases the chance of civil war, which in turn results in a government with a longer political life and with more incentives to invest in fiscal capacity. These conditions depend on the cohesiveness of institutions, but in a non-trivial and novel way: a higher risk of an external conflict that results in lower political turnover, but that also makes a foreign invasion more likely, contributes to state-building only if institutions are sufficiently incohesive.||||@arxiv||||2020/01/08||||External Threats, Political Turnover and Fiscal Capacity||||In most of the recent literature on state capacity, the significance of wars in state-building assumes that threats from foreign countries generate common interests among domestic groups, leading...||||https://arxiv.org/abs/2001.02322v1||||econ||||
420||||None||||Distributed Mechanism Design for Multicast Transmission||||arXiv.org||||2019/01/08||||Distributed Mechanism Design for Multicast Transmission||||Heydaribeni, Nasimeh || Anastasopoulos, Achilleas||||https://arxiv.org/pdf/1803.08111||||1803.08111||||In the standard Mechanism Design framework (Hurwicz-Reiter), there is a central authority that gathers agents' messages and subsequently determines the allocation and tax for each agent.   We consider a scenario where, due to communication overhead and other constraints, such broadcasting of messages to a central authority cannot take place. Instead, only local message exchange is allowed between agents. As a result, each agent should be able to determine her own allocation and tax based on the messages in the local neighborhood, as defined by a given message graph describing the communication constraints.   This scenario gives rise to a novel research direction that we call "Distributed Mechanism Design".   In this paper, we propose such a distributed mechanism for the problem of rate allocation in a multicast transmission network.   The proposed mechanism fully implements the optimal allocation in Nash equilibria and its message space dimension is linear with respect to the number of agents in the network.||||@arxiv||||2018/03/21||||Distributed Mechanism Design for Multicast Transmission||||In the standard Mechanism Design framework (Hurwicz-Reiter), there is a central authority that gathers agents' messages and subsequently determines the allocation and tax for each agent.   We...||||https://arxiv.org/abs/1803.08111v2||||cs||||
421||||None||||How to design a derivatives market?||||arXiv.org||||2019/09/19||||How to design a derivatives market?||||Baldacci, Bastien || Jusselin, Paul || Rosenbaum, Mathieu||||https://arxiv.org/pdf/1909.09257||||1909.09257||||We consider the problem of designing a derivatives exchange aiming at addressing clients needs in terms of listed options and providing suitable liquidity. We proceed into two steps. First we use a quantization method to select the options that should be displayed by the exchange. Then, using a principal-agent approach, we design a make take fees contract between the exchange and the market maker. The role of this contract is to provide incentives to the market maker so that he offers small spreads for the whole range of listed options, hence attracting transactions and meeting the commercial requirements of the exchange.||||@arxiv||||2019/09/19||||How to design a derivatives market?||||We consider the problem of designing a derivatives exchange aiming at addressing clients needs in terms of listed options and providing suitable liquidity. We proceed into two steps. First we use...||||https://arxiv.org/abs/1909.09257v1||||econ||||
422||||None||||Estimation and HAC-based Inference for Machine Learning Time Series Regressions||||arXiv.org||||2019/12/13||||Estimation and HAC-based Inference for Machine Learning Time Series Regressions||||Babii, Andrii || Ghysels, Eric || Striaukas, Jonas||||https://arxiv.org/pdf/1912.06307||||1912.06307||||Time series regression analysis in econometrics typically involves a framework relying on a set of mixing conditions to establish consistency and asymptotic normality of parameter estimates and HAC-type estimators of the residual long-run variances to conduct proper inference. This article introduces structured machine learning regressions for high-dimensional time series data using the aforementioned commonly used setting. To recognize the time series data structures we rely on the sparse-group LASSO estimator. We derive a new Fuk-Nagaev inequality for a class of $τ$-dependent processes with heavier than Gaussian tails, nesting $α$-mixing processes as a special case, and establish estimation, prediction, and inferential properties, including convergence rates of the HAC estimator for the long-run variance based on LASSO residuals. An empirical application to nowcasting US GDP growth indicates that the estimator performs favorably compared to other alternatives and that the text data can be a useful addition to more traditional numerical data.||||@arxiv||||2019/12/13||||Estimation and HAC-based Inference for Machine Learning Time...||||Time series regression analysis in econometrics typically involves a framework relying on a set of mixing conditions to establish consistency and asymptotic normality of parameter estimates and...||||https://arxiv.org/abs/1912.06307v1||||econ||||
423||||None||||Nonparametric Regression with Selectively Missing Covariates||||arXiv.org||||2019/12/03||||Nonparametric Regression with Selectively Missing Covariates||||Breunig, Christoph || Haan, Peter||||https://arxiv.org/pdf/1810.00411||||1810.00411||||We consider the problem of regressions with selectively observed covariates in a nonparametric framework. Our approach relies on instrumental variables that explain variation in the latent covariates but have no direct effect on selection. The regression function of interest is shown to be a weighted version of observed conditional expectation where the weighting function is a fraction of selection probabilities. Nonparametric identification of the fractional probability weight (FPW) function is achieved via a partial completeness assumption. We provide primitive functional form assumptions for partial completeness to hold. The identification result is constructive for the FPW series estimator. We derive the rate of convergence and also the pointwise asymptotic distribution. In both cases, the asymptotic performance of the FPW series estimator does not suffer from the inverse problem which derives from the nonparametric instrumental variable approach. In a Monte Carlo study, we analyze the finite sample properties of our estimator and we demonstrate the usefulness of our method in analyses based on survey data. In the empirical application, we focus on two different applications. We estimate the association between income and health using linked data from the SHARE survey data and administrative pension information and use pension entitlements as an instrument. In the second application we revisit the question how income affects the demand for housing based on data from the Socio-Economic Panel Study. In this application we use regional income information on the residential block level as an instrument. In both applications we show that income is selectively missing and we demonstrate that standard methods that do not account for the nonrandom selection process lead to significantly biased estimates for individuals with low income.||||@arxiv||||2018/09/30||||Nonparametric Regression with Selectively Missing Covariates||||We consider the problem of regressions with selectively observed covariates in a nonparametric framework. Our approach relies on instrumental variables that explain variation in the latent...||||https://arxiv.org/abs/1810.00411v2||||econ||||
424||||None||||Goodness-of-Fit Tests based on Series Estimators in Nonparametric Instrumental Regression||||arXiv.org||||2019/09/23||||Goodness-of-Fit Tests based on Series Estimators in Nonparametric Instrumental Regression||||Breunig, Christoph||||https://arxiv.org/pdf/1909.10133||||1909.10133||||This paper proposes several tests of restricted specification in nonparametric instrumental regression. Based on series estimators, test statistics are established that allow for tests of the general model against a parametric or nonparametric specification as well as a test of exogeneity of the vector of regressors. The tests' asymptotic distributions under correct specification are derived and their consistency against any alternative model is shown. Under a sequence of local alternative hypotheses, the asymptotic distributions of the tests is derived. Moreover, uniform consistency is established over a class of alternatives whose distance to the null hypothesis shrinks appropriately as the sample size increases. A Monte Carlo study examines finite sample performance of the test statistics.||||@arxiv||||2019/09/23||||Goodness-of-Fit Tests based on Series Estimators in Nonparametric...||||This paper proposes several tests of restricted specification in nonparametric instrumental regression. Based on series estimators, test statistics are established that allow for tests of the...||||https://arxiv.org/abs/1909.10133v1||||econ||||
425||||None||||Optimal Iterative Threshold-Kernel Estimation of Jump Diffusion Processes||||arXiv.org||||2019/09/11||||Optimal Iterative Threshold-Kernel Estimation of Jump Diffusion Processes||||Figueroa-López, José E. || Li, Cheng || Nisen, Jeffrey||||https://arxiv.org/pdf/1811.07499||||1811.07499||||In this paper, we propose a new threshold-kernel jump-detection method for jump-diffusion processes, which iteratively applies thresholding and kernel methods in an approximately optimal way to achieve improved finite-sample performance. We use the expected number of jump misclassifications as the objective function to optimally select the threshold parameter of the jump detection scheme. We prove that the objective function is quasi-convex and obtain a new second-order infill approximation of the optimal threshold in closed form. The approximate optimal threshold depends not only on the spot volatility, but also the jump intensity and the value of the jump density at the origin. Estimation methods for these quantities are then developed, where the spot volatility is estimated by a kernel estimator with thresholding and the value of the jump density at the origin is estimated by a density kernel estimator applied to those increments deemed to contains jumps by the chosen thresholding criterion. Due to the interdependency between the model parameters and the approximate optimal estimators built to estimate them, a type of iterative fixed-point algorithm is developed to implement them. Simulation studies for a prototypical stochastic volatility model show that it is not only feasible to implement the higher-order local optimal threshold scheme but also that this is superior to those based only on the first order approximation and/or on average values of the parameters over the estimation time period.||||@arxiv||||2018/11/19||||Optimal Iterative Threshold-Kernel Estimation of Jump Diffusion Processes||||In this paper, we propose a new threshold-kernel jump-detection method for jump-diffusion processes, which iteratively applies thresholding and kernel methods in an approximately optimal way to...||||https://arxiv.org/abs/1811.07499v2||||econ||||
426||||None||||De-biased Machine Learning for Compliers||||arXiv.org||||2019/09/10||||De-biased Machine Learning for Compliers||||Singh, Rahul || Sun, Liyang||||https://arxiv.org/pdf/1909.05244||||1909.05244||||Instrumental variable identification is a concept in causal statistics for estimating the counterfactual effect of treatment D on output Y controlling for covariates X using observational data. Even when measurements of (Y,D) are confounded, the treatment effect on the subpopulation of compliers can nonetheless be identified if an instrumental variable Z is available, which is independent of (Y,D) conditional on X and the unmeasured confounder. We introduce a de-biased machine learning (DML) approach to estimating complier parameters with high-dimensional data. Complier parameters include local average treatment effect, average complier characteristics, and complier counterfactual outcome distributions. In our approach, the de-biasing is itself performed by machine learning, a variant called de-biased machine learning via regularized Riesz representers (DML-RRR). We prove our estimator is consistent, asymptotically normal, and semi-parametrically efficient. In experiments, our estimator outperforms state of the art alternatives. We use it to estimate the effect of 401(k) participation on the distribution of net financial assets.||||@arxiv||||2019/09/10||||De-biased Machine Learning for Compliers||||Instrumental variable identification is a concept in causal statistics for estimating the counterfactual effect of treatment D on output Y controlling for covariates X using observational data....||||https://arxiv.org/abs/1909.05244v1||||cs||||
427||||None||||A Review on Energy, Environmental, and Sustainability Implications of Connected and Automated Vehicles||||arXiv.org||||2019/02/17||||A Review on Energy, Environmental, and Sustainability Implications of Connected and Automated Vehicles||||Taiebat, Morteza || Brown, Austin L. || Safford, Hannah R. || Qu, Shen || Xu, Ming||||https://arxiv.org/pdf/1901.10581||||1901.10581||||Connected and automated vehicles (CAVs) are poised to reshape transportation and mobility by replacing humans as the driver and service provider. While the primary stated motivation for vehicle automation is to improve safety and convenience of road mobility, this transformation also provides a valuable opportunity to improve vehicle energy efficiency and reduce emissions in the transportation sector. Progress in vehicle efficiency and functionality, however, does not necessarily translate to net positive environmental outcomes. Here we examine the interactions between CAV technology and the environment at four levels of increasing complexity: vehicle, transportation system, urban system, and society. We find that environmental impacts come from CAV-facilitated transformations at all four levels, rather than from CAV technology directly. We anticipate net positive environmental impacts at the vehicle, transportation system, and urban system levels, but expect greater vehicle utilization and shifts in travel patterns at the society level to offset some of these benefits. Focusing on the vehicle-level improvements associated with CAV technology is likely to yield excessively optimistic estimates of environmental benefits. Future research and policy efforts should strive to clarify the extent and possible synergetic effects from a systems level in order to envisage and address concerns regarding the short- and long-term sustainable adoption of CAV technology.||||@arxiv||||2019/01/23||||A Review on Energy, Environmental, and Sustainability Implications...||||Connected and automated vehicles (CAVs) are poised to reshape transportation and mobility by replacing humans as the driver and service provider. While the primary stated motivation for vehicle...||||https://arxiv.org/abs/1901.10581v2||||cs||||
428||||None||||Deep Neural Networks for Choice Analysis: Architectural Design with Alternative-Specific Utility Functions||||arXiv.org||||2019/09/16||||Deep Neural Networks for Choice Analysis: Architectural Design with Alternative-Specific Utility Functions||||Wang, Shenhao || Zhao, Jinhua||||https://arxiv.org/pdf/1909.07481||||1909.07481||||Whereas deep neural network (DNN) is increasingly applied to choice analysis, it is challenging to reconcile domain-specific behavioral knowledge with generic-purpose DNN, to improve DNN's interpretability and predictive power, and to identify effective regularization methods for specific tasks. This study designs a particular DNN architecture with alternative-specific utility functions (ASU-DNN) by using prior behavioral knowledge. Unlike a fully connected DNN (F-DNN), which computes the utility value of an alternative k by using the attributes of all the alternatives, ASU-DNN computes it by using only k's own attributes. Theoretically, ASU-DNN can dramatically reduce the estimation error of F-DNN because of its lighter architecture and sparser connectivity. Empirically, ASU-DNN has 2-3% higher prediction accuracy than F-DNN over the whole hyperparameter space in a private dataset that we collected in Singapore and a public dataset in R mlogit package. The alternative-specific connectivity constraint, as a domain-knowledge-based regularization method, is more effective than the most popular generic-purpose explicit and implicit regularization methods and architectural hyperparameters. ASU-DNN is also more interpretable because it provides a more regular substitution pattern of travel mode choices than F-DNN does. The comparison between ASU-DNN and F-DNN can also aid in testing the behavioral knowledge. Our results reveal that individuals are more likely to compute utility by using an alternative's own attributes, supporting the long-standing practice in choice modeling. Overall, this study demonstrates that prior behavioral knowledge could be used to guide the architecture design of DNN, to function as an effective domain-knowledge-based regularization method, and to improve both the interpretability and predictive power of DNN in choice analysis.||||@arxiv||||2019/09/16||||Deep Neural Networks for Choice Analysis: Architectural Design...||||Whereas deep neural network (DNN) is increasingly applied to choice analysis, it is challenging to reconcile domain-specific behavioral knowledge with generic-purpose DNN, to improve DNN's...||||https://arxiv.org/abs/1909.07481v1||||cs||||
429||||None||||A Socioeconomic Well-Being Index||||arXiv.org||||2020/01/04||||A Socioeconomic Well-Being Index||||Trindade, A. Alexandre || Shirvani, Abootaleb || Ma, Xiaohan||||https://arxiv.org/pdf/2001.01036||||2001.01036||||An annual well-being index constructed from thirteen socioeconomic factors is proposed in order to dynamically measure the mood of the US citizenry. Econometric models are fitted to the log-returns of the index in order to quantify its tail risk and perform option pricing and risk budgeting. By providing a statistically sound assessment of socioeconomic content, the index is consistent with rational finance theory, enabling the construction and valuation of insurance-type financial instruments to serve as contracts written against it. Endogenously, the VXO volatility measure of the stock market appears to be the greatest contributor to tail risk. Exogenously, "stress-testing" the index against the politically important factors of trade imbalance and legal immigration, quantify the systemic risk. For probability levels in the range of 5% to 10%, values of trade below these thresholds are associated with larger downward movements of the index than for immigration at the same level. The main intent of the index is to provide early-warning for negative changes in the mood of citizens, thus alerting policy makers and private agents to potential future market downturns.||||@arxiv||||2020/01/04||||A Socioeconomic Well-Being Index||||An annual well-being index constructed from thirteen socioeconomic factors is proposed in order to dynamically measure the mood of the US citizenry. Econometric models are fitted to the...||||https://arxiv.org/abs/2001.01036v1||||econ||||
430||||None||||$L_2$Boosting for Economic Applications||||arXiv.org||||2017/02/10||||$L_2$Boosting for Economic Applications||||Luo, Ye || Spindler, Martin||||https://arxiv.org/pdf/1702.03244||||1702.03244||||In the recent years more and more high-dimensional data sets, where the number of parameters $p$ is high compared to the number of observations $n$ or even larger, are available for applied researchers. Boosting algorithms represent one of the major advances in machine learning and statistics in recent years and are suitable for the analysis of such data sets. While Lasso has been applied very successfully for high-dimensional data sets in Economics, boosting has been underutilized in this field, although it has been proven very powerful in fields like Biostatistics and Pattern Recognition. We attribute this to missing theoretical results for boosting. The goal of this paper is to fill this gap and show that boosting is a competitive method for inference of a treatment effect or instrumental variable (IV) estimation in a high-dimensional setting. First, we present the $L_2$Boosting with componentwise least squares algorithm and variants which are tailored for regression problems which are the workhorse for most Econometric problems. Then we show how $L_2$Boosting can be used for estimation of treatment effects and IV estimation. We highlight the methods and illustrate them with simulations and empirical examples. For further results and technical details we refer to Luo and Spindler (2016, 2017) and to the online supplement of the paper.||||@arxiv||||2017/02/10||||$L_2$Boosting for Economic Applications||||In the recent years more and more high-dimensional data sets, where the number of parameters $p$ is high compared to the number of observations $n$ or even larger, are available for applied...||||https://arxiv.org/abs/1702.03244v1||||econ||||
431||||None||||Social learning equilibria||||arXiv.org||||2019/09/27||||Social learning equilibria||||Mossel, Elchanan || Mueller-Frank, Manuel || Sly, Allan || Tamuz, Omer||||https://arxiv.org/pdf/1207.5895||||1207.5895||||We consider a large class of social learning models in which a group of agents face uncertainty regarding a state of the world, share the same utility function, observe private signals, and interact in a general dynamic setting. We introduce Social Learning Equilibria, a static equilibrium concept that abstracts away from the details of the given extensive form, but nevertheless captures the corresponding asymptotic equilibrium behavior. We establish general conditions for agreement, herding, and information aggregation in equilibrium, highlighting a connection between agreement and information aggregation.||||@arxiv||||2012/07/25||||Social learning equilibria||||We consider a large class of social learning models in which a group of agents face uncertainty regarding a state of the world, share the same utility function, observe private signals, and...||||https://arxiv.org/abs/1207.5895v4||||econ||||
432||||None||||Probability Assessments of an Ice-Free Arctic: Comparing Statistical and Climate Model Projections||||arXiv.org||||2019/12/23||||Probability Assessments of an Ice-Free Arctic: Comparing Statistical and Climate Model Projections||||Diebold, Francis X. || Rudebusch, Glenn D.||||https://arxiv.org/pdf/1912.10774||||1912.10774||||The downward trend in Arctic sea ice is a key factor determining the pace and intensity of future global climate change; moreover, declines in sea ice can have a wide range of additional environmental and economic consequences. Based on several decades of satellite data, we provide statistical forecasts of Arctic sea ice extent during the rest of this century. The best-fitting statistical model indicates that sea ice is diminishing at an increasing rate. By contrast, average projections from the CMIP5 global climate models foresee a gradual slowing of sea ice loss even in high carbon emissions scenarios. Our long-range statistical projections also deliver probability assessments of the timing of an ice-free Arctic. This analysis indicates almost a 60 percent chance of an effectively ice-free Arctic Ocean in the 2030s -- much earlier than the average projection from global climate models.||||@arxiv||||2019/12/23||||Probability Assessments of an Ice-Free Arctic: Comparing...||||The downward trend in Arctic sea ice is a key factor determining the pace and intensity of future global climate change; moreover, declines in sea ice can have a wide range of additional...||||https://arxiv.org/abs/1912.10774v1||||econ||||
433||||None||||Profit-oriented sales forecasting: a comparison of forecasting techniques from a business perspective||||arXiv.org||||2020/02/03||||Profit-oriented sales forecasting: a comparison of forecasting techniques from a business perspective||||Van Calster, Tine || Bossche, Filip Van den || Baesens, Bart || Lemahieu, Wilfried||||https://arxiv.org/pdf/2002.00949||||2002.00949||||Choosing the technique that is the best at forecasting your data, is a problem that arises in any forecasting application. Decades of research have resulted into an enormous amount of forecasting methods that stem from statistics, econometrics and machine learning (ML), which leads to a very difficult and elaborate choice to make in any forecasting exercise. This paper aims to facilitate this process for high-level tactical sales forecasts by comparing a large array of techniques for 35 times series that consist of both industry data from the Coca-Cola Company and publicly available datasets. However, instead of solely focusing on the accuracy of the resulting forecasts, this paper introduces a novel and completely automated profit-driven approach that takes into account the expected profit that a technique can create during both the model building and evaluation process. The expected profit function that is used for this purpose, is easy to understand and adaptable to any situation by combining forecasting accuracy with business expertise. Furthermore, we examine the added value of ML techniques, the inclusion of external factors and the use of seasonal models in order to ascertain which type of model works best in tactical sales forecasting. Our findings show that simple seasonal time series models consistently outperform other methodologies and that the profit-driven approach can lead to selecting a different forecasting model.||||@arxiv||||2020/02/03||||Profit-oriented sales forecasting: a comparison of forecasting...||||Choosing the technique that is the best at forecasting your data, is a problem that arises in any forecasting application. Decades of research have resulted into an enormous amount of forecasting...||||https://arxiv.org/abs/2002.00949v1||||cs||||
434||||None||||Expressive mechanisms for equitable rent division on a budget||||arXiv.org||||2020/02/11||||Expressive mechanisms for equitable rent division on a budget||||Velez, Rodrigo A.||||https://arxiv.org/pdf/1902.02935||||1902.02935||||We study the incentive properties of envy-free mechanisms for the allocation of rooms and payments of rent among financially constrained roommates. Each agent reports her values for rooms, her housing earmark (soft budget), and an index that reflects the difficulty the agent experiences from having to pay over this amount. Then an envy-free allocation for these reports is recommended. We identify conditions under which the complete information non-cooperative outcomes of these mechanisms are exactly the envy-free allocations with respect to true preferences.||||@arxiv||||2019/02/08||||Expressive mechanisms for equitable rent division on a budget||||We study the incentive properties of envy-free mechanisms for the allocation of rooms and payments of rent among financially constrained roommates. Each agent reports her values for rooms, her...||||https://arxiv.org/abs/1902.02935v2||||cs||||
435||||None||||Nonparametric instrumental variable estimation under monotonicity||||arXiv.org||||2015/07/19||||Nonparametric instrumental variable estimation under monotonicity||||Chetverikov, Denis || Wilhelm, Daniel||||https://arxiv.org/pdf/1507.05270||||1507.05270||||The ill-posedness of the inverse problem of recovering a regression function in a nonparametric instrumental variable model leads to estimators that may suffer from a very slow, logarithmic rate of convergence. In this paper, we show that restricting the problem to models with monotone regression functions and monotone instruments significantly weakens the ill-posedness of the problem. In stark contrast to the existing literature, the presence of a monotone instrument implies boundedness of our measure of ill-posedness when restricted to the space of monotone functions. Based on this result we derive a novel non-asymptotic error bound for the constrained estimator that imposes monotonicity of the regression function. For a given sample size, the bound is independent of the degree of ill-posedness as long as the regression function is not too steep. As an implication, the bound allows us to show that the constrained estimator converges at a fast, polynomial rate, independently of the degree of ill-posedness, in a large, but slowly shrinking neighborhood of constant functions. Our simulation study demonstrates significant finite-sample performance gains from imposing monotonicity even when the regression function is rather far from being a constant. We apply the constrained estimator to the problem of estimating gasoline demand functions from U.S. data.||||@arxiv||||2015/07/19||||Nonparametric instrumental variable estimation under monotonicity||||The ill-posedness of the inverse problem of recovering a regression function in a nonparametric instrumental variable model leads to estimators that may suffer from a very slow, logarithmic rate...||||https://arxiv.org/abs/1507.05270v1||||econ||||
436||||None||||The Research on the Stagnant Development of Shantou Special Economic Zone Under Reform and Opening-Up Policy||||arXiv.org||||2017/11/24||||The Research on the Stagnant Development of Shantou Special Economic Zone Under Reform and Opening-Up Policy||||Cai, Bowen||||https://arxiv.org/pdf/1711.08877||||1711.08877||||This study briefly introduces the development of Shantou Special Economic Zone under Reform and Opening-Up Policy from 1980 through 2016 with a focus on policy making issues and its influences on local economy. This paper is divided into two parts, 1980 to 1991, 1992 to 2016 in accordance with the separation of the original Shantou District into three cities: Shantou, Chaozhou and Jieyang in the end of 1991. This study analyzes the policy making issues in the separation of the original Shantou District, the influences of the policy on Shantou's economy after separation, the possibility of merging the three cities into one big new economic district in the future and reasons that lead to the stagnant development of Shantou in recent 20 years. This paper uses statistical longitudinal analysis in analyzing economic problems with applications of non-parametric statistics through generalized additive model and time series forecasting methods. The paper is authored by Bowen Cai solely, who is the graduate student in the PhD program of Applied and Computational Mathematics and Statistics at the University of Notre Dame with concentration in big data analysis.||||@arxiv||||2017/11/24||||The Research on the Stagnant Development of Shantou Special...||||This study briefly introduces the development of Shantou Special Economic Zone under Reform and Opening-Up Policy from 1980 through 2016 with a focus on policy making issues and its influences on...||||https://arxiv.org/abs/1711.08877v1||||econ||||
437||||None||||On testing substitutability||||arXiv.org||||2018/05/19||||On testing substitutability||||Croitoru, Cosmina || Mehlhorn, Kurt||||https://arxiv.org/pdf/1805.07642||||1805.07642||||The papers~\cite{hatfimmokomi11} and~\cite{azizbrilharr13} propose algorithms for testing whether the choice function induced by a (strict) preference list of length $N$ over a universe $U$ is substitutable. The running time of these algorithms is $O(|U|^3\cdot N^3)$, respectively $O(|U|^2\cdot N^3)$. In this note we present an algorithm with running time $O(|U|^2\cdot N^2)$. Note that $N$ may be exponential in the size $|U|$ of the universe.||||@arxiv||||2018/05/19||||On testing substitutability||||The papers~\cite{hatfimmokomi11} and~\cite{azizbrilharr13} propose algorithms for testing whether the choice function induced by a (strict) preference list of length $N$ over a universe $U$ is...||||https://arxiv.org/abs/1805.07642v1||||cs||||
438||||None||||L1-Penalized Quantile Regression in High-Dimensional Sparse Models||||arXiv.org||||2019/09/26||||L1-Penalized Quantile Regression in High-Dimensional Sparse Models||||Belloni, Alexandre || Chernozhukov, Victor||||https://arxiv.org/pdf/0904.2931||||0904.2931||||We consider median regression and, more generally, a possibly infinite collection of quantile regressions in high-dimensional sparse models. In these models the overall number of regressors $p$ is very large, possibly larger than the sample size $n$, but only $s$ of these regressors have non-zero impact on the conditional quantile of the response variable, where $s$ grows slower than $n$. We consider quantile regression penalized by the $\ell_1$-norm of coefficients ($\ell_1$-QR). First, we show that $\ell_1$-QR is consistent at the rate $\sqrt{s/n} \sqrt{\log p}$. The overall number of regressors $p$ affects the rate only through the $\log p$ factor, thus allowing nearly exponential growth in the number of zero-impact regressors. The rate result holds under relatively weak conditions, requiring that $s/n$ converges to zero at a super-logarithmic speed and that regularization parameter satisfies certain theoretical constraints. Second, we propose a pivotal, data-driven choice of the regularization parameter and show that it satisfies these theoretical constraints. Third, we show that $\ell_1$-QR correctly selects the true minimal model as a valid submodel, when the non-zero coefficients of the true model are well separated from zero. We also show that the number of non-zero coefficients in $\ell_1$-QR is of same stochastic order as $s$. Fourth, we analyze the rate of convergence of a two-step estimator that applies ordinary quantile regression to the selected model. Fifth, we evaluate the performance of $\ell_1$-QR in a Monte-Carlo experiment, and illustrate its use on an international economic growth application.||||@arxiv||||2009/04/19||||L1-Penalized Quantile Regression in High-Dimensional Sparse Models||||We consider median regression and, more generally, a possibly infinite collection of quantile regressions in high-dimensional sparse models. In these models the overall number of regressors $p$ is...||||https://arxiv.org/abs/0904.2931v5||||econ||||
439||||None||||Informational Content of Factor Structures in Simultaneous Binary Response Models||||arXiv.org||||2019/10/03||||Informational Content of Factor Structures in Simultaneous Binary Response Models||||Khan, Shakeeb || Maurel, Arnaud || Zhang, Yichong||||https://arxiv.org/pdf/1910.01318||||1910.01318||||We study the informational content of factor structures in discrete triangular systems. Factor structures have been employed in a variety of settings in cross sectional and panel data models, and in this paper we formally quantify their identifying power in a bivariate system often employed in the treatment effects literature. Our main findings are that imposing a factor structure yields point identification of parameters of interest, such as the coefficient associated with the endogenous regressor in the outcome equation, under weaker assumptions than usually required in these systems. In particular, we show that an exclusion restriction, requiring an explanatory variable in the outcome equation to be excluded from the treatment equation, is no longer necessary for identification. Under such settings, we propose a rank estimator for both the factor loading and the causal effect parameter that are root-n consistent and asymptotically normal. The estimator's finite sample properties are evaluated through a simulation study. We also establish identification results in models with more general factor structures, that are characterized by nonparametric functional forms and multiple idiosyncratic shocks.||||@arxiv||||2019/10/03||||Informational Content of Factor Structures in Simultaneous Binary...||||We study the informational content of factor structures in discrete triangular systems. Factor structures have been employed in a variety of settings in cross sectional and panel data models, and...||||https://arxiv.org/abs/1910.01318v1||||econ||||
440||||None||||A two-dimensional propensity score matching method for longitudinal quasi-experimental studies: A focus on travel behavior and the built environment||||arXiv.org||||2019/11/05||||A two-dimensional propensity score matching method for longitudinal quasi-experimental studies: A focus on travel behavior and the built environment||||Zhong, Haotian || Li, Wei || Boarnet, Marlon G.||||https://arxiv.org/pdf/1911.00667||||1911.00667||||The lack of longitudinal studies of the relationship between the built environment and travel behavior has been widely discussed in the literature. This paper discusses how standard propensity score matching estimators can be extended to enable such studies by pairing observations across two dimensions: longitudinal and cross-sectional. Researchers mimic randomized controlled trials (RCTs) and match observations in both dimensions, to find synthetic control groups that are similar to the treatment group and to match subjects synthetically across before-treatment and after-treatment time periods. We call this a two-dimensional propensity score matching (2DPSM). This method demonstrates superior performance for estimating treatment effects based on Monte Carlo evidence. A near-term opportunity for such matching is identifying the impact of transportation infrastructure on travel behavior.||||@arxiv||||2019/11/02||||A two-dimensional propensity score matching method for...||||The lack of longitudinal studies of the relationship between the built environment and travel behavior has been widely discussed in the literature. This paper discusses how standard propensity...||||https://arxiv.org/abs/1911.00667v2||||econ||||
441||||None||||Associating Ridesourcing with Road Safety Outcomes: Insights from Austin Texas||||arXiv.org||||2020/01/08||||Associating Ridesourcing with Road Safety Outcomes: Insights from Austin Texas||||Kontou, Eleftheria || McDonald, Noreen C.||||https://arxiv.org/pdf/2001.03461||||2001.03461||||Improving road safety and setting targets for reducing traffic-related crashes and deaths are highlighted as part of the United Nation's sustainable development goals and vision zero efforts around the globe. The advent of transportation network companies, such as ridesourcing, expands mobility options in cities and may impact road safety outcomes. In this study, we analyze the effects of ridesourcing use on road crashes, injuries, fatalities, and driving while intoxicated (DWI) offenses in Travis County Texas. Our approach leverages real-time ridesourcing volume to explain variation in road safety outcomes. Spatial panel data models with fixed effects are deployed to examine whether the use of ridesourcing is significantly associated with road crashes and other safety metrics. Our results suggest that for a 10% increase in ridesourcing trips, we expect a 0.12% decrease in road crashes (p<0.05), a 0.25% decrease in road injuries (p<0.001), and a 0.36% decrease in DWI offenses (p<0.0001) in Travis County. Ridesourcing use is not associated with road fatalities at a 0.05 significance level. This study augments existing work because it moves beyond binary indicators of ridesourcing presence or absence and analyzes patterns within an urbanized area rather than metropolitan-level variation. Contributions include developing a data-rich approach for assessing the impacts of ridesourcing use on our transportation system's safety, which may serve as a template for future analyses of other US cities. Our findings provide feedback to policymakers by clarifying associations between ridesourcing use and traffic safety, while helping identify sets of actions to achieve safer and more efficient shared mobility systems.||||@arxiv||||2020/01/08||||Associating Ridesourcing with Road Safety Outcomes: Insights from...||||Improving road safety and setting targets for reducing traffic-related crashes and deaths are highlighted as part of the United Nation's sustainable development goals and vision zero efforts...||||https://arxiv.org/abs/2001.03461v1||||econ||||
442||||None||||At What Level Should One Cluster Standard Errors in Paired Experiments, and in Stratified Experiments with Small Strata?||||arXiv.org||||2020/01/17||||At What Level Should One Cluster Standard Errors in Paired Experiments, and in Stratified Experiments with Small Strata?||||de Chaisemartin, Clément || Ramirez-Cuellar, Jaime||||https://arxiv.org/pdf/1906.00288||||1906.00288||||In paired experiments, units are matched into pairs, and one unit of each pair is randomly assigned to treatment. To estimate the treatment effect, researchers often regress their outcome on a treatment indicator and pair fixed effects, clustering standard errors at the unit-of-randomization level. We show that the variance estimator in this regression may be severely downward biased: under constant treatment effect, its expectation equals 1/2 of the true variance. Instead, we show that researchers should cluster their standard errors at the pair level. Using simulations, we show that those results extend to stratified experiments with few units per strata.||||@arxiv||||2019/06/01||||At What Level Should One Cluster Standard Errors in Paired...||||In paired experiments, units are matched into pairs, and one unit of each pair is randomly assigned to treatment. To estimate the treatment effect, researchers often regress their outcome on a...||||https://arxiv.org/abs/1906.00288v4||||econ||||
443||||None||||Oracle Estimation of a Change Point in High Dimensional Quantile Regression||||arXiv.org||||2016/12/16||||Oracle Estimation of a Change Point in High Dimensional Quantile Regression||||Lee, Sokbae || Liao, Yuan || Seo, Myung Hwan || Shin, Youngki||||https://arxiv.org/pdf/1603.00235||||1603.00235||||In this paper, we consider a high-dimensional quantile regression model where the sparsity structure may differ between two sub-populations. We develop $\ell_1$-penalized estimators of both regression coefficients and the threshold parameter. Our penalized estimators not only select covariates but also discriminate between a model with homogeneous sparsity and a model with a change point. As a result, it is not necessary to know or pretest whether the change point is present, or where it occurs. Our estimator of the change point achieves an oracle property in the sense that its asymptotic distribution is the same as if the unknown active sets of regression coefficients were known. Importantly, we establish this oracle property without a perfect covariate selection, thereby avoiding the need for the minimum level condition on the signals of active covariates. Dealing with high-dimensional quantile regression with an unknown change point calls for a new proof technique since the quantile loss function is non-smooth and furthermore the corresponding objective function is non-convex with respect to the change point. The technique developed in this paper is applicable to a general M-estimation framework with a change point, which may be of independent interest. The proposed methods are then illustrated via Monte Carlo experiments and an application to tipping in the dynamics of racial segregation.||||@arxiv||||2016/03/01||||Oracle Estimation of a Change Point in High Dimensional Quantile Regression||||In this paper, we consider a high-dimensional quantile regression model where the sparsity structure may differ between two sub-populations. We develop $\ell_1$-penalized estimators of both...||||https://arxiv.org/abs/1603.00235v2||||econ||||
444||||None||||LASSO-Driven Inference in Time and Space||||arXiv.org||||2019/04/25||||LASSO-Driven Inference in Time and Space||||Chernozhukov, Victor || Härdle, Wolfgang K. || Huang, Chen || Wang, Weining||||https://arxiv.org/pdf/1806.05081||||1806.05081||||We consider the estimation and inference in a system of high-dimensional regression equations allowing for temporal and cross-sectional dependency in covariates and error processes, covering rather general forms of weak dependence. A sequence of regressions with many regressors using LASSO (Least Absolute Shrinkage and Selection Operator) is applied for variable selection purpose, and an overall penalty level is carefully chosen by a block multiplier bootstrap procedure to account for multiplicity of the equations and dependencies in the data. Correspondingly, oracle properties with a jointly selected tuning parameter are derived. We further provide high-quality de-biased simultaneous inference on the many target parameters of the system. We provide bootstrap consistency results of the test procedure, which are based on a general Bahadur representation for the $Z$-estimators with dependent data. Simulations demonstrate good performance of the proposed inference procedure. Finally, we apply the method to quantify spillover effects of textual sentiment indices in a financial market and to test the connectedness among sectors.||||@arxiv||||2018/06/13||||LASSO-Driven Inference in Time and Space||||We consider the estimation and inference in a system of high-dimensional regression equations allowing for temporal and cross-sectional dependency in covariates and error processes, covering...||||https://arxiv.org/abs/1806.05081v3||||econ||||
445||||None||||Adversarial Generalized Method of Moments||||arXiv.org||||2018/04/24||||Adversarial Generalized Method of Moments||||Lewis, Greg || Syrgkanis, Vasilis||||https://arxiv.org/pdf/1803.07164||||1803.07164||||We provide an approach for learning deep neural net representations of models described via conditional moment restrictions. Conditional moment restrictions are widely used, as they are the language by which social scientists describe the assumptions they make to enable causal inference. We formulate the problem of estimating the underling model as a zero-sum game between a modeler and an adversary and apply adversarial training. Our approach is similar in nature to Generative Adversarial Networks (GAN), though here the modeler is learning a representation of a function that satisfies a continuum of moment conditions and the adversary is identifying violating moments. We outline ways of constructing effective adversaries in practice, including kernels centered by k-means clustering, and random forests. We examine the practical performance of our approach in the setting of non-parametric instrumental variable regression.||||@arxiv||||2018/03/19||||Adversarial Generalized Method of Moments||||We provide an approach for learning deep neural net representations of models described via conditional moment restrictions. Conditional moment restrictions are widely used, as they are the...||||https://arxiv.org/abs/1803.07164v2||||cs||||
446||||None||||Semiparametric estimation of heterogeneous treatment effects under the nonignorable assignment condition||||arXiv.org||||2019/02/26||||Semiparametric estimation of heterogeneous treatment effects under the nonignorable assignment condition||||Takahata, Keisuke || Hoshino, Takahiro||||https://arxiv.org/pdf/1902.09978||||1902.09978||||We propose a semiparametric two-stage least square estimator for the heterogeneous treatment effects (HTE). HTE is the solution to certain integral equation which belongs to the class of Fredholm integral equations of the first kind, which is known to be ill-posed problem. Naive semi/nonparametric methods do not provide stable solution to such problems. Then we propose to approximate the function of interest by orthogonal series under the constraint which makes the inverse mapping of integral to be continuous and eliminates the ill-posedness. We illustrate the performance of the proposed estimator through simulation experiments.||||@arxiv||||2019/02/26||||Semiparametric estimation of heterogeneous treatment effects under...||||We propose a semiparametric two-stage least square estimator for the heterogeneous treatment effects (HTE). HTE is the solution to certain integral equation which belongs to the class of Fredholm...||||https://arxiv.org/abs/1902.09978v1||||econ||||
447||||None||||Determining the dimension of factor structures in non-stationary large datasets||||arXiv.org||||2018/06/10||||Determining the dimension of factor structures in non-stationary large datasets||||Barigozzi, Matteo || Trapani, Lorenzo||||https://arxiv.org/pdf/1806.03647||||1806.03647||||We propose a procedure to determine the dimension of the common factor space in a large, possibly non-stationary, dataset. Our procedure is designed to determine whether there are (and how many) common factors (i) with linear trends, (ii) with stochastic trends, (iii) with no trends, i.e. stationary. Our analysis is based on the fact that the largest eigenvalues of a suitably scaled covariance matrix of the data (corresponding to the common factor part) diverge, as the dimension $N$ of the dataset diverges, whilst the others stay bounded. Therefore, we propose a class of randomised test statistics for the null that the $p$-th eigenvalue diverges, based directly on the estimated eigenvalue. The tests only requires minimal assumptions on the data, and no restrictions on the relative rates of divergence of $N$ and $T$ are imposed. Monte Carlo evidence shows that our procedure has very good finite sample properties, clearly dominating competing approaches when no common factors are present. We illustrate our methodology through an application to US bond yields with different maturities observed over the last 30 years. A common linear trend and two common stochastic trends are found and identified as the classical level, slope and curvature factors.||||@arxiv||||2018/06/10||||Determining the dimension of factor structures in non-stationary...||||We propose a procedure to determine the dimension of the common factor space in a large, possibly non-stationary, dataset. Our procedure is designed to determine whether there are (and how many)...||||https://arxiv.org/abs/1806.03647v1||||econ||||
448||||None||||Evolution of Regional Innovation with Spatial Knowledge Spillovers: Convergence or Divergence?||||arXiv.org||||2018/03/06||||Evolution of Regional Innovation with Spatial Knowledge Spillovers: Convergence or Divergence?||||Qiu, Jinwen || Liu, Wenjian || Ning, Ning||||https://arxiv.org/pdf/1801.06936||||1801.06936||||This paper extends endogenous economic growth models to incorporate knowledge externality. We explores whether spatial knowledge spillovers among regions exist, whether spatial knowledge spillovers promote regional innovative activities, and whether external knowledge spillovers affect the evolution of regional innovations in the long run. We empirically verify the theoretical results through applying spatial statistics and econometric model in the analysis of panel data of 31 regions in China. An accurate estimate of the range of knowledge spillovers is achieved and the convergence of regional knowledge growth rate is found, with clear evidences that developing regions benefit more from external knowledge spillovers than developed regions.||||@arxiv||||2018/01/22||||Evolution of Regional Innovation with Spatial Knowledge...||||This paper extends endogenous economic growth models to incorporate knowledge externality. We explores whether spatial knowledge spillovers among regions exist, whether spatial knowledge...||||https://arxiv.org/abs/1801.06936v3||||econ||||
449||||None||||Relative Net Utility and the Saint Petersburg Paradox||||arXiv.org||||2019/10/24||||Relative Net Utility and the Saint Petersburg Paradox||||Muller, Daniel || Marwala, Tshilidzi||||https://arxiv.org/pdf/1910.09544||||1910.09544||||The famous St Petersburg Paradox shows that the theory of expected value does not capture the real-world economics of decision-making problem. Over the years, many economic theories were developed to resolve the paradox and explain the subjective utility of the expected outcomes and risk aversion. In this paper, we use the concept of the net utility to resolve the St Petersburg paradox. The reason why the principle of absolute instead of net utility does not work is because it is a first order approximation of some unknown utility function. Because the net utility concept is able to explain both behavioral economics and the St Petersburg paradox it is deemed a universal approach to handling utility. Finally, this paper explored how artificial intelligent (AI) agent will make choices and observed that if AI agent uses the nominal utility approach it will see infinite reward while if it uses the net utility approach it will see the limited reward that human beings see.||||@arxiv||||2019/10/21||||Relative Net Utility and the Saint Petersburg Paradox||||The famous St Petersburg Paradox shows that the theory of expected value does not capture the real-world economics of decision-making problem. Over the years, many economic theories were developed...||||https://arxiv.org/abs/1910.09544v2||||cs||||
450||||None||||Nonparametric Estimation and Inference in Economic and Psychological Experiments||||arXiv.org||||2019/12/06||||Nonparametric Estimation and Inference in Economic and Psychological Experiments||||Seri, Raffaello || Centorrino, Samuele || Bernasconi, Michele||||https://arxiv.org/pdf/1904.11156||||1904.11156||||The goal of this paper is to provide some tools for nonparametric estimation and inference in psychological and economic experiments. We consider an experimental framework in which each of $n$subjects provides $T$ responses to a vector of $T$ stimuli. We propose to estimate the unknown function $f$ linking stimuli to responses through a nonparametric sieve estimator. We give conditions for consistency when either $n$ or $T$ or both diverge. The rate of convergence depends upon the error covariance structure, that is allowed to differ across subjects. With these results we derive the optimal divergence rate of the dimension of the sieve basis with both $n$ and $T$. We provide guidance about the optimal balance between the number of subjects and questions in a laboratory experiment and argue that a large $n$is often better than a large $T$. We derive conditions for asymptotic normality of functionals of the estimator of $T$ and apply them to obtain the asymptotic distribution of the Wald test when the number of constraints under the null is finite and when it diverges along with other asymptotic parameters. Lastly, we investigate the previous properties when the conditional covariance matrix is replaced by an estimator.||||@arxiv||||2019/04/25||||Nonparametric Estimation and Inference in Economic and...||||The goal of this paper is to provide some tools for nonparametric estimation and inference in psychological and economic experiments. We consider an experimental framework in which each of...||||https://arxiv.org/abs/1904.11156v3||||econ||||
451||||None||||Emergent inequality and endogenous dynamics in a simple behavioral macroeconomic model||||arXiv.org||||2019/07/03||||Emergent inequality and endogenous dynamics in a simple behavioral macroeconomic model||||Asano, Yuki M. || Kolb, Jakob J. || Heitzig, Jobst || Farmer, J. Doyne||||https://arxiv.org/pdf/1907.02155||||1907.02155||||Standard macroeconomic models assume that households are rational in the sense that they are perfect utility maximizers, and explain economic dynamics in terms of shocks that drive the economy away from the stead-state. Here we build on a standard macroeconomic model in which a single rational representative household makes a savings decision of how much to consume or invest. In our model households are myopic boundedly rational heterogeneous agents embedded in a social network. From time to time each household updates its savings rate by copying the savings rate of its neighbor with the highest consumption. If the updating time is short, the economy is stuck in a poverty trap, but for longer updating times economic output approaches its optimal value, and we observe a critical transition to an economy with irregular endogenous oscillations in economic output, resembling a business cycle. In this regime households divide into two groups: Poor households with low savings rates and rich households with high savings rates. Thus inequality and economic dynamics both occur spontaneously as a consequence of imperfect household decision making. Our work here supports an alternative program of research that substitutes utility maximization for behaviorally grounded decision making.||||@arxiv||||2019/07/03||||Emergent inequality and endogenous dynamics in a simple behavioral...||||Standard macroeconomic models assume that households are rational in the sense that they are perfect utility maximizers, and explain economic dynamics in terms of shocks that drive the economy...||||https://arxiv.org/abs/1907.02155v1||||econ||||
452||||None||||Maximum Likelihood Estimation in Possibly Misspecified Dynamic Models with Time-Inhomogeneous Markov Regimes||||arXiv.org||||2018/05/10||||Maximum Likelihood Estimation in Possibly Misspecified Dynamic Models with Time-Inhomogeneous Markov Regimes||||Pouzo, Demian || Psaradakis, Zacharias || Sola, Martin||||https://arxiv.org/pdf/1612.04932||||1612.04932||||This paper considers maximum likelihood (ML) estimation in a large class of models with hidden Markov regimes. We investigate consistency and local asymptotic normality of the ML estimator under general conditions which allow for autoregressive dynamics in the observable process, time-inhomogeneous Markov regime sequences, and possible model misspecification. A Monte Carlo study examines the finite-sample properties of the ML estimator. An empirical application is also discussed.||||@arxiv||||2016/12/15||||Maximum Likelihood Estimation in Possibly Misspecified Dynamic...||||This paper considers maximum likelihood (ML) estimation in a large class of models with hidden Markov regimes. We investigate consistency and local asymptotic normality of the ML estimator under...||||https://arxiv.org/abs/1612.04932v2||||econ||||
453||||None||||Behavioral and Game-Theoretic Security Investments in Interdependent Systems Modeled by Attack Graphs||||arXiv.org||||2020/01/09||||Behavioral and Game-Theoretic Security Investments in Interdependent Systems Modeled by Attack Graphs||||Abdallah, Mustafa || Naghizadeh, Parinaz || Hota, Ashish R. || Cason, Timothy || Bagchi, Saurabh || Sundaram, Shreyas||||https://arxiv.org/pdf/2001.03213||||2001.03213||||We consider a system consisting of multiple interdependent assets, and a set of defenders, each responsible for securing a subset of the assets against an attacker. The interdependencies between the assets are captured by an attack graph, where an edge from one asset to another indicates that if the former asset is compromised, an attack can be launched on the latter asset. Each edge has an associated probability of successful attack, which can be reduced via security investments by the defenders. In such scenarios, we investigate the security investments that arise under certain features of human decision-making that have been identified in behavioral economics. In particular, humans have been shown to perceive probabilities in a nonlinear manner, typically overweighting low probabilities and underweighting high probabilities. We show that suboptimal investments can arise under such weighting in certain network topologies. We also show that pure strategy Nash equilibria exist in settings with multiple (behavioral) defenders, and study the inefficiency of the equilibrium investments by behavioral defenders compared to a centralized socially optimal solution.||||@arxiv||||2020/01/09||||Behavioral and Game-Theoretic Security Investments in...||||We consider a system consisting of multiple interdependent assets, and a set of defenders, each responsible for securing a subset of the assets against an attacker. The interdependencies between...||||https://arxiv.org/abs/2001.03213v1||||cs||||
454||||None||||Identification with Latent Choice Sets||||arXiv.org||||2019/08/08||||Identification with Latent Choice Sets||||Kamat, Vishal||||https://arxiv.org/pdf/1711.02048||||1711.02048||||In a common experimental format, individuals are randomly assigned to either a treatment group with access to a program or a control group without access. In such experiments, analyzing the average effects of the treatment of program access may be hindered by the problem that some control individuals do not comply with their assigned status and receive program access from outside the experiment. Available tools to account for such a problem typically require the researcher to observe the receipt of program access for every individual. However, in many experiments, this is not the case as data is not collected on where any individual received access. In this paper, I develop a framework to show how data on only each individual's treatment assignment status, program participation decision and outcome can be exploited to learn about the average effects of program access. I propose a nonparametric selection model with latent choice sets to relate where access was received to the treatment assignment status, participation decision and outcome, and a linear programming procedure to compute the identified set for parameters evaluating the average effects of program access in this model. I illustrate the framework by analyzing the average effects of Head Start preschool access using the Head Start Impact Study. I find that the provision of Head Start access induces parents to enroll their child into Head Start and also positively impacts test scores, and that these effects heterogeneously depend on the availability of access to an alternative preschool.||||@arxiv||||2017/11/06||||Identification with Latent Choice Sets||||In a common experimental format, individuals are randomly assigned to either a treatment group with access to a program or a control group without access. In such experiments, analyzing the...||||https://arxiv.org/abs/1711.02048v3||||econ||||
455||||None||||Testing for observation-dependent regime switching in mixture autoregressive models||||arXiv.org||||2017/11/10||||Testing for observation-dependent regime switching in mixture autoregressive models||||Meitz, Mika || Saikkonen, Pentti||||https://arxiv.org/pdf/1711.03959||||1711.03959||||Testing for regime switching when the regime switching probabilities are specified either as constants (`mixture models') or are governed by a finite-state Markov chain (`Markov switching models') are long-standing problems that have also attracted recent interest. This paper considers testing for regime switching when the regime switching probabilities are time-varying and depend on observed data (`observation-dependent regime switching'). Specifically, we consider the likelihood ratio test for observation-dependent regime switching in mixture autoregressive models. The testing problem is highly nonstandard, involving unidentified nuisance parameters under the null, parameters on the boundary, singular information matrices, and higher-order approximations of the log-likelihood. We derive the asymptotic null distribution of the likelihood ratio test statistic in a general mixture autoregressive setting using high-level conditions that allow for various forms of dependence of the regime switching probabilities on past observations, and we illustrate the theory using two particular mixture autoregressive models. The likelihood ratio test has a nonstandard asymptotic distribution that can easily be simulated, and Monte Carlo studies show the test to have satisfactory finite sample size and power properties.||||@arxiv||||2017/11/10||||Testing for observation-dependent regime switching in mixture...||||Testing for regime switching when the regime switching probabilities are specified either as constants (`mixture models') or are governed by a finite-state Markov chain (`Markov switching models')...||||https://arxiv.org/abs/1711.03959v1||||econ||||
456||||None||||Deriving the factor endowment--commodity output relationship for Thailand (1920-1927) using a three-factor two-good general equilibrium trade model||||arXiv.org||||2018/10/11||||Deriving the factor endowment--commodity output relationship for Thailand (1920-1927) using a three-factor two-good general equilibrium trade model||||Nakada, Yoshiaki||||https://arxiv.org/pdf/1810.04819||||1810.04819||||Feeny (1982, pp. 26-28) referred to a three-factor two-good general equilibrium trade model, when he explained the relative importance of trade and factor endowments in Thailand 1880-1940. For example, Feeny (1982) stated that the growth in labor stock would be responsible for a substantial increase in rice output relative to textile output. Is Feeny's statement plausible? The purpose of this paper is to derive the Rybczynski sign patterns, which express the factor endowment--commodity output relationship, for Thailand during the period 1920 to 1927 using the EWS (economy-wide substitution)-ratio vector. A 'strong Rybczynski result' necessarily holds. I derived three Rybczynski sign patterns. However, a more detailed estimate allowed a reduction from three candidates to two. I restrict the analysis to the period 1920-1927 because of data availability. The results imply that Feeny's statement might not necessarily hold. Hence, labor stock might not affect the share of exportable sector in national income positively. Moreover, the percentage of Chinese immigration in the total population growth was not as large as expected. This study will be useful when simulating real wage in Thailand.||||@arxiv||||2018/10/11||||Deriving the factor endowment--commodity output relationship for...||||Feeny (1982, pp. 26-28) referred to a three-factor two-good general equilibrium trade model, when he explained the relative importance of trade and factor endowments in Thailand 1880-1940. For...||||https://arxiv.org/abs/1810.04819v1||||econ||||
457||||None||||LASSO Methods for Gaussian Instrumental Variables Models||||arXiv.org||||2011/02/23||||LASSO Methods for Gaussian Instrumental Variables Models||||Belloni, Alexandre || Chernozhukov, Victor || Hansen, Christian||||https://arxiv.org/pdf/1012.1297||||1012.1297||||In this note, we propose to use sparse methods (e.g. LASSO, Post-LASSO, sqrt-LASSO, and Post-sqrt-LASSO) to form first-stage predictions and estimate optimal instruments in linear instrumental variables (IV) models with many instruments in the canonical Gaussian case. The methods apply even when the number of instruments is much larger than the sample size. We derive asymptotic distributions for the resulting IV estimators and provide conditions under which these sparsity-based IV estimators are asymptotically oracle-efficient. In simulation experiments, a sparsity-based IV estimator with a data-driven penalty performs well compared to recently advocated many-instrument-robust procedures. We illustrate the procedure in an empirical example using the Angrist and Krueger (1991) schooling data.||||@arxiv||||2010/12/06||||LASSO Methods for Gaussian Instrumental Variables Models||||In this note, we propose to use sparse methods (e.g. LASSO, Post-LASSO, sqrt-LASSO, and Post-sqrt-LASSO) to form first-stage predictions and estimate optimal instruments in linear instrumental...||||https://arxiv.org/abs/1012.1297v2||||econ||||
458||||None||||Testing for unobserved heterogeneous treatment effects in a nonseparable model with endogenous selection||||arXiv.org||||2018/03/20||||Testing for unobserved heterogeneous treatment effects in a nonseparable model with endogenous selection||||Hsu, Yu-Chin || Huang, Ta-Cheng || Xu, Haiqing||||https://arxiv.org/pdf/1803.07514||||1803.07514||||Unobserved heterogeneous treatment effects have been emphasized in the policy evaluation literature. This paper proposes a nonparametric test for unobserved heterogeneous treatment effects in a general framework, allowing for self-selection to the treatment. The proposed modified Kolmogorov-Smirnov-type test is consistent and simple to implement. Monte Carlo simulations show that our test performs well in finite samples. For illustration, we apply our test to study heterogeneous treatment effects of the Job Training Partnership Act on earnings and the impacts of fertility on family income.||||@arxiv||||2018/03/20||||Testing for unobserved heterogeneous treatment effects in a...||||Unobserved heterogeneous treatment effects have been emphasized in the policy evaluation literature. This paper proposes a nonparametric test for unobserved heterogeneous treatment effects in a...||||https://arxiv.org/abs/1803.07514v1||||econ||||
459||||None||||Spectral inference for large Stochastic Blockmodels with nodal covariates||||arXiv.org||||2019/08/18||||Spectral inference for large Stochastic Blockmodels with nodal covariates||||Mele, Angelo || Hao, Lingxin || Cape, Joshua || Priebe, Carey E.||||https://arxiv.org/pdf/1908.06438||||1908.06438||||In many applications of network analysis, it is important to distinguish between observed and unobserved factors affecting network structure. To this end, we develop spectral estimators for both unobserved blocks and the effect of covariates in stochastic blockmodels. Our main strategy is to reformulate the stochastic blockmodel estimation problem as recovery of latent positions in a generalized random dot product graph. On the theoretical side, we establish asymptotic normality of our estimators for the subsequent purpose of performing inference. On the applied side, we show that computing our estimator is much faster than standard variational expectation--maximization algorithms and scales well for large networks. The results in this paper provide a foundation to estimate the effect of observed covariates as well as unobserved latent community structure on the probability of link formation in networks.||||@arxiv||||2019/08/18||||Spectral inference for large Stochastic Blockmodels with nodal covariates||||In many applications of network analysis, it is important to distinguish between observed and unobserved factors affecting network structure. To this end, we develop spectral estimators for both...||||https://arxiv.org/abs/1908.06438v1||||econ||||
460||||None||||Shapley-like values without symmetry||||arXiv.org||||2019/05/10||||Shapley-like values without symmetry||||Clark, Jacob North || Montgomery-Smith, Stephen||||https://arxiv.org/pdf/1809.07747||||1809.07747||||Following the work of Lloyd Shapley on the Shapley value, and tangentially the work of Guillermo Owen, we offer an alternative non-probabilistic formulation of part of the work of Robert J. Weber in his 1978 paper "Probabilistic values for games." Specifically, we focus upon efficient but not symmetric allocations of value for cooperative games. We retain standard efficiency and linearity, and offer an alternative condition, "reasonableness," to replace the other usual axioms. In the pursuit of the result, we discover properties of the linear maps that describe the allocations. This culminates in a special class of games for which any other map that is "reasonable, efficient" can be written as a convex combination of members of this special class of allocations, via an application of the Krein-Milman theorem.||||@arxiv||||2018/09/20||||Shapley-like values without symmetry||||Following the work of Lloyd Shapley on the Shapley value, and tangentially the work of Guillermo Owen, we offer an alternative non-probabilistic formulation of part of the work of Robert J. Weber...||||https://arxiv.org/abs/1809.07747v2||||econ||||
461||||None||||Markov Chain Models of Refugee Migration Data||||arXiv.org||||2019/03/19||||Markov Chain Models of Refugee Migration Data||||Huang, Vincent || Unwin, James||||https://arxiv.org/pdf/1903.08255||||1903.08255||||The application of Markov chains to modelling refugee crises is explored, focusing on local migration of individuals at the level of cities and days. As an explicit example we apply the Markov chains migration model developed here to UNHCR data on the Burundi refugee crisis. We compare our method to a state-of-the-art `agent-based' model of Burundi refugee movements, and highlight that Markov chain approaches presented here can improve the match to data while simultaneously being more algorithmically efficient.||||@arxiv||||2019/03/19||||Markov Chain Models of Refugee Migration Data||||The application of Markov chains to modelling refugee crises is explored, focusing on local migration of individuals at the level of cities and days. As an explicit example we apply the Markov...||||https://arxiv.org/abs/1903.08255v1||||cs||||
462||||None||||Computational Socioeconomics||||arXiv.org||||2019/05/15||||Computational Socioeconomics||||Gao, Jian || Zhang, Yi-Cheng || Zhou, Tao||||https://arxiv.org/pdf/1905.06166||||1905.06166||||Uncovering the structure of socioeconomic systems and timely estimation of socioeconomic status are significant for economic development. The understanding of socioeconomic processes provides foundations to quantify global economic development, to map regional industrial structure, and to infer individual socioeconomic status. In this review, we will make a brief manifesto about a new interdisciplinary research field named Computational Socioeconomics, followed by detailed introduction about data resources, computational tools, data-driven methods, theoretical models and novel applications at multiple resolutions, including the quantification of global economic inequality and complexity, the map of regional industrial structure and urban perception, the estimation of individual socioeconomic status and demographic, and the real-time monitoring of emergent events. This review, together with pioneering works we have highlighted, will draw increasing interdisciplinary attentions and induce a methodological shift in future socioeconomic studies.||||@arxiv||||2019/05/15||||Computational Socioeconomics||||Uncovering the structure of socioeconomic systems and timely estimation of socioeconomic status are significant for economic development. The understanding of socioeconomic processes provides...||||https://arxiv.org/abs/1905.06166v1||||econ||||
463||||None||||Asymptotic Behavior of Bayesian Learners with Misspecified Models||||arXiv.org||||2019/10/23||||Asymptotic Behavior of Bayesian Learners with Misspecified Models||||Esponda, Ignacio || Pouzo, Demian || Yamamoto, Yuichi||||https://arxiv.org/pdf/1904.08551||||1904.08551||||We consider an agent who represents uncertainty about the environment via a possibly misspecified model. Each period, the agent takes an action, observes a consequence, and uses Bayes' rule to update her belief about the environment. This framework has become increasingly popular in economics to study behavior driven by incorrect or biased beliefs. Current literature has characterized asymptotic behavior under fairly specific assumptions. By first showing that the key element to predict the agent's behavior is the frequency of her past actions, we are able to characterize asymptotic behavior in general settings in terms of the solutions of a generalization of a differential equation that describes the evolution of the frequency of actions. We then present a series of implications that can be readily applied to economic applications, thus providing off-the-shelf tools that can be used to characterize behavior under misspecified learning.||||@arxiv||||2019/04/18||||Asymptotic Behavior of Bayesian Learners with Misspecified Models||||We consider an agent who represents uncertainty about the environment via a possibly misspecified model. Each period, the agent takes an action, observes a consequence, and uses Bayes' rule to...||||https://arxiv.org/abs/1904.08551v2||||econ||||
464||||None||||Why Finnish polytechnics reject top applicants||||arXiv.org||||2019/08/15||||Why Finnish polytechnics reject top applicants||||Koerselman, Kristian||||https://arxiv.org/pdf/1908.05443||||1908.05443||||I use a panel of higher education clearinghouse data to study the centralized assignment of applicants to Finnish polytechnics. I show that on a yearly basis, large numbers of top applicants unnecessarily remain unassigned to any program. There are programs which rejected applicants would find acceptable, but the assignment mechanism both discourages applicants from applying, and stops programs from admitting those who do. A mechanism which would admit each year's most eligible applicants has the potential to substantially reduce re-applications, thereby shortening the long queues into Finnish higher education.||||@arxiv||||2019/08/15||||Why Finnish polytechnics reject top applicants||||I use a panel of higher education clearinghouse data to study the centralized assignment of applicants to Finnish polytechnics. I show that on a yearly basis, large numbers of top applicants...||||https://arxiv.org/abs/1908.05443v1||||econ||||
465||||None||||Strategy-Proof and Non-Wasteful Multi-Unit Auction via Social Network||||arXiv.org||||2019/11/20||||Strategy-Proof and Non-Wasteful Multi-Unit Auction via Social Network||||Kawasaki, Takehiro || Barrot, Nathanael || Takanashi, Seiji || Todo, Taiki || Yokoo, Makoto||||https://arxiv.org/pdf/1911.08809||||1911.08809||||Auctions via social network, pioneered by Li et al. (2017), have been attracting considerable attention in the literature of mechanism design for auctions. However, no known mechanism has satisfied strategy-proofness, non-deficit, non-wastefulness, and individual rationality for the multi-unit unit-demand auction, except for some naive ones. In this paper, we first propose a mechanism that satisfies all the above properties. We then make a comprehensive comparison with two naive mechanisms, showing that the proposed mechanism dominates them in social surplus, seller's revenue, and incentive of buyers for truth-telling. We also analyze the characteristics of the social surplus and the revenue achieved by the proposed mechanism, including the constant approximability of the worst-case efficiency loss and the complexity of optimizing revenue from the seller's perspective.||||@arxiv||||2019/11/20||||Strategy-Proof and Non-Wasteful Multi-Unit Auction via Social Network||||Auctions via social network, pioneered by Li et al. (2017), have been attracting considerable attention in the literature of mechanism design for auctions. However, no known mechanism has...||||https://arxiv.org/abs/1911.08809v1||||cs||||
466||||None||||A Comparison of Economic Agent-Based Model Calibration Methods||||arXiv.org||||2019/02/15||||A Comparison of Economic Agent-Based Model Calibration Methods||||Platt, Donovan||||https://arxiv.org/pdf/1902.05938||||1902.05938||||Interest in agent-based models of financial markets and the wider economy has increased consistently over the last few decades, in no small part due to their ability to reproduce a number of empirically-observed stylised facts that are not easily recovered by more traditional modelling approaches. Nevertheless, the agent-based modelling paradigm faces mounting criticism, focused particularly on the rigour of current validation and calibration practices, most of which remain qualitative and stylised fact-driven. While the literature on quantitative and data-driven approaches has seen significant expansion in recent years, most studies have focused on the introduction of new calibration methods that are neither benchmarked against existing alternatives nor rigorously tested in terms of the quality of the estimates they produce. We therefore compare a number of prominent ABM calibration methods, both established and novel, through a series of computational experiments in an attempt to determine the respective strengths and weaknesses of each approach and the overall quality of the resultant parameter estimates. We find that Bayesian estimation, though less popular in the literature, consistently outperforms frequentist, objective function-based approaches and results in reasonable parameter estimates in many contexts. Despite this, we also find that agent-based model calibration techniques require further development in order to definitively calibrate large-scale models. We therefore make suggestions for future research.||||@arxiv||||2019/02/15||||A Comparison of Economic Agent-Based Model Calibration Methods||||Interest in agent-based models of financial markets and the wider economy has increased consistently over the last few decades, in no small part due to their ability to reproduce a number of...||||https://arxiv.org/abs/1902.05938v1||||econ||||
467||||None||||Climate Policy under Spatial Heat Transport: Cooperative and Noncooperative Regional Outcomes||||arXiv.org||||2019/09/09||||Climate Policy under Spatial Heat Transport: Cooperative and Noncooperative Regional Outcomes||||Cai, Yongyang || Brock, William || Xepapadeas, Anastasios || Judd, Kenneth||||https://arxiv.org/pdf/1909.04009||||1909.04009||||We build a novel stochastic dynamic regional integrated assessment model (IAM) of the climate and economic system including a number of important climate science elements that are missing in most IAMs. These elements are spatial heat transport from the Equator to the Poles, sea level rise, permafrost thaw and tipping points. We study optimal policies under cooperation and noncooperation between two regions (the North and the Tropic-South) in the face of risks and recursive utility. We introduce a new general computational algorithm to find feedback Nash equilibrium. Our results suggest that when the elements of climate science are ignored, important policy variables such as the optimal regional carbon tax and adaptation could be seriously biased. We also find the regional carbon tax is significantly smaller in the feedback Nash equilibrium than in the social planner's problem in each region, and the North has higher carbon taxes than the Tropic-South.||||@arxiv||||2019/09/09||||Climate Policy under Spatial Heat Transport: Cooperative and...||||We build a novel stochastic dynamic regional integrated assessment model (IAM) of the climate and economic system including a number of important climate science elements that are missing in most...||||https://arxiv.org/abs/1909.04009v1||||econ||||
468||||None||||Interactive coin offerings||||arXiv.org||||2019/08/12||||Interactive coin offerings||||Teutsch, Jason || Buterin, Vitalik || Brown, Christopher||||https://arxiv.org/pdf/1908.04295||||1908.04295||||Ethereum has emerged as a dynamic platform for exchanging cryptocurrency tokens. While token crowdsales cannot simultaneously guarantee buyers both certainty of valuation and certainty of participation, we show that if each token buyer specifies a desired purchase quantity at each valuation then everyone can successfully participate. Our implementation introduces smart contract techniques which recruit outside participants in order to circumvent computational complexity barriers.||||@arxiv||||2019/08/12||||Interactive coin offerings||||Ethereum has emerged as a dynamic platform for exchanging cryptocurrency tokens. While token crowdsales cannot simultaneously guarantee buyers both certainty of valuation and certainty of...||||https://arxiv.org/abs/1908.04295v1||||cs||||
469||||None||||High Dimensional Classification through $\ell_0$-Penalized Empirical Risk Minimization||||arXiv.org||||2018/11/23||||High Dimensional Classification through $\ell_0$-Penalized Empirical Risk Minimization||||Chen, Le-Yu || Lee, Sokbae||||https://arxiv.org/pdf/1811.09540||||1811.09540||||We consider a high dimensional binary classification problem and construct a classification procedure by minimizing the empirical misclassification risk with a penalty on the number of selected features. We derive non-asymptotic probability bounds on the estimated sparsity as well as on the excess misclassification risk. In particular, we show that our method yields a sparse solution whose l0-norm can be arbitrarily close to true sparsity with high probability and obtain the rates of convergence for the excess misclassification risk. The proposed procedure is implemented via the method of mixed integer linear programming. Its numerical performance is illustrated in Monte Carlo experiments.||||@arxiv||||2018/11/23||||High Dimensional Classification through $\ell_0$-Penalized...||||We consider a high dimensional binary classification problem and construct a classification procedure by minimizing the empirical misclassification risk with a penalty on the number of selected...||||https://arxiv.org/abs/1811.09540v1||||econ||||
470||||None||||Augmented Factor Models with Applications to Validating Market Risk Factors and Forecasting Bond Risk Premia||||arXiv.org||||2018/09/17||||Augmented Factor Models with Applications to Validating Market Risk Factors and Forecasting Bond Risk Premia||||Fan, Jianqing || Ke, Yuan || Liao, Yuan||||https://arxiv.org/pdf/1603.07041||||1603.07041||||We study factor models augmented by observed covariates that have explanatory powers on the unknown factors. In financial factor models, the unknown factors can be reasonably well explained by a few observable proxies, such as the Fama-French factors. In diffusion index forecasts, identified factors are strongly related to several directly measurable economic variables such as consumption-wealth variable, financial ratios, and term spread. With those covariates, both the factors and loadings are identifiable up to a rotation matrix even only with a finite dimension. To incorporate the explanatory power of these covariates, we propose a smoothed principal component analysis (PCA): (i) regress the data onto the observed covariates, and (ii) take the principal components of the fitted data to estimate the loadings and factors. This allows us to accurately estimate the percentage of both explained and unexplained components in factors and thus to assess the explanatory power of covariates. We show that both the estimated factors and loadings can be estimated with improved rates of convergence compared to the benchmark method. The degree of improvement depends on the strength of the signals, representing the explanatory power of the covariates on the factors. The proposed estimator is robust to possibly heavy-tailed distributions. We apply the model to forecast US bond risk premia, and find that the observed macroeconomic characteristics contain strong explanatory powers of the factors. The gain of forecast is more substantial when the characteristics are incorporated to estimate the common factors than directly used for forecasts.||||@arxiv||||2016/03/23||||Augmented Factor Models with Applications to Validating Market...||||We study factor models augmented by observed covariates that have explanatory powers on the unknown factors. In financial factor models, the unknown factors can be reasonably well explained by a...||||https://arxiv.org/abs/1603.07041v2||||econ||||
471||||None||||Transaction Costs in Collective Waste Recovery Systems in the EU||||arXiv.org||||2018/04/18||||Transaction Costs in Collective Waste Recovery Systems in the EU||||Nozharov, Shteryo||||https://arxiv.org/pdf/1804.06792||||1804.06792||||The study aims to identify the institutional flaws of the current EU waste management model by analysing the economic model of extended producer responsibility and collective waste management systems and to create a model for measuring the transaction costs borne by waste recovery organizations. The model was approbated by analysing the Bulgarian collective waste management systems that have been complying with the EU legislation for the last 10 years. The analysis focuses on waste oils because of their economic importance and the limited number of studies and analyses in this field as the predominant body of research to date has mainly addressed packaging waste, mixed household waste or discarded electrical and electronic equipment. The study aims to support the process of establishing a circular economy in the EU, which was initiated in 2015.||||@arxiv||||2018/04/18||||Transaction Costs in Collective Waste Recovery Systems in the EU||||The study aims to identify the institutional flaws of the current EU waste management model by analysing the economic model of extended producer responsibility and collective waste management...||||https://arxiv.org/abs/1804.06792v1||||econ||||
472||||None||||Climate Change and Agriculture: Subsistence Farmers' Response to Extreme Heat||||arXiv.org||||2019/02/28||||Climate Change and Agriculture: Subsistence Farmers' Response to Extreme Heat||||Aragón, Fernando M. || Oteiza, Francisco || Rud, Juan Pablo||||https://arxiv.org/pdf/1902.09204||||1902.09204||||This paper examines how subsistence farmers respond to extreme heat. Using micro-data from Peruvian households, we find that high temperatures reduce agricultural productivity, increase area planted, and change crop mix. These findings are consistent with farmers using input adjustments as a short-term mechanism to attenuate the effect of extreme heat on output. This response seems to complement other coping strategies, such as selling livestock, but exacerbates the drop in yields, a standard measure of agricultural productivity. Using our estimates, we show that accounting for land adjustments is important to quantify damages associated with climate change.||||@arxiv||||2019/02/25||||Climate Change and Agriculture: Subsistence Farmers' Response...||||This paper examines how subsistence farmers respond to extreme heat. Using micro-data from Peruvian households, we find that high temperatures reduce agricultural productivity, increase area...||||https://arxiv.org/abs/1902.09204v2||||econ||||
473||||None||||Variational Bayes Estimation of Discrete-Margined Copula Models with Application to Time Series||||arXiv.org||||2018/07/20||||Variational Bayes Estimation of Discrete-Margined Copula Models with Application to Time Series||||Loaiza-Maya, Ruben || Smith, Michael Stanley||||https://arxiv.org/pdf/1712.09150||||1712.09150||||We propose a new variational Bayes estimator for high-dimensional copulas with discrete, or a combination of discrete and continuous, margins. The method is based on a variational approximation to a tractable augmented posterior, and is faster than previous likelihood-based approaches. We use it to estimate drawable vine copulas for univariate and multivariate Markov ordinal and mixed time series. These have dimension $rT$, where $T$ is the number of observations and $r$ is the number of series, and are difficult to estimate using previous methods. The vine pair-copulas are carefully selected to allow for heteroskedasticity, which is a feature of most ordinal time series data. When combined with flexible margins, the resulting time series models also allow for other common features of ordinal data, such as zero inflation, multiple modes and under- or over-dispersion. Using six example series, we illustrate both the flexibility of the time series copula models, and the efficacy of the variational Bayes estimator for copulas of up to 792 dimensions and 60 parameters. This far exceeds the size and complexity of copula models for discrete data that can be estimated using previous methods.||||@arxiv||||2017/12/26||||Variational Bayes Estimation of Discrete-Margined Copula Models...||||We propose a new variational Bayes estimator for high-dimensional copulas with discrete, or a combination of discrete and continuous, margins. The method is based on a variational approximation to...||||https://arxiv.org/abs/1712.09150v2||||econ||||
474||||None||||Mean Field Equilibrium: Uniqueness, Existence, and Comparative Statics||||arXiv.org||||2019/10/30||||Mean Field Equilibrium: Uniqueness, Existence, and Comparative Statics||||Light, Bar || Weintraub, Gabriel||||https://arxiv.org/pdf/1903.02273||||1903.02273||||The standard solution concept for stochastic games is Markov perfect equilibrium (MPE); however, its computation becomes intractable as the number of players increases. Instead, we consider mean field equilibrium (MFE) that has been popularized in the recent literature. MFE takes advantage of averaging effects in models with a large number of players. We make three main contributions. First, our main result provides conditions that ensure the uniqueness of an MFE. We believe this uniqueness result is the first of its nature in the class of models we study. Second, we generalize previous MFE existence results. Third, we provide general comparative statics results. We apply our results to dynamic oligopoly models and to heterogeneous agent macroeconomic models commonly used in previous work in economics and operations.||||@arxiv||||2019/03/06||||Mean Field Equilibrium: Uniqueness, Existence, and Comparative Statics||||The standard solution concept for stochastic games is Markov perfect equilibrium (MPE); however, its computation becomes intractable as the number of players increases. Instead, we consider mean...||||https://arxiv.org/abs/1903.02273v2||||cs||||
475||||None||||The Likelihood of Mixed Hitting Times||||arXiv.org||||2019/05/09||||The Likelihood of Mixed Hitting Times||||Abbring, Jaap H. || Salimans, Tim||||https://arxiv.org/pdf/1905.03463||||1905.03463||||We present a method for computing the likelihood of a mixed hitting-time model that specifies durations as the first time a latent Lévy process crosses a heterogeneous threshold. This likelihood is not generally known in closed form, but its Laplace transform is. Our approach to its computation relies on numerical methods for inverting Laplace transforms that exploit special properties of the first passage times of Lévy processes. We use our method to implement a maximum likelihood estimator of the mixed hitting-time model in MATLAB. We illustrate the application of this estimator with an analysis of Kennan's (1985) strike data.||||@arxiv||||2019/05/09||||The Likelihood of Mixed Hitting Times||||We present a method for computing the likelihood of a mixed hitting-time model that specifies durations as the first time a latent Lévy process crosses a heterogeneous threshold. This likelihood...||||https://arxiv.org/abs/1905.03463v1||||econ||||
476||||None||||The Effect of Partisanship and Political Advertising on Close Family Ties||||arXiv.org||||2018/06/03||||The Effect of Partisanship and Political Advertising on Close Family Ties||||Chen, M. Keith || Rohla, Ryne||||https://arxiv.org/pdf/1711.10602||||1711.10602||||Research on growing American political polarization and antipathy primarily studies public institutions and political processes, ignoring private effects including strained family ties. Using anonymized smartphone-location data and precinct-level voting, we show that Thanksgiving dinners attended by opposing-party precinct residents were 30-50 minutes shorter than same-party dinners. This decline from a mean of 257 minutes survives extensive spatial and demographic controls. Dinner reductions in 2016 tripled for travelers from media markets with heavy political advertising --- an effect not observed in 2015 --- implying a relationship to election-related behavior. Effects appear asymmetric: while fewer Democratic-precinct residents traveled in 2016 than 2015, political differences shortened Thanksgiving dinners more among Republican-precinct residents. Nationwide, 34 million person-hours of cross-partisan Thanksgiving discourse were lost in 2016 to partisan effects.||||@arxiv||||2017/11/28||||The Effect of Partisanship and Political Advertising on Close Family Ties||||Research on growing American political polarization and antipathy primarily studies public institutions and political processes, ignoring private effects including strained family ties. Using...||||https://arxiv.org/abs/1711.10602v2||||econ||||
477||||None||||Generational political dynamics of retirement pensions systems: An agent based model||||arXiv.org||||2019/09/13||||Generational political dynamics of retirement pensions systems: An agent based model||||Bacelar, Sérgio || Antunes, Luis||||https://arxiv.org/pdf/1909.08706||||1909.08706||||The increasing difficulties in financing the welfare state and in particular public retirement pensions have been one of the outcomes both of the decrease of fertility and birth rates combined with the increase of life expectancy. The dynamics of retirement pensions are usually studied in Economics using overlapping generation models. These models are based on simplifying assumptions like the use of a representative agent to ease the problem of tractability. Alternatively, we propose to use agent-based modelling (ABM), relaxing the need for those assumptions and enabling the use of interacting and heterogeneous agents assigning special importance to the study of inter-generational relations. We treat pension dynamics both in economics and political perspectives. The model we build, following the ODD protocol, will try to understand the dynamics of choice of public versus private retirement pensions resulting from the conflicting preferences of different agents but also from the cooperation between them. The aggregation of these individual preferences is done by voting. We combine a microsimulation approach following the evolution of synthetic populations along time, with the ABM approach studying the interactions between the different agent types. Our objective is to depict the conditions for the survival of the public pensions system emerging from the relation between egoistic and altruistic individual and collective behaviours.||||@arxiv||||2019/09/13||||Generational political dynamics of retirement pensions systems: An...||||The increasing difficulties in financing the welfare state and in particular public retirement pensions have been one of the outcomes both of the decrease of fertility and birth rates combined...||||https://arxiv.org/abs/1909.08706v1||||econ||||
478||||None||||Multitask Learning Deep Neural Networks to Combine Revealed and Stated Preference Data||||arXiv.org||||2019/08/22||||Multitask Learning Deep Neural Networks to Combine Revealed and Stated Preference Data||||Wang, Shenhao || Wang, Qingyi || Zhao, Jinhua||||https://arxiv.org/pdf/1901.00227||||1901.00227||||It is an enduring question how to combine revealed preference (RP) and stated preference (SP) data to analyze travel behavior. This study presents a framework of multitask learning deep neural networks (MTLDNNs) for this question, and demonstrates that MTLDNNs are more generic than the traditional nested logit (NL) method, due to its capacity of automatic feature learning and soft constraints. About 1,500 MTLDNN models are designed and applied to the survey data that was collected in Singapore and focused on the RP of four current travel modes and the SP with autonomous vehicles (AV) as the one new travel mode in addition to those in RP. We found that MTLDNNs consistently outperform six benchmark models and particularly the classical NL models by about 5% prediction accuracy in both RP and SP datasets. This performance improvement can be mainly attributed to the soft constraints specific to MTLDNNs, including its innovative architectural design and regularization methods, but not much to the generic capacity of automatic feature learning endowed by a standard feedforward DNN architecture. Besides prediction, MTLDNNs are also interpretable. The empirical results show that AV is mainly the substitute of driving and AV alternative-specific variables are more important than the socio-economic variables in determining AV adoption. Overall, this study introduces a new MTLDNN framework to combine RP and SP, and demonstrates its theoretical flexibility and empirical power for prediction and interpretation. Future studies can design new MTLDNN architectures to reflect the speciality of RP and SP and extend this work to other behavioral analysis.||||@arxiv||||2019/01/02||||Multitask Learning Deep Neural Networks to Combine Revealed and...||||It is an enduring question how to combine revealed preference (RP) and stated preference (SP) data to analyze travel behavior. This study presents a framework of multitask learning deep neural...||||https://arxiv.org/abs/1901.00227v2||||cs||||
479||||None||||Objective Social Choice: Using Auxiliary Information to Improve Voting Outcomes||||arXiv.org||||2020/01/27||||Objective Social Choice: Using Auxiliary Information to Improve Voting Outcomes||||Pitis, Silviu || Zhang, Michael R.||||https://arxiv.org/pdf/2001.10092||||2001.10092||||How should one combine noisy information from diverse sources to make an inference about an objective ground truth? This frequently recurring, normative question lies at the core of statistics, machine learning, policy-making, and everyday life. It has been called "combining forecasts", "meta-analysis", "ensembling", and the "MLE approach to voting", among other names. Past studies typically assume that noisy votes are identically and independently distributed (i.i.d.), but this assumption is often unrealistic. Instead, we assume that votes are independent but not necessarily identically distributed and that our ensembling algorithm has access to certain auxiliary information related to the underlying model governing the noise in each vote. In our present work, we: (1) define our problem and argue that it reflects common and socially relevant real world scenarios, (2) propose a multi-arm bandit noise model and count-based auxiliary information set, (3) derive maximum likelihood aggregation rules for ranked and cardinal votes under our noise model, (4) propose, alternatively, to learn an aggregation rule using an order-invariant neural network, and (5) empirically compare our rules to common voting rules and naive experience-weighted modifications. We find that our rules successfully use auxiliary information to outperform the naive baselines.||||@arxiv||||2020/01/27||||Objective Social Choice: Using Auxiliary Information to Improve...||||How should one combine noisy information from diverse sources to make an inference about an objective ground truth? This frequently recurring, normative question lies at the core of statistics,...||||https://arxiv.org/abs/2001.10092v1||||cs||||
480||||None||||The "power" dimension in a process of exchange||||arXiv.org||||2019/11/12||||The "power" dimension in a process of exchange||||Banterle, Alberto||||https://arxiv.org/pdf/1809.08293||||1809.08293||||The field of study of this paper is the analysis of the exchange between two subjects. Circumscribed to the micro dimension, it is however expanded with respect to standard economic theory by introducing both the dimension of power and the motivation to exchange. The basic reference is made by the reflections of those economists, preeminently John Kenneth Galbraith, who criticize the removal from the neoclassical economy of the "power" dimension. We have also referred to the criticism that Galbraith, among others, makes to the assumption of neoclassical economists that the "motivation" in exchanges is solely linked to the reward, to the money obtained in the exchange. We have got around the problem of having a large number of types of power and also a large number of forms of motivation by directly taking into account the effects on the welfare of each subject, regardless of the means with which they are achieved: that is, referring to everything that happens in the negotiation process to the potential or real variations of the welfare function induced in each subject due to the exercise of the specific form of power, on a case by case basis, and of the intensity of the motivation to perform the exchange. In the construction of a mathematical model we paid great attention to its usability in field testing.||||@arxiv||||2018/09/21||||The "power" dimension in a process of exchange||||The field of study of this paper is the analysis of the exchange between two subjects. Circumscribed to the micro dimension, it is however expanded with respect to standard economic theory by...||||https://arxiv.org/abs/1809.08293v3||||econ||||
481||||None||||Estimation of High-Dimensional Seemingly Unrelated Regression Models||||arXiv.org||||2018/11/13||||Estimation of High-Dimensional Seemingly Unrelated Regression Models||||Tan, Lidan || Chiong, Khai X. || Moon, Hyungsik Roger||||https://arxiv.org/pdf/1811.05567||||1811.05567||||In this paper, we investigate seemingly unrelated regression (SUR) models that allow the number of equations (N) to be large, and to be comparable to the number of the observations in each equation (T). It is well known in the literature that the conventional SUR estimator, for example, the generalized least squares (GLS) estimator of Zellner (1962) does not perform well. As the main contribution of the paper, we propose a new feasible GLS estimator called the feasible graphical lasso (FGLasso) estimator. For a feasible implementation of the GLS estimator, we use the graphical lasso estimation of the precision matrix (the inverse of the covariance matrix of the equation system errors) assuming that the underlying unknown precision matrix is sparse. We derive asymptotic theories of the new estimator and investigate its finite sample properties via Monte-Carlo simulations.||||@arxiv||||2018/11/13||||Estimation of High-Dimensional Seemingly Unrelated Regression Models||||In this paper, we investigate seemingly unrelated regression (SUR) models that allow the number of equations (N) to be large, and to be comparable to the number of the observations in each...||||https://arxiv.org/abs/1811.05567v1||||econ||||
482||||None||||Nonparametric Estimation of the Random Coefficients Model: An Elastic Net Approach||||arXiv.org||||2019/09/19||||Nonparametric Estimation of the Random Coefficients Model: An Elastic Net Approach||||Heiss, Florian || Hetzenecker, Stephan || Osterhaus, Maximilian||||https://arxiv.org/pdf/1909.08434||||1909.08434||||This paper investigates and extends the computationally attractive nonparametric random coefficients estimator of Fox, Kim, Ryan, and Bajari (2011). We show that their estimator is a special case of the nonnegative LASSO, explaining its sparse nature observed in many applications. Recognizing this link, we extend the estimator, transforming it to a special case of the nonnegative elastic net. The extension improves the estimator's recovery of the true support and allows for more accurate estimates of the random coefficients' distribution. Our estimator is a generalization of the original estimator and therefore, is guaranteed to have a model fit at least as good as the original one. A theoretical analysis of both estimators' properties shows that, under conditions, our generalized estimator approximates the true distribution more accurately. Two Monte Carlo experiments and an application to a travel mode data set illustrate the improved performance of the generalized estimator.||||@arxiv||||2019/09/18||||Nonparametric Estimation of the Random Coefficients Model: An...||||This paper investigates and extends the computationally attractive nonparametric random coefficients estimator of Fox, Kim, Ryan, and Bajari (2011). We show that their estimator is a special case...||||https://arxiv.org/abs/1909.08434v2||||econ||||
483||||None||||The Crawler: Two Equivalence Results for Object (Re)allocation Problems when Preferences Are Single-peaked||||arXiv.org||||2019/12/14||||The Crawler: Two Equivalence Results for Object (Re)allocation Problems when Preferences Are Single-peaked||||Tamura, Yuki || Hosseini, Hadi||||https://arxiv.org/pdf/1912.06909||||1912.06909||||For object reallocation problems, if preferences are strict but otherwise unrestricted, the Top Trading Cycle rule (TTC) is the leading rule: It is the only rule satisfying efficiency, the endowment lower bound, and strategy-proofness; moreover, TTC coincides with the core. However, on the subdomain of single-peaked preferences, Bade (2019a) defines a new rule, the "crawler", which also satisfies the first three properties. Our first theorem states that the crawler and a naturally defined "dual" rule are actually the same. Next, for object allocation problems, we define a probabilistic version of the crawler by choosing an endowment profile at random according to a uniform distribution, and applying the original definition. Our second theorem states that this rule is the same as the "random priority rule" which, as proved by Knuth (1996) and Abdulkadiroglu and Sönmez (1998), is equivalent to the "core from random endowments".||||@arxiv||||2019/12/14||||The Crawler: Two Equivalence Results for Object (Re)allocation...||||For object reallocation problems, if preferences are strict but otherwise unrestricted, the Top Trading Cycle rule (TTC) is the leading rule: It is the only rule satisfying efficiency, the...||||https://arxiv.org/abs/1912.06909v1||||cs||||
484||||None||||Detecting Identification Failure in Moment Condition Models||||arXiv.org||||2019/08/29||||Detecting Identification Failure in Moment Condition Models||||Forneron, Jean-Jacques||||https://arxiv.org/pdf/1907.13093||||1907.13093||||This paper develops an approach to detect identification failures in a large class of moment condition models. This is achieved by introducing a quasi-Jacobian matrix which is asymptotically singular under higher-order local identification as well as weak/set identification; in these settings, standard asymptotics are not valid. Under (semi)-strong identification, where standard asymptotics are valid, this matrix is asymptotically equivalent to the usual Jacobian matrix. After re-scaling, it is thus asymptotically non-singular. Together, these results imply that the eigenvalues of the quasi-Jacobian can detect potential local and global identification failures. Furthermore, the quasi-Jacobian is informative about the span of the identification failure. This information permits two-step identification robust subvector inference without any a priori knowledge of the underlying identification structure. Monte-Carlo simulations and empirical applications illustrate the results.||||@arxiv||||2019/07/30||||Detecting Identification Failure in Moment Condition Models||||This paper develops an approach to detect identification failures in a large class of moment condition models. This is achieved by introducing a quasi-Jacobian matrix which is asymptotically...||||https://arxiv.org/abs/1907.13093v2||||econ||||
485||||None||||Estimation of Large Network Formation Games||||arXiv.org||||2020/01/12||||Estimation of Large Network Formation Games||||Ridder, Geert || Sheng, Shuyang||||https://arxiv.org/pdf/2001.03838||||2001.03838||||This paper develops estimation methods for network formation models using observed data from a single large network. The model allows for utility externalities from friends of friends and friends in common, so the expected utility is nonlinear in the link choices of an agent. We propose a novel method that uses the Legendre transform to express the expected utility as a linear function of the individual link choices. This implies that the optimal link decision is that for an agent who myopically chooses to establish links or not to the other members of the network. The dependence between the agent's link choices is through an auxiliary variable. We propose a two-step estimation procedure that requires weak assumptions on equilibrium selection, is simple to compute, and has consistent and asymptotically normal estimators for the parameters. Monte Carlo results show that the estimation procedure performs well.||||@arxiv||||2020/01/12||||Estimation of Large Network Formation Games||||This paper develops estimation methods for network formation models using observed data from a single large network. The model allows for utility externalities from friends of friends and friends...||||https://arxiv.org/abs/2001.03838v1||||econ||||
486||||None||||Detecting correlations and triangular arbitrage opportunities in the Forex by means of multifractal detrended cross-correlations analysis||||arXiv.org||||2019/10/31||||Detecting correlations and triangular arbitrage opportunities in the Forex by means of multifractal detrended cross-correlations analysis||||Gębarowski, Robert || Oświęcimka, Paweł || Wątorek, Marcin || Drożdż, Stanisław||||https://arxiv.org/pdf/1906.07491||||1906.07491||||Multifractal detrended cross-correlation methodology is described and applied to Foreign exchange (Forex) market time series. Fluctuations of high frequency exchange rates of eight major world currencies over 2010-2018 period are used to study cross-correlations. The study is motivated by fundamental questions in complex systems' response to significant environmental changes and by potential applications in investment strategies, including detecting triangular arbitrage opportunities. Dominant multiscale cross-correlations between the exchange rates are found to typically occur at smaller fluctuation levels. However hierarchical organization of ties expressed in terms of dendrograms, with a novel application of the multiscale cross-correlation coefficient, are more pronounced at large fluctuations. The cross-correlations are quantified to be stronger on average between those exchange rate pairs that are bound within triangular relations. Some pairs from outside triangular relations are however identified to be exceptionally strongly correlated as compared to the average strength of triangular correlations.This in particular applies to those exchange rates that involve Australian and New Zealand dollars and reflects their economic relations. Significant events with impact on the Forex are shown to induce triangular arbitrage opportunities which at the same time reduce cross--correlations on the smallest time scales and act destructively on the multiscale organization of correlations. In 2010--2018 such instances took place in connection with the Swiss National Bank intervention and the weakening of British pound sterling accompanying the initiation of Brexit procedure. The methodology could be applicable to temporal and multiscale pattern detection in any time series.||||@arxiv||||2019/06/18||||Detecting correlations and triangular arbitrage opportunities in...||||Multifractal detrended cross-correlation methodology is described and applied to Foreign exchange (Forex) market time series. Fluctuations of high frequency exchange rates of eight major world...||||https://arxiv.org/abs/1906.07491v2||||cs||||
487||||None||||Best Subset Binary Prediction||||arXiv.org||||2018/05/16||||Best Subset Binary Prediction||||Chen, Le-Yu || Lee, Sokbae||||https://arxiv.org/pdf/1610.02738||||1610.02738||||We consider a variable selection problem for the prediction of binary outcomes. We study the best subset selection procedure by which the covariates are chosen by maximizing Manski (1975, 1985)'s maximum score objective function subject to a constraint on the maximal number of selected variables. We show that this procedure can be equivalently reformulated as solving a mixed integer optimization problem, which enables computation of the exact or an approximate solution with a definite approximation error bound. In terms of theoretical results, we obtain non-asymptotic upper and lower risk bounds when the dimension of potential covariates is possibly much larger than the sample size. Our upper and lower risk bounds are minimax rate-optimal when the maximal number of selected variables is fixed and does not increase with the sample size. We illustrate usefulness of the best subset binary prediction approach via Monte Carlo simulations and an empirical application of the work-trip transportation mode choice.||||@arxiv||||2016/10/09||||Best Subset Binary Prediction||||We consider a variable selection problem for the prediction of binary outcomes. We study the best subset selection procedure by which the covariates are chosen by maximizing Manski (1975, 1985)'s...||||https://arxiv.org/abs/1610.02738v7||||econ||||
488||||None||||Stochastic Switching Games||||arXiv.org||||2018/07/10||||Stochastic Switching Games||||Li, Liangchen || Ludkovski, Michael||||https://arxiv.org/pdf/1807.03893||||1807.03893||||We study nonzero-sum stochastic switching games. Two players compete for market dominance through controlling (via timing options) the discrete-state market regime $M$. Switching decisions are driven by a continuous stochastic factor $X$ that modulates instantaneous revenue rates and switching costs. This generates a competitive feedback between the short-term fluctuations due to $X$ and the medium-term advantages based on $M$. We construct threshold-type Feedback Nash Equilibria which characterize stationary strategies describing long-run dynamic equilibrium market organization. Two sequential approximation schemes link the switching equilibrium to (i) constrained optimal switching, (ii) multi-stage timing games. We provide illustrations using an Ornstein-Uhlenbeck $X$ that leads to a recurrent equilibrium $M^\ast$ and a Geometric Brownian Motion $X$ that makes $M^\ast$ eventually "absorbed" as one player eventually gains permanent advantage. Explicit computations and comparative statics regarding the emergent macroscopic market equilibrium are also provided.||||@arxiv||||2018/07/10||||Stochastic Switching Games||||We study nonzero-sum stochastic switching games. Two players compete for market dominance through controlling (via timing options) the discrete-state market regime $M$. Switching decisions are...||||https://arxiv.org/abs/1807.03893v1||||econ||||
489||||None||||Comparing the Forecasting Performances of Linear Models for Electricity Prices with High RES Penetration||||arXiv.org||||2019/11/12||||Comparing the Forecasting Performances of Linear Models for Electricity Prices with High RES Penetration||||Gianfreda, Angelica || Ravazzolo, Francesco || Rossini, Luca||||https://arxiv.org/pdf/1801.01093||||1801.01093||||This paper compares alternative univariate versus multivariate models, frequentist versus Bayesian autoregressive and vector autoregressive specifications, for hourly day-ahead electricity prices, both with and without renewable energy sources. The accuracy of point and density forecasts are inspected in four main European markets (Germany, Denmark, Italy and Spain) characterized by different levels of renewable energy power generation. Our results show that the Bayesian VAR specifications with exogenous variables dominate other multivariate and univariate specifications, in terms of both point and density forecasting.||||@arxiv||||2018/01/03||||Comparing the Forecasting Performances of Linear Models for...||||This paper compares alternative univariate versus multivariate models, frequentist versus Bayesian autoregressive and vector autoregressive specifications, for hourly day-ahead electricity prices,...||||https://arxiv.org/abs/1801.01093v3||||econ||||
490||||None||||The Syntax of the Accounting Language: A First Step||||arXiv.org||||2019/06/26||||The Syntax of the Accounting Language: A First Step||||Botafogo, Frederico||||https://arxiv.org/pdf/1906.10865||||1906.10865||||We review and interpret two basic propositions published by Ellerman (2014). The propositions address the algebraic structure of T accounts and double entry bookkeeping (DEB). The paper builds on this previous contribution with the view of reconciling the two, apparently dichotomous, perspectives of accounting measurement: the one that focuses preferably on the stock of wealth and to the one that focuses preferably on the flow of income. The paper claims that T-accounts and DEB have an underlying algebraic structure suitable for approaching measurement from either or both perspectives. Accountants preferences for stocks or flows can be framed in ways which are mutually consistent. The paper is a first step in addressing this consistency issue. It avoids the difficult mathematics of abstract algebra by applying the concept of syntax to accounting numbers such that the accounting procedure qualifies as a formal language with which accountants convey meaning.||||@arxiv||||2019/06/26||||The Syntax of the Accounting Language: A First Step||||We review and interpret two basic propositions published by Ellerman (2014). The propositions address the algebraic structure of T accounts and double entry bookkeeping (DEB). The paper builds on...||||https://arxiv.org/abs/1906.10865v1||||econ||||
491||||None||||SortedEffects: Sorted Causal Effects in R||||arXiv.org||||2019/11/06||||SortedEffects: Sorted Causal Effects in R||||Chen, Shuowen || Chernozhukov, Victor || Fernández-Val, Iván || Luo, Ye||||https://arxiv.org/pdf/1909.00836||||1909.00836||||Chernozhukov et al. (2018) proposed the sorted effect method for nonlinear regression models. This method consists of reporting percentiles of the partial effects in addition to the average commonly used to summarize the heterogeneity in the partial effects. They also proposed to use the sorted effects to carry out classification analysis where the observational units are classified as most and least affected if their causal effects are above or below some tail sorted effects. The R package SortedEffects implements the estimation and inference methods therein and provides tools to visualize the results. This vignette serves as an introduction to the package and displays basic functionality of the functions within.||||@arxiv||||2019/09/02||||SortedEffects: Sorted Causal Effects in R||||Chernozhukov et al. (2018) proposed the sorted effect method for nonlinear regression models. This method consists of reporting percentiles of the partial effects in addition to the average...||||https://arxiv.org/abs/1909.00836v3||||econ||||
492||||None||||Efficiency in Micro-Behaviors and FL Bias||||arXiv.org||||2018/05/11||||Efficiency in Micro-Behaviors and FL Bias||||Kazutaka, Kurihara || Tutiya, Yohei||||https://arxiv.org/pdf/1805.04225||||1805.04225||||In this paper, we propose a model which simulates odds distributions of pari-mutuel betting system under two hypotheses on the behavior of bettors: 1. The amount of bets increases very rapidly as the deadline for betting comes near. 2. Each bettor bets on a horse which gives the largest expectation value of the benefit. The results can be interpreted as such efficient behaviors do not serve to extinguish the FL bias but even produce stronger FL bias.||||@arxiv||||2018/05/11||||Efficiency in Micro-Behaviors and FL Bias||||In this paper, we propose a model which simulates odds distributions of pari-mutuel betting system under two hypotheses on the behavior of bettors: 1. The amount of bets increases very rapidly as...||||https://arxiv.org/abs/1805.04225v1||||econ||||
493||||None||||Dynamic Models with Robust Decision Makers: Identification and Estimation||||arXiv.org||||2019/01/29||||Dynamic Models with Robust Decision Makers: Identification and Estimation||||Christensen, Timothy M.||||https://arxiv.org/pdf/1812.11246||||1812.11246||||This paper studies identification and estimation of a class of dynamic models in which the decision maker (DM) is uncertain about the data-generating process. The DM surrounds a benchmark model that he or she fears is misspecified by a set of models. Decisions are evaluated under a worst-case model delivering the lowest utility among all models in this set. The DM's benchmark model and preference parameters are jointly underidentified. With the benchmark model held fixed, primitive conditions are established for identification of the DM's worst-case model and preference parameters. The key step in the identification analysis is to establish existence and uniqueness of the DM's continuation value function allowing for unbounded statespace and unbounded utilities. To do so, fixed-point results are derived for monotone, convex operators that act on a Banach space of thin-tailed functions arising naturally from the structure of the continuation value recursion. The fixed-point results are quite general; applications to models with learning and Rust-type dynamic discrete choice models are also discussed. For estimation, a perturbation result is derived which provides a necessary and sufficient condition for consistent estimation of continuation values and the worst-case model. The result also allows convergence rates of estimators to be characterized. An empirical application studies an endowment economy where the DM's benchmark model may be interpreted as an aggregate of experts' forecasting models. The application reveals time-variation in the way the DM pessimistically distorts benchmark probabilities. Consequences for asset pricing are explored and connections are drawn with the literature on macroeconomic uncertainty.||||@arxiv||||2018/12/28||||Dynamic Models with Robust Decision Makers: Identification and Estimation||||This paper studies identification and estimation of a class of dynamic models in which the decision maker (DM) is uncertain about the data-generating process. The DM surrounds a benchmark model...||||https://arxiv.org/abs/1812.11246v3||||econ||||
494||||None||||Collaborative knowledge creation: Evidence from Japanese patent data||||arXiv.org||||2019/08/04||||Collaborative knowledge creation: Evidence from Japanese patent data||||Mori, Tomoya || Sakaguchi, Shosei||||https://arxiv.org/pdf/1908.01256||||1908.01256||||This paper presents micro-econometric evidence for collaborative knowledge creation at the level of individual researchers. The key determinant for developing new ideas is the exchange of differentiated knowledge among collaborators. To stay creative, inventors seek opportunities to shift their technological expertise to unexplored niches by utilizing the differentiated knowledge of new collaborators. Furthermore, a more active recombination of collaborators by an inventor facilitates the selection of collaborators to raise the amount of differentiated knowledge from their collaborators.||||@arxiv||||2019/08/04||||Collaborative knowledge creation: Evidence from Japanese patent data||||This paper presents micro-econometric evidence for collaborative knowledge creation at the level of individual researchers. The key determinant for developing new ideas is the exchange of...||||https://arxiv.org/abs/1908.01256v1||||econ||||
495||||None||||Broken Detailed Balance and Non-Equilibrium Dynamics in Noisy Social Learning Models||||arXiv.org||||2019/06/27||||Broken Detailed Balance and Non-Equilibrium Dynamics in Noisy Social Learning Models||||Vaidya, Tushar || Chotibut, Thiparat || Piliouras, Georgios||||https://arxiv.org/pdf/1906.11481||||1906.11481||||We propose new Degroot-type social learning models with feedback in a continuous time, to investigate the effect of a noisy information source on consensus formation in a social network. Unlike the standard Degroot framework, noisy information models destroy consensus formation. On the other hand, the noisy opinion dynamics converge to the equilibrium distribution that encapsulates correlations among agents' opinions. Interestingly, such an equilibrium distribution is also a non-equilibrium steady state (NESS) with a non-zero probabilistic current loop. Thus, noisy information source leads to a NESS at long times that encodes persistent correlated opinion dynamics of learning agents.||||@arxiv||||2019/06/27||||Broken Detailed Balance and Non-Equilibrium Dynamics in Noisy...||||We propose new Degroot-type social learning models with feedback in a continuous time, to investigate the effect of a noisy information source on consensus formation in a social network. Unlike...||||https://arxiv.org/abs/1906.11481v1||||cs||||
496||||None||||J. S. Mill's Liberal Principle and Unanimity||||arXiv.org||||2019/03/18||||J. S. Mill's Liberal Principle and Unanimity||||Green, Edward J.||||https://arxiv.org/pdf/1903.07769||||1903.07769||||The broad concept of an individual's welfare is actually a cluster of related specific concepts that bear a "family resemblance" to one another. One might care about how a policy will affect people both in terms of their subjective preferences and also in terms of some notion of their objective interests. This paper provides a framework for evaluation of policies in terms of welfare criteria that combine these two considerations. Sufficient conditions are provided for such a criterion to imply the same ranking of social states as does Pareto's unanimity criterion. Sufficiency is proved via study of a community of agents with interdependent ordinal preferences.||||@arxiv||||2019/03/18||||J. S. Mill's Liberal Principle and Unanimity||||The broad concept of an individual's welfare is actually a cluster of related specific concepts that bear a "family resemblance" to one another. One might care about how a policy will affect...||||https://arxiv.org/abs/1903.07769v1||||econ||||
497||||None||||Measuring Differences in Stochastic Network Structure||||arXiv.org||||2020/02/07||||Measuring Differences in Stochastic Network Structure||||Auerbach, Eric||||https://arxiv.org/pdf/1903.11117||||1903.11117||||How can one determine whether a community-level treatment, such as the introduction of a social program or trade shock, alters agents' incentives to form links in a network? This paper proposes analogues of a two-sample Kolmogorov-Smirnov test, widely used in the literature to test the null hypothesis of "no treatment effects", for network data. It first specifies a testing problem in which the null hypothesis is that two networks are drawn from the same random graph model. It then describes two randomization tests based on the magnitude of the difference between the networks' adjacency matrices as measured by the $2\to2$ and $\infty\to1$ operator norms. Power properties of the tests are examined analytically, in simulation, and through two real-world applications. A key finding is that the test based on the $\infty\to1$ norm can be substantially more powerful than that based on the $2\to2$ norm for the kinds of sparse and degree-heterogeneous networks common in economics.||||@arxiv||||2019/03/26||||Measuring Differences in Stochastic Network Structure||||How can one determine whether a community-level treatment, such as the introduction of a social program or trade shock, alters agents' incentives to form links in a network? This paper proposes...||||https://arxiv.org/abs/1903.11117v4||||econ||||
498||||None||||Inference Related to Common Breaks in a Multivariate System with Joined Segmented Trends with Applications to Global and Hemispheric Temperatures||||arXiv.org||||2018/05/25||||Inference Related to Common Breaks in a Multivariate System with Joined Segmented Trends with Applications to Global and Hemispheric Temperatures||||Kim, Dukpa || Oka, Tatsushi || Estrada, Francisco || Perron, Pierre||||https://arxiv.org/pdf/1805.09937||||1805.09937||||What transpires from recent research is that temperatures and radiative forcing seem to be characterized by a linear trend with two changes in the rate of growth. The first occurs in the early 60s and indicates a very large increase in the rate of growth of both temperature and radiative forcing series. This was termed as the "onset of sustained global warming". The second is related to the more recent so-called hiatus period, which suggests that temperatures and total radiative forcing have increased less rapidly since the mid-90s compared to the larger rate of increase from 1960 to 1990. There are two issues that remain unresolved. The first is whether the breaks in the slope of the trend functions of temperatures and radiative forcing are common. This is important because common breaks coupled with the basic science of climate change would strongly suggest a causal effect from anthropogenic factors to temperatures. The second issue relates to establishing formally via a proper testing procedure that takes into account the noise in the series, whether there was indeed a `hiatus period' for temperatures since the mid 90s. This is important because such a test would counter the widely held view that the hiatus is the product of natural internal variability. Our paper provides tests related to both issues. The results show that the breaks in temperatures and radiative forcing are common and that the hiatus is characterized by a significant decrease in their rate of growth. The statistical results are of independent interest and applicable more generally.||||@arxiv||||2018/05/25||||Inference Related to Common Breaks in a Multivariate System with...||||What transpires from recent research is that temperatures and radiative forcing seem to be characterized by a linear trend with two changes in the rate of growth. The first occurs in the early 60s...||||https://arxiv.org/abs/1805.09937v1||||econ||||
499||||None||||Discrete Choice under Risk with Limited Consideration||||arXiv.org||||2019/02/18||||Discrete Choice under Risk with Limited Consideration||||Barseghyan, Levon || Molinari, Francesca || Thirkettle, Matthew||||https://arxiv.org/pdf/1902.06629||||1902.06629||||This paper is concerned with learning decision makers' (DMs) preferences using data on observed choices from a finite set of risky alternatives with monetary outcomes. We propose a discrete choice model with unobserved heterogeneity in consideration sets (the collection of alternatives considered by DMs) and unobserved heterogeneity in standard risk aversion. In this framework, stochastic choice is driven both by different rankings of alternatives induced by unobserved heterogeneity in risk preferences and by different sets of alternatives considered. We obtain sufficient conditions for semi-nonparametric point identification of both the distribution of unobserved heterogeneity in preferences and the distribution of consideration sets. Our method yields an estimator that is easy to compute and that can be used in markets with a large number of alternatives. We apply our method to a dataset on property insurance purchases. We find that although households are on average strongly risk averse, they consider lower coverages more frequently than higher coverages. Finally, we estimate the monetary losses associated with limited consideration in our application.||||@arxiv||||2019/02/18||||Discrete Choice under Risk with Limited Consideration||||This paper is concerned with learning decision makers' (DMs) preferences using data on observed choices from a finite set of risky alternatives with monetary outcomes. We propose a discrete choice...||||https://arxiv.org/abs/1902.06629v1||||econ||||
500||||None||||Bilateral Tariffs Under International Competition||||arXiv.org||||2020/01/08||||Bilateral Tariffs Under International Competition||||Kutalia, Tsotne || Tevzadze, Revaz||||https://arxiv.org/pdf/2001.02426||||2001.02426||||This paper explores the gain maximization problem of two nations engaging in non-cooperative bilateral trade. Probabilistic model of an exchange of commodities under different price systems is considered. Volume of commodities exchanged determines the demand each nation has over the counter party's currency. However, each nation can manipulate this quantity by imposing a tariff on imported commodities. As long as the gain from trade is determined by the balance between imported and exported commodities, such a scenario results in a two party game where Nash equilibrium tariffs are determined for various foreign currency demand functions and ultimately, the exchange rate based on optimal tariffs is obtained.||||@arxiv||||2020/01/08||||Bilateral Tariffs Under International Competition||||This paper explores the gain maximization problem of two nations engaging in non-cooperative bilateral trade. Probabilistic model of an exchange of commodities under different price systems is...||||https://arxiv.org/abs/2001.02426v1||||econ||||
501||||None||||Proceedings Seventeenth Conference on Theoretical Aspects of Rationality and Knowledge||||arXiv.org||||2019/07/19||||Proceedings Seventeenth Conference on Theoretical Aspects of Rationality and Knowledge||||Moss, Lawrence S.||||https://arxiv.org/pdf/1907.08335||||1907.08335||||This is the proceedings of the Seventeenth conference on Theoretical Aspects of Rationality and Knowledge, 17-19 July 2019, Institut de Recherche en Informatique de Toulouse (IRIT), Toulouse University Toulouse, France. The mission of the TARK conferences is to bring together researchers from a wide variety of fields, including Artificial Intelligence, Cryptography, Distributed Computing, Economics and Game Theory, Linguistics, Philosophy, and Psychology, in order to further our understanding of interdisciplinary issues involving reasoning about rationality and knowledge.||||@arxiv||||2019/07/19||||Proceedings Seventeenth Conference on Theoretical Aspects of...||||This is the proceedings of the Seventeenth conference on Theoretical Aspects of Rationality and Knowledge, 17-19 July 2019, Institut de Recherche en Informatique de Toulouse (IRIT), Toulouse...||||https://arxiv.org/abs/1907.08335v1||||cs||||
502||||None||||Why understanding multiplex social network structuring processes will help us better understand the evolution of human behavior||||arXiv.org||||2019/03/26||||Why understanding multiplex social network structuring processes will help us better understand the evolution of human behavior||||Atkisson, Curtis || Górski, Piotr J. || Jackson, Matthew O. || Hołyst, Janusz A. || D'Souza, Raissa M.||||https://arxiv.org/pdf/1903.11183||||1903.11183||||Anthropologists have long appreciated that single-layer networks are insufficient descriptions of human interactions---individuals are embedded in complex networks with dependencies. One debate explicitly about this surrounds food sharing. Some argue that failing to find reciprocal food sharing means that some process other than reciprocity must be occurring, whereas others argue for models that allow reciprocity to span domains. The analysis of multi-dimensional social networks has recently garnered the attention of the mathematics and physics communities. Multilayer networks are ubiquitous and have consequences, so processes giving rise to them are important social phenomena. Recent models of these processes show how ignoring layer interdependencies can lead one to miss why a layer formed the way it did, and/or draw erroneous conclusions. Understanding the structuring processes that underlie multiplex networks will help understand increasingly rich datasets, which give better, richer, and more accurate pictures of social interactions.||||@arxiv||||2019/03/26||||Why understanding multiplex social network structuring processes...||||Anthropologists have long appreciated that single-layer networks are insufficient descriptions of human interactions---individuals are embedded in complex networks with dependencies. One debate...||||https://arxiv.org/abs/1903.11183v1||||cs||||
503||||None||||Generalized Random Forests||||arXiv.org||||2018/04/05||||Generalized Random Forests||||Athey, Susan || Tibshirani, Julie || Wager, Stefan||||https://arxiv.org/pdf/1610.01271||||1610.01271||||We propose generalized random forests, a method for non-parametric statistical estimation based on random forests (Breiman, 2001) that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. Following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest. We propose a flexible, computationally efficient algorithm for growing generalized random forests, develop a large sample theory for our method showing that our estimates are consistent and asymptotically Gaussian, and provide an estimator for their asymptotic variance that enables valid confidence intervals. We use our approach to develop new methods for three statistical tasks: non-parametric quantile regression, conditional average partial effect estimation, and heterogeneous treatment effect estimation via instrumental variables. A software implementation, grf for R and C++, is available from CRAN.||||@arxiv||||2016/10/05||||Generalized Random Forests||||We propose generalized random forests, a method for non-parametric statistical estimation based on random forests (Breiman, 2001) that can be used to fit any quantity of interest identified as the...||||https://arxiv.org/abs/1610.01271v4||||econ||||
504||||None||||Skills to not fall behind in school||||arXiv.org||||2020/01/28||||Skills to not fall behind in school||||Polo, Felipe Maia||||https://arxiv.org/pdf/2001.10519||||2001.10519||||Many recent studies emphasize how important the role of cognitive and social-emotional skills can be in determining people's quality of life. Although skills are of great importance in many aspects, in this paper we will focus our efforts to better understand the relationship between several types of skills with academic progress delay. Our dataset contains the same students in 2012 and 2017, and we consider that there was a academic progress delay for a specific student if he or she progressed less than expected in school grades. Our methodology primarily includes the use of a Bayesian logistic regression model and our results suggest that both cognitive and social-emotional skills may impact the conditional probability of falling behind in school, and the magnitude of the impact between the two types of skills can be comparable.||||@arxiv||||2020/01/28||||Skills to not fall behind in school||||Many recent studies emphasize how important the role of cognitive and social-emotional skills can be in determining people's quality of life. Although skills are of great importance in many...||||https://arxiv.org/abs/2001.10519v1||||econ||||
505||||None||||Nonparametric Identification of First-Price Auction with Unobserved Competition: A Density Discontinuity Framework||||arXiv.org||||2019/08/15||||Nonparametric Identification of First-Price Auction with Unobserved Competition: A Density Discontinuity Framework||||Guerre, Emmanuel || Luo, Yao||||https://arxiv.org/pdf/1908.05476||||1908.05476||||We consider nonparametric identification of independent private value first-price auction models, in which the analyst only observes winning bids. Our benchmark model assumes an exogenous number of bidders $N$. We show that, if the bidders observe $N$, the resulting discontinuities in the winning bid density can be used to identify the distribution of $N$. The private value distribution can be identified in a second step. A second class of models considers endogenously-determined $N$, due to a reserve price or an entry cost. If bidders observe $N$, these models are also identifiable using winning bid discontinuities. If bidders cannot observe $N$, however, identification is not possible unless the analyst observes an instrument which affects the reserve price or entry cost. Lastly, we derive some testable restrictions for whether bidders observe the number of competitors and whether endogenous participation is due to a reserve price or entry cost. An application to USFS timber auction data illustrates the usefulness of our theoretical results for competition analysis, showing that nearly one bid out of three can be non competitive. It also suggests that the risk aversion bias caused by a mismeasured competition can be large.||||@arxiv||||2019/08/15||||Nonparametric Identification of First-Price Auction with...||||We consider nonparametric identification of independent private value first-price auction models, in which the analyst only observes winning bids. Our benchmark model assumes an exogenous number...||||https://arxiv.org/abs/1908.05476v1||||econ||||
506||||None||||Optimal electricity demand response contracting with responsiveness incentives||||arXiv.org||||2019/05/26||||Optimal electricity demand response contracting with responsiveness incentives||||Aïd, René || Possamaï, Dylan || Touzi, Nizar||||https://arxiv.org/pdf/1810.09063||||1810.09063||||Despite the success of demand response programs in retail electricity markets in reducing average consumption, the random responsiveness of consumers to price event makes their efficiency questionable to achieve the flexibility needed for electric systems with a large share of renewable energy. The variance of consumers' responses depreciates the value of these mechanisms and makes them weakly reliable. This paper aims at designing demand response contracts which allow to act on both the average consumption and its variance. The interaction between a risk--averse producer and a risk--averse consumer is modelled through a Principal--Agent problem, thus accounting for the moral hazard underlying demand response contracts. We provide closed--form solution for the optimal contract in the case of constant marginal costs of energy and volatility for the producer and constant marginal value of energy for the consumer. We show that the optimal contract has a rebate form where the initial condition of the consumption serves as a baseline. Further, the consumer cannot manipulate the baseline at his own advantage. The second--best price for energy and volatility are non--constant and non--increasing in time. The price for energy is lower (resp. higher) than the marginal cost of energy during peak--load (resp. off--peak) periods. We illustrate the potential benefit issued from the implementation of an incentive mechanism on the responsiveness of the consumer by calibrating our model with publicly available data. We predict a significant increase of responsiveness under our optimal contract and a significant increase of the producer satisfaction.||||@arxiv||||2018/10/22||||Optimal electricity demand response contracting with...||||Despite the success of demand response programs in retail electricity markets in reducing average consumption, the random responsiveness of consumers to price event makes their efficiency...||||https://arxiv.org/abs/1810.09063v3||||econ||||
507||||None||||Hysteresis of economic networks in an XY model||||arXiv.org||||2018/08/10||||Hysteresis of economic networks in an XY model||||Hosseiny, Ali || Absalan, Mohammadreza || Sherafati, Mohammad || Gallegati, Mauro||||https://arxiv.org/pdf/1808.03404||||1808.03404||||Many-body systems can have multiple equilibria. Though the energy of equilibria might be the same, still systems may resist to switch from an unfavored equilibrium to a favored one. In this paper we investigate occurrence of such phenomenon in economic networks. In times of crisis when governments intend to stimulate economy, a relevant question is on the proper size of stimulus bill. To address the answer, we emphasize the role of hysteresis in economic networks. In times of crises, firms and corporations cut their productions; now since their level of activity is correlated, metastable features in the network become prominent. This means that economic networks resist against the recovery actions. To measure the size of resistance in the network against recovery, we deploy the XY model. Though theoretically the XY model has no hysteresis, when it comes to the kinetic behavior in the deterministic regimes, we observe a dynamic hysteresis. We find that to overcome the hysteresis of the network, a minimum size of stimulation is needed for success. Our simulations show that as long as the networks are Watts-Strogatz, such minimum is independent of the characteristics of the networks.||||@arxiv||||2018/08/10||||Hysteresis of economic networks in an XY model||||Many-body systems can have multiple equilibria. Though the energy of equilibria might be the same, still systems may resist to switch from an unfavored equilibrium to a favored one. In this paper...||||https://arxiv.org/abs/1808.03404v1||||cond-mat||||
508||||None||||A Regulated Market Under Sanctions: On Tail Dependence Between Oil, Gold, and Tehran Stock Exchange Index||||arXiv.org||||2019/11/02||||A Regulated Market Under Sanctions: On Tail Dependence Between Oil, Gold, and Tehran Stock Exchange Index||||Shirvani, Abootaleb || Volchenkov, Dimitri||||https://arxiv.org/pdf/1911.01826||||1911.01826||||We demonstrate that the tail dependence should always be taken into account as a proxy for systematic risk of loss for investments. We provide the clear statistical evidence of that the structure of investment portfolios on a regulated market should be adjusted to the price of gold. Our finding suggests that the active bartering of oil for goods would prevent collapsing the national market facing international sanctions.||||@arxiv||||2019/11/02||||A Regulated Market Under Sanctions: On Tail Dependence Between...||||We demonstrate that the tail dependence should always be taken into account as a proxy for systematic risk of loss for investments. We provide the clear statistical evidence of that the structure...||||https://arxiv.org/abs/1911.01826v1||||econ||||
509||||None||||Maximizing Welfare in Social Networks under a Utility Driven Influence Diffusion Model||||arXiv.org||||2019/05/30||||Maximizing Welfare in Social Networks under a Utility Driven Influence Diffusion Model||||Banerjee, Prithu || Chen, Wei || Lakshmanan, Laks V. S.||||https://arxiv.org/pdf/1807.02502||||1807.02502||||Motivated by applications such as viral marketing, the problem of influence maximization (IM) has been extensively studied in the literature. The goal is to select a small number of users to adopt an item such that it results in a large cascade of adoptions by others. Existing works have three key limitations. (1) They do not account for economic considerations of a user in buying/adopting items. (2) Most studies on multiple items focus on competition, with complementary items receiving limited attention. (3) For the network owner, maximizing social welfare is important to ensure customer loyalty, which is not addressed in prior work in the IM literature. In this paper, we address all three limitations and propose a novel model called UIC that combines utility-driven item adoption with influence propagation over networks. Focusing on the mutually complementary setting, we formulate the problem of social welfare maximization in this novel setting. We show that while the objective function is neither submodular nor supermodular, surprisingly a simple greedy allocation algorithm achieves a factor of $(1-1/e-ε)$ of the optimum expected social welfare. We develop \textsf{bundleGRD}, a scalable version of this approximation algorithm, and demonstrate, with comprehensive experiments on real and synthetic datasets, that it significantly outperforms all baselines.||||@arxiv||||2018/07/06||||Maximizing Welfare in Social Networks under a Utility Driven...||||Motivated by applications such as viral marketing, the problem of influence maximization (IM) has been extensively studied in the literature. The goal is to select a small number of users to adopt...||||https://arxiv.org/abs/1807.02502v2||||cs||||
510||||None||||Almost Quasi-linear Utilities in Disguise: Positive-representation An Extension of Roberts' Theorem||||arXiv.org||||2019/10/26||||Almost Quasi-linear Utilities in Disguise: Positive-representation An Extension of Roberts' Theorem||||Nehama, Ilan||||https://arxiv.org/pdf/1910.12131||||1910.12131||||This work deals with the implementation of social choice rules using dominant strategies for unrestricted preferences. The seminal Gibbard-Satterthwaite theorem shows that only few unappealing social choice rules can be implemented unless we assume some restrictions on the preferences or allow monetary transfers. When monetary transfers are allowed and quasi-linear utilities w.r.t. money are assumed, Vickrey-Clarke-Groves (VCG) mechanisms were shown to implement any affine-maximizer, and by the work of Roberts, only affine-maximizers can be implemented whenever the type sets of the agents are rich enough.   In this work, we generalize these results and define a new class of preferences: Preferences which are positive-represented by a quasi-linear utility. That is, agents whose preference on a subspace of the outcomes can be modeled using a quasi-linear utility. We show that the characterization of VCG mechanisms as the incentive-compatible mechanisms extends naturally to this domain. Our result follows from a simple reduction to the characterization of VCG mechanisms. Hence, we see our result more as a fuller more correct version of the VCG characterization.   This work also highlights a common misconception in the community attributing the VCG result to the usage of transferable utility. Our result shows that the incentive-compatibility of the VCG mechanisms does not rely on money being a common denominator, but rather on the ability of the designer to fine the agents on a continuous (maybe agent-specific) scale.   We think these two insights, considering the utility as a representation and not as the preference itself (which is common in the economic community) and considering utilities which represent the preference only for the relevant domain, would turn out to fruitful in other domains as well.||||@arxiv||||2019/10/26||||Almost Quasi-linear Utilities in Disguise: Positive-representation...||||This work deals with the implementation of social choice rules using dominant strategies for unrestricted preferences. The seminal Gibbard-Satterthwaite theorem shows that only few unappealing...||||https://arxiv.org/abs/1910.12131v1||||cs||||
511||||None||||Achieving perfect coordination amongst agents in the co-action minority game||||arXiv.org||||2018/05/24||||Achieving perfect coordination amongst agents in the co-action minority game||||Rajpal, Hardik || Dhar, Deepak||||https://arxiv.org/pdf/1802.06770||||1802.06770||||We discuss the strategy that rational agents can use to maximize their expected long-term payoff in the co-action minority game. We argue that the agents will try to get into a cyclic state, where each of the $(2N +1)$ agent wins exactly $N$ times in any continuous stretch of $(2N+1)$ days. We propose and analyse a strategy for reaching such a cyclic state quickly, when any direct communication between agents is not allowed, and only the publicly available common information is the record of total number of people choosing the first restaurant in the past. We determine exactly the average time required to reach the periodic state for this strategy. We show that it varies as $(N/\ln 2) [1 + α\cos (2 π\log_2 N)$], for large $N$, where the amplitude $α$ of the leading term in the log-periodic oscillations is found be $\frac{8 π^2}{(\ln 2)^2} \exp{(- 2 π^2/\ln 2)} \approx {\color{blue}7 \times 10^{-11}}$.||||@arxiv||||2018/02/17||||Achieving perfect coordination amongst agents in the co-action...||||We discuss the strategy that rational agents can use to maximize their expected long-term payoff in the co-action minority game. We argue that the agents will try to get into a cyclic state, where...||||https://arxiv.org/abs/1802.06770v2||||econ||||
512||||None||||Selectivity correction in discrete-continuous models for the willingness to work as crowd-shippers and travel time tolerance||||arXiv.org||||2018/10/01||||Selectivity correction in discrete-continuous models for the willingness to work as crowd-shippers and travel time tolerance||||Le, Tho V. || Ukkusuri, Satish V.||||https://arxiv.org/pdf/1810.00985||||1810.00985||||The objective of this study is to understand the different behavioral considerations that govern the choice of people to engage in a crowd-shipping market. Using novel data collected by the researchers in the US, we develop discrete-continuous models. A binary logit model has been used to estimate crowd-shippers' willingness to work, and an ordinary least-square regression model has been employed to calculate crowd-shippers' maximum tolerance for shipping and delivery times. A selectivity-bias term has been included in the model to correct for the conditional relationships of the crowd-shipper's willingness to work and their maximum travel time tolerance. The results show socio-demographic characteristics (e.g. age, gender, race, income, and education level), transporting freight experience, and number of social media usages significant influence the decision to participate in the crowd-shipping market. In addition, crowd-shippers pay expectations were found to be reasonable and concurrent with the literature on value-of-time. Findings from this research are helpful for crowd-shipping companies to identify and attract potential shippers. In addition, an understanding of crowd-shippers - their behaviors, perceptions, demographics, pay expectations, and in which contexts they are willing to divert from their route - are valuable to the development of business strategies such as matching criteria and compensation schemes for driver-partners.||||@arxiv||||2018/10/01||||Selectivity correction in discrete-continuous models for the...||||The objective of this study is to understand the different behavioral considerations that govern the choice of people to engage in a crowd-shipping market. Using novel data collected by the...||||https://arxiv.org/abs/1810.00985v1||||econ||||
513||||None||||Notes on a Social Transmission Model with a Continuum of Agents||||arXiv.org||||2020/02/10||||Notes on a Social Transmission Model with a Continuum of Agents||||Golub, Benjamin||||https://arxiv.org/pdf/2002.03569||||2002.03569||||This note presents a simple overlapping-generations (OLG) model of the transmission of a trait, such as a culture. Initially, some fraction of agents carry the trait. In each time period, young agents are ``born'' and are influenced by some older agents. Agents adopt the trait only if at least a certain number of their influencers have the trait. This influence may occur due to rational choice (e.g., because the young agents are playing a coordination game with old agents who are already committed to a strategy), or for some other reason. Our interest is in how the process of social influence unfolds over time, and whether a trait will persist or die out. We characterize the dynamics of the fraction of active agents and relate the analysis to classic results on branching processes and random graphs.||||@arxiv||||2020/02/10||||Notes on a Social Transmission Model with a Continuum of Agents||||This note presents a simple overlapping-generations (OLG) model of the transmission of a trait, such as a culture. Initially, some fraction of agents carry the trait. In each time period, young...||||https://arxiv.org/abs/2002.03569v1||||econ||||
514||||None||||Partial Mean Processes with Generated Regressors: Continuous Treatment Effects and Nonseparable Models||||arXiv.org||||2018/10/31||||Partial Mean Processes with Generated Regressors: Continuous Treatment Effects and Nonseparable Models||||Lee, Ying-Ying||||https://arxiv.org/pdf/1811.00157||||1811.00157||||Partial mean with generated regressors arises in several econometric problems, such as the distribution of potential outcomes with continuous treatments and the quantile structural function in a nonseparable triangular model. This paper proposes a nonparametric estimator for the partial mean process, where the second step consists of a kernel regression on regressors that are estimated in the first step. The main contribution is a uniform expansion that characterizes in detail how the estimation error associated with the generated regressor affects the limiting distribution of the marginal integration estimator. The general results are illustrated with two examples: the generalized propensity score for a continuous treatment (Hirano and Imbens, 2004) and control variables in triangular models (Newey, Powell, and Vella, 1999; Imbens and Newey, 2009). An empirical application to the Job Corps program evaluation demonstrates the usefulness of the method.||||@arxiv||||2018/10/31||||Partial Mean Processes with Generated Regressors: Continuous...||||Partial mean with generated regressors arises in several econometric problems, such as the distribution of potential outcomes with continuous treatments and the quantile structural function in a...||||https://arxiv.org/abs/1811.00157v1||||econ||||
515||||None||||Bootstrap Consistency for Quadratic Forms of Sample Averages with Increasing Dimension||||arXiv.org||||2015/08/17||||Bootstrap Consistency for Quadratic Forms of Sample Averages with Increasing Dimension||||Pouzo, Demian||||https://arxiv.org/pdf/1411.2701||||1411.2701||||This paper establishes consistency of the weighted bootstrap for quadratic forms $\left( n^{-1/2} \sum_{i=1}^{n} Z_{i,n} \right)^{T}\left( n^{-1/2} \sum_{i=1}^{n} Z_{i,n} \right)$ where $(Z_{i,n})_{i=1}^{n}$ are mean zero, independent $\mathbb{R}^{d}$-valued random variables and $d=d(n)$ is allowed to grow with the sample size $n$, slower than $n^{1/4}$. The proof relies on an adaptation of Lindeberg interpolation technique whereby we simplify the original problem to a Gaussian approximation problem. We apply our bootstrap results to model-specification testing problems when the number of moments is allowed to grow with the sample size.||||@arxiv||||2014/11/11||||Bootstrap Consistency for Quadratic Forms of Sample Averages with...||||This paper establishes consistency of the weighted bootstrap for quadratic forms $\left( n^{-1/2} \sum_{i=1}^{n} Z_{i,n} \right)^{T}\left( n^{-1/2} \sum_{i=1}^{n} Z_{i,n} \right)$ where...||||https://arxiv.org/abs/1411.2701v4||||econ||||
516||||None||||Score-Driven Exponential Random Graphs: A New Class of Time-Varying Parameter Models for Dynamical Networks||||arXiv.org||||2019/05/26||||Score-Driven Exponential Random Graphs: A New Class of Time-Varying Parameter Models for Dynamical Networks||||Di Gangi, Domenico || Bormetti, Giacomo || Lillo, Fabrizio||||https://arxiv.org/pdf/1905.10806||||1905.10806||||Motivated by the evidence that real-world networks evolve in time and may exhibit non-stationary features, we propose an extension of the Exponential Random Graph Models (ERGMs) accommodating the time variation of network parameters. Within the ERGM framework, a network realization is sampled from a static probability distribution defined parametrically in terms of network statistics. Inspired by the fast growing literature on Dynamic Conditional Score-driven models, in our approach, each parameter evolves according to an updating rule driven by the score of the conditional distribution. We demonstrate the flexibility of the score-driven ERGMs, both as data generating processes and as filters, and we prove the advantages of the dynamic version with respect to the static one. Our method captures dynamical network dependencies, that emerge from the data, and allows for a test discriminating between static or time-varying parameters. Finally, we corroborate our findings with the application to networks from real financial and political systems exhibiting non stationary dynamics.||||@arxiv||||2019/05/26||||Score-Driven Exponential Random Graphs: A New Class of...||||Motivated by the evidence that real-world networks evolve in time and may exhibit non-stationary features, we propose an extension of the Exponential Random Graph Models (ERGMs) accommodating the...||||https://arxiv.org/abs/1905.10806v1||||econ||||
517||||None||||Existence in Multidimensional Screening with General Nonlinear Preferences||||arXiv.org||||2018/12/08||||Existence in Multidimensional Screening with General Nonlinear Preferences||||Zhang, Kelvin Shuangjian||||https://arxiv.org/pdf/1710.08549||||1710.08549||||We generalize the approach of Carlier (2001) and provide an existence proof for the multidimensional screening problem with general nonlinear preferences. We first formulate the principal's problem as a maximization problem with $G$-convexity constraints and then use $G$-convex analysis to prove existence.||||@arxiv||||2017/10/23||||Existence in Multidimensional Screening with General Nonlinear Preferences||||We generalize the approach of Carlier (2001) and provide an existence proof for the multidimensional screening problem with general nonlinear preferences. We first formulate the principal's...||||https://arxiv.org/abs/1710.08549v2||||econ||||
518||||None||||On the simulation of the Hawkes process via Lambert-W functions||||arXiv.org||||2019/07/22||||On the simulation of the Hawkes process via Lambert-W functions||||Magris, Martin||||https://arxiv.org/pdf/1907.09162||||1907.09162||||Several methods have been developed for the simulation of the Hawkes process. The oldest approach is the inverse sampling transform (ITS) suggested in \citep{ozaki1979maximum}, but rapidly abandoned in favor of more efficient alternatives. This manuscript shows that the ITS approach can be conveniently discussed in terms of Lambert-W functions. An optimized and efficient implementation suggests that this approach is computationally more performing than more recent alternatives available for the simulation of the Hawkes process.||||@arxiv||||2019/07/22||||On the simulation of the Hawkes process via Lambert-W functions||||Several methods have been developed for the simulation of the Hawkes process. The oldest approach is the inverse sampling transform (ITS) suggested in \citep{ozaki1979maximum}, but rapidly...||||https://arxiv.org/abs/1907.09162v1||||econ||||
519||||None||||The Wisdom of a Kalman Crowd||||arXiv.org||||2019/01/23||||The Wisdom of a Kalman Crowd||||Nash, Ulrik W.||||https://arxiv.org/pdf/1901.08133||||1901.08133||||The Kalman Filter has been called one of the greatest inventions in statistics during the 20th century. Its purpose is to measure the state of a system by processing the noisy data received from different electronic sensors. In comparison, a useful resource for managers in their effort to make the right decisions is the wisdom of crowds. This phenomenon allows managers to combine judgments by different employees to get estimates that are often more accurate and reliable than estimates, which managers produce alone. Since harnessing the collective intelligence of employees, and filtering signals from multiple noisy sensors appear related, we looked at the possibility of using the Kalman Filter on estimates by people. Our predictions suggest, and our findings based on the Survey of Professional Forecasters reveal, that the Kalman Filter can help managers solve their decision-making problems by giving them stronger signals before they choose. Indeed, when used on a subset of forecasters identified by the Contribution Weighted Model, the Kalman Filter beat that rule clearly, across all the forecasting horizons in the survey.||||@arxiv||||2019/01/23||||The Wisdom of a Kalman Crowd||||The Kalman Filter has been called one of the greatest inventions in statistics during the 20th century. Its purpose is to measure the state of a system by processing the noisy data received from...||||https://arxiv.org/abs/1901.08133v1||||econ||||
520||||None||||Causal Inference Under Approximate Neighborhood Interference||||arXiv.org||||2019/11/16||||Causal Inference Under Approximate Neighborhood Interference||||Leung, Michael P.||||https://arxiv.org/pdf/1911.07085||||1911.07085||||This paper studies causal inference in randomized experiments under network interference. Most existing models of interference posit that treatments assigned to alters only affect the ego's response through a low-dimensional exposure mapping, which only depends on units within some known network radius around the ego. We propose a substantially weaker "approximate neighborhood interference" (ANI) assumption, which allows treatments assigned to alters far from the ego to have a small, but potentially nonzero, impact on the ego's response. Unlike the exposure mapping model, we can show that ANI is satisfied in well-known models of social interactions. Despite its generality, inference in a single-network setting is still possible under ANI, as we prove that standard inverse-probability weighting estimators can consistently estimate treatment and spillover effects and are asymptotically normal. For practical inference, we propose a new conservative variance estimator based on a network bootstrap and suggest a data-dependent bandwidth using the network diameter. Finally, we illustrate our results in a simulation study and empirical application.||||@arxiv||||2019/11/16||||Causal Inference Under Approximate Neighborhood Interference||||This paper studies causal inference in randomized experiments under network interference. Most existing models of interference posit that treatments assigned to alters only affect the ego's...||||https://arxiv.org/abs/1911.07085v1||||econ||||
521||||None||||Learning from Neighbors about a Changing State||||arXiv.org||||2020/01/20||||Learning from Neighbors about a Changing State||||Dasaratha, Krishna || Golub, Benjamin || Hak, Nir||||https://arxiv.org/pdf/1801.02042||||1801.02042||||Agents learn about a changing state using private signals and past actions of neighbors in a network. We characterize equilibrium learning and social influence in this setting. We then examine when agents can aggregate information well, responding quickly to recent changes. A key sufficient condition for good aggregation is that each individual's neighbors have sufficiently different types of private information. In contrast, when signals are homogeneous, aggregation is suboptimal on any network. We also examine behavioral versions of the model, and show that achieving good aggregation requires a sophisticated understanding of correlations in neighbors' actions. The model provides a Bayesian foundation for a tractable learning dynamic in networks, closely related to the DeGroot model, and offers new tools for counterfactual and welfare analyses.||||@arxiv||||2018/01/06||||Learning from Neighbors about a Changing State||||Agents learn about a changing state using private signals and past actions of neighbors in a network. We characterize equilibrium learning and social influence in this setting. We then examine...||||https://arxiv.org/abs/1801.02042v6||||cs||||
522||||None||||Bootstrapping Structural Change Tests||||arXiv.org||||2018/11/09||||Bootstrapping Structural Change Tests||||Boldea, Otilia || Cornea-Madeira, Adriana || Hall, Alastair R.||||https://arxiv.org/pdf/1811.04125||||1811.04125||||This paper analyses the use of bootstrap methods to test for parameter change in linear models estimated via Two Stage Least Squares (2SLS). Two types of test are considered: one where the null hypothesis is of no change and the alternative hypothesis involves discrete change at k unknown break-points in the sample; and a second test where the null hypothesis is that there is discrete parameter change at l break-points in the sample against an alternative in which the parameters change at l + 1 break-points. In both cases, we consider inferences based on a sup-Wald-type statistic using either the wild recursive bootstrap or the wild fixed bootstrap. We establish the asymptotic validity of these bootstrap tests under a set of general conditions that allow the errors to exhibit conditional and/or unconditional heteroskedasticity, and report results from a simulation study that indicate the tests yield reliable inferences in the sample sizes often encountered in macroeconomics. The analysis covers the cases where the first-stage estimation of 2SLS involves a model whose parameters are either constant or themselves subject to discrete parameter change. If the errors exhibit unconditional heteroskedasticity and/or the reduced form is unstable then the bootstrap methods are particularly attractive because the limiting distributions of the test statistics are not pivotal.||||@arxiv||||2018/11/09||||Bootstrapping Structural Change Tests||||This paper analyses the use of bootstrap methods to test for parameter change in linear models estimated via Two Stage Least Squares (2SLS). Two types of test are considered: one where the null...||||https://arxiv.org/abs/1811.04125v1||||econ||||
524||||None||||A Justification of Conditional Confidence Intervals||||arXiv.org||||2019/01/19||||A Justification of Conditional Confidence Intervals||||Beutner, Eric || Heinemann, Alexander || Smeekes, Stephan||||https://arxiv.org/pdf/1710.00643||||1710.00643||||To quantify uncertainty around point estimates of conditional objects such as conditional means or variances, parameter uncertainty has to be taken into account. Attempts to incorporate parameter uncertainty are typically based on the unrealistic assumption of observing two independent processes, where one is used for parameter estimation, and the other for conditioning upon. Such unrealistic foundation raises the question whether these intervals are theoretically justified in a realistic setting. This paper presents an asymptotic justification for this type of intervals that does not require such an unrealistic assumption, but relies on a sample-split approach instead. By showing that our sample-split intervals coincide asymptotically with the standard intervals, we provide a novel, and realistic, justification for confidence intervals of conditional objects. The analysis is carried out for a rich class of time series models.||||@arxiv||||2017/10/02||||A Justification of Conditional Confidence Intervals||||To quantify uncertainty around point estimates of conditional objects such as conditional means or variances, parameter uncertainty has to be taken into account. Attempts to incorporate parameter...||||https://arxiv.org/abs/1710.00643v2||||econ||||
525||||None||||Efficient allocations in double auction markets||||arXiv.org||||2020/01/05||||Efficient allocations in double auction markets||||Pennanen, Teemu||||https://arxiv.org/pdf/2001.02071||||2001.02071||||This paper proposes a simple descriptive model for discrete-time double auction markets of divisible assets. As in the classical models of exchange economics, we consider a finite set of agents described by their initial endowments and preferences. Instead of the classical Walrasian-type market models, however, we assume that all trades take place in double auctions where the agents communicate through sealed limit orders for buying and selling. We find that, in repeated call auctions, nonstrategic bidding leads to a sequence of allocations that converges to individually rational Pareto allocations.||||@arxiv||||2020/01/05||||Efficient allocations in double auction markets||||This paper proposes a simple descriptive model for discrete-time double auction markets of divisible assets. As in the classical models of exchange economics, we consider a finite set of agents...||||https://arxiv.org/abs/2001.02071v1||||econ||||
526||||None||||Semiparametric Difference-in-Differences with Potentially Many Control Variables||||arXiv.org||||2019/01/08||||Semiparametric Difference-in-Differences with Potentially Many Control Variables||||Chang, Neng-Chieh||||https://arxiv.org/pdf/1812.10846||||1812.10846||||This paper discusses difference-in-differences (DID) estimation when there exist many control variables, potentially more than the sample size. In this case, traditional estimation methods, which require a limited number of variables, do not work. One may consider using statistical or machine learning (ML) methods. However, by the well-known theory of inference of ML methods proposed in Chernozhukov et al. (2018), directly applying ML methods to the conventional semiparametric DID estimators will cause significant bias and make these DID estimators fail to be sqrt{N}-consistent. This article proposes three new DID estimators for three different data structures, which are able to shrink the bias and achieve sqrt{N}-consistency and asymptotic normality with mean zero when applying ML methods. This leads to straightforward inferential procedures. In addition, I show that these new estimators have the small bias property (SBP), meaning that their bias will converge to zero faster than the pointwise bias of the nonparametric estimator on which it is based.||||@arxiv||||2018/12/27||||Semiparametric Difference-in-Differences with Potentially Many...||||This paper discusses difference-in-differences (DID) estimation when there exist many control variables, potentially more than the sample size. In this case, traditional estimation methods, which...||||https://arxiv.org/abs/1812.10846v3||||econ||||
527||||None||||Inference in high-dimensional set-identified affine models||||arXiv.org||||2019/03/29||||Inference in high-dimensional set-identified affine models||||Gafarov, Bulat||||https://arxiv.org/pdf/1904.00111||||1904.00111||||This paper proposes both point-wise and uniform confidence sets (CS) for an element $θ_{1}$ of a parameter vector $θ\in\mathbb{R}^{d}$ that is partially identified by affine moment equality and inequality conditions. The method is based on an estimator of a regularized support function of the identified set. This estimator is \emph{half-median unbiased} and has an \emph{asymptotic linear representation} which provides closed form standard errors and enables optimization-free multiplier bootstrap. The proposed CS can be computed as a solution to a finite number of linear and convex quadratic programs, which leads to a substantial decrease in \emph{computation time} and \emph{guarantee of global optimum}. As a result, the method provides uniformly valid inference in applications with the dimension of the parameter space, $d$, and the number of inequalities, $k$, that were previously computationally unfeasible ($d,k >100$). The proposed approach is then extended to construct polygon-shaped joint CS for multiple components of $θ$. Inference for coefficients in the linear IV regression model with interval outcome is used as an illustrative example.   Key Words: Affine moment inequalities; Asymptotic linear representation; Delta\textendash Method; Interval data; Intersection bounds; Partial identification; Regularization; Strong approximation; Stochastic Programming; Subvector inference; Uniform inference.||||@arxiv||||2019/03/29||||Inference in high-dimensional set-identified affine models||||This paper proposes both point-wise and uniform confidence sets (CS) for an element $θ_{1}$ of a parameter vector $θ\in\mathbb{R}^{d}$ that is partially identified by affine moment...||||https://arxiv.org/abs/1904.00111v1||||econ||||
528||||None||||Focused Bayesian Prediction||||arXiv.org||||2019/12/29||||Focused Bayesian Prediction||||Loaiza-Maya, Ruben || Martin, Gael M. || Frazier, David T.||||https://arxiv.org/pdf/1912.12571||||1912.12571||||We propose a new method for conducting Bayesian prediction that delivers accurate predictions without correctly specifying the unknown true data generating process. A prior is defined over a class of plausible predictive models. After observing data, we update the prior to a posterior over these models, via a criterion that captures a user-specified measure of predictive accuracy. Under regularity, this update yields posterior concentration onto the element of the predictive class that maximizes the expectation of the accuracy measure. In a series of simulation experiments and empirical examples we find notable gains in predictive accuracy relative to conventional likelihood-based prediction.||||@arxiv||||2019/12/29||||Focused Bayesian Prediction||||We propose a new method for conducting Bayesian prediction that delivers accurate predictions without correctly specifying the unknown true data generating process. A prior is defined over a class...||||https://arxiv.org/abs/1912.12571v1||||econ||||
529||||None||||Blackwell dominance in large samples||||arXiv.org||||2019/08/06||||Blackwell dominance in large samples||||Mu, Xiaosheng || Pomatto, Luciano || Strack, Philipp || Tamuz, Omer||||https://arxiv.org/pdf/1906.02838||||1906.02838||||We study repeated independent Blackwell experiments; standard examples include drawing multiple samples from a population, or performing a measurement in different locations. In the baseline setting of a binary state of nature, we compare experiments in terms of their informativeness in large samples. Addressing a question due to Blackwell (1951) we show that generically, an experiment is more informative than another in large samples if and only if it has higher Renyi divergences. As an application of our techniques we in addition provide a novel characterization of k-th order stochastic dominance as second-order stochastic dominance of large i.i.d. sums.||||@arxiv||||2019/06/06||||Blackwell dominance in large samples||||We study repeated independent Blackwell experiments; standard examples include drawing multiple samples from a population, or performing a measurement in different locations. In the baseline...||||https://arxiv.org/abs/1906.02838v3||||econ||||
530||||None||||Regression Discontinuity Design with Multiple Groups for Heterogeneous Causal Effect Estimation||||arXiv.org||||2019/05/11||||Regression Discontinuity Design with Multiple Groups for Heterogeneous Causal Effect Estimation||||Toda, Takayuki || Wakano, Ayako || Hoshino, Takahiro||||https://arxiv.org/pdf/1905.04443||||1905.04443||||We propose a new estimation method for heterogeneous causal effects which utilizes a regression discontinuity (RD) design for multiple datasets with different thresholds. The standard RD design is frequently used in applied researches, but the result is very limited in that the average treatment effects is estimable only at the threshold on the running variable. In application studies it is often the case that thresholds are different among databases from different regions or firms. For example thresholds for scholarship differ with states. The proposed estimator based on the augmented inverse probability weighted local linear estimator can estimate the average effects at an arbitrary point on the running variable between the thresholds under mild conditions, while the method adjust for the difference of the distributions of covariates among datasets. We perform simulations to investigate the performance of the proposed estimator in the finite samples.||||@arxiv||||2019/05/11||||Regression Discontinuity Design with Multiple Groups for...||||We propose a new estimation method for heterogeneous causal effects which utilizes a regression discontinuity (RD) design for multiple datasets with different thresholds. The standard RD design is...||||https://arxiv.org/abs/1905.04443v1||||econ||||
531||||None||||Preserve or retreat? Willingness-to-pay for Coastline Protection in New South Wales||||arXiv.org||||2019/02/06||||Preserve or retreat? Willingness-to-pay for Coastline Protection in New South Wales||||Ardeshiri, Ali || Swait, Joffre || Heagney, Elizabeth C. || Kovac, Mladen||||https://arxiv.org/pdf/1902.03310||||1902.03310||||Coastal erosion is a global and pervasive phenomenon that predicates a need for a strategic approach to the future management of coastal values and assets (both built and natural), should we invest in protective structures like seawalls that aim to preserve specific coastal features, or allow natural coastline retreat to preserve sandy beaches and other coastal ecosystems. Determining the most suitable management approach in a specific context requires a better understanding of the full suite of economic values the populations holds for coastal assets, including non-market values. In this study, we characterise New South Wales residents willingness to pay to maintain sandy beaches (width and length). We use an innovative application of a Latent Class Binary Logit model to deal with Yea-sayers and Nay-sayers, as well as revealing the latent heterogeneity among sample members. We find that 65% of the population would be willing to pay some amount of levy, dependent on the policy setting. In most cases, there is no effect of degree of beach deterioration characterised as loss of width and length of sandy beaches of between 5% and 100% on respondents willingness to pay for a management levy. This suggests that respondents who agreed to pay a management levy were motivated to preserve sandy beaches in their current state irrespective of the severity of sand loss likely to occur as a result of coastal erosion. Willingness to pay also varies according to beach type (amongst Iconic, Main, Bay and Surf beaches) a finding that can assist with spatial prioritisation of coastal management. Not recognizing the presence of nay-sayers in the data or recognizing them but eliminating them from the estimation will result in biased WTP results and, consequently, biased policy propositions by coastal managers.||||@arxiv||||2019/02/06||||Preserve or retreat? Willingness-to-pay for Coastline Protection...||||Coastal erosion is a global and pervasive phenomenon that predicates a need for a strategic approach to the future management of coastal values and assets (both built and natural), should we...||||https://arxiv.org/abs/1902.03310v1||||econ||||
532||||None||||Dominantly Truthful Multi-task Peer Prediction with a Constant Number of Tasks||||arXiv.org||||2019/11/01||||Dominantly Truthful Multi-task Peer Prediction with a Constant Number of Tasks||||Kong, Yuqing||||https://arxiv.org/pdf/1911.00272||||1911.00272||||In the setting where participants are asked multiple similar possibly subjective multi-choice questions (e.g. Do you like Panda Express? Y/N; do you like Chick-fil-A? Y/N), a series of peer prediction mechanisms are designed to incentivize honest reports and some of them achieve dominantly truthfulness: truth-telling is a dominant strategy and strictly dominate other "non-permutation strategy" with some mild conditions. However, a major issue hinders the practical usage of those mechanisms: they require the participants to perform an infinite number of tasks. When the participants perform a finite number of tasks, these mechanisms only achieve approximated dominant truthfulness. The existence of a dominantly truthful multi-task peer prediction mechanism that only requires a finite number of tasks remains to be an open question that may have a negative result, even with full prior knowledge.   This paper answers this open question by proposing a new mechanism, Determinant based Mutual Information Mechanism (DMI-Mechanism), that is dominantly truthful when the number of tasks is at least 2C and the number of participants is at least 2. C is the number of choices for each question (C=2 for binary-choice questions). In addition to incentivizing honest reports, DMI-Mechanism can also be transferred into an information evaluation rule that identifies high-quality information without verification when there are at least 3 participants. To the best of our knowledge, DMI-Mechanism is the first dominantly truthful mechanism that works for a finite number of tasks, not to say a small constant number of tasks.||||@arxiv||||2019/11/01||||Dominantly Truthful Multi-task Peer Prediction with a Constant...||||In the setting where participants are asked multiple similar possibly subjective multi-choice questions (e.g. Do you like Panda Express? Y/N; do you like Chick-fil-A? Y/N), a series of peer...||||https://arxiv.org/abs/1911.00272v1||||cs||||
533||||None||||Adaptive inference for a semiparametric generalized autoregressive conditional heteroscedastic model||||arXiv.org||||2019/09/16||||Adaptive inference for a semiparametric generalized autoregressive conditional heteroscedastic model||||Jiang, Feiyu || Li, Dong || Zhu, Ke||||https://arxiv.org/pdf/1907.04147||||1907.04147||||This paper considers a semiparametric generalized autoregressive conditional heteroscedastic (S-GARCH) model. For this model, we first estimate the time-varying long run component by the kernel estimator, and then estimate the non-time-varying parameters in short run component by the quasi maximum likelihood estimator (QMLE). We show that the QMLE is asymptotically normal with the parametric convergence rate. Next, we provide a consistent Bayesian information criterion for order selection. Furthermore, we construct a Lagrange multiplier test for linear parameter constraint and a portmanteau test for model checking, and obtain their asymptotic null distributions. Our entire statistical inference procedure works for the non-stationary data with two important features: first, our QMLE and two tests are adaptive to the unknown form of the long run component; second, our QMLE and two tests share the same efficiency and testing power as those in variance target method when the S-GARCH model is stationary.||||@arxiv||||2019/07/09||||Adaptive inference for a semiparametric generalized autoregressive...||||This paper considers a semiparametric generalized autoregressive conditional heteroscedastic (S-GARCH) model. For this model, we first estimate the time-varying long run component by the kernel...||||https://arxiv.org/abs/1907.04147v2||||econ||||
534||||None||||How do governments determine policy priorities? Studying development strategies through spillover networks||||arXiv.org||||2019/02/01||||How do governments determine policy priorities? Studying development strategies through spillover networks||||Guerrero, Omar A. || Castañeda, Gonzalo || Chávez-Juárez, Florian||||https://arxiv.org/pdf/1902.00432||||1902.00432||||Determining policy priorities is a challenging task for any government because there may be, for example, a multiplicity of objectives to be simultaneously attained, a multidimensional policy space to be explored, inefficiencies in the implementation of public policies, interdependencies between policy issues, etc. Altogether, these factor s generate a complex landscape that governments need to navigate in order to reach their goals. To address this problem, we develop a framework to model the evolution of development indicators as a political economy game on a network. Our approach accounts for the --recently documented-- network of spillovers between policy issues, as well as the well-known political economy problem arising from budget assignment. This allows us to infer not only policy priorities, but also the effective use of resources in each policy issue. Using development indicators data from more than 100 countries over 11 years, we show that the country-specific context is a central determinant of the effectiveness of policy priorities. In addition, our model explains well-known aggregate facts about the relationship between corruption and development. Finally, this framework provides a new analytic tool to generate bespoke advice on development strategies.||||@arxiv||||2019/02/01||||How do governments determine policy priorities? Studying...||||Determining policy priorities is a challenging task for any government because there may be, for example, a multiplicity of objectives to be simultaneously attained, a multidimensional policy...||||https://arxiv.org/abs/1902.00432v1||||econ||||
535||||None||||Estimating a Behavioral New Keynesian Model||||arXiv.org||||2019/12/16||||Estimating a Behavioral New Keynesian Model||||Andrade, Joaquim || Cordeiro, Pedro || Lambais, Guilherme||||https://arxiv.org/pdf/1912.07601||||1912.07601||||This paper analyzes identification issues of a behavorial New Keynesian model and estimates it using likelihood-based and limited-information methods with identification-robust confidence sets. The model presents some of the same difficulties that exist in simple benchmark DSGE models, but the analytical solution is able to indicate in what conditions the cognitive discounting parameter (attention to the future) can be identified and the robust estimation methods is able to confirm its importance for explaining the proposed behavioral model.||||@arxiv||||2019/12/16||||Estimating a Behavioral New Keynesian Model||||This paper analyzes identification issues of a behavorial New Keynesian model and estimates it using likelihood-based and limited-information methods with identification-robust confidence sets....||||https://arxiv.org/abs/1912.07601v1||||econ||||
536||||None||||The structure of the environment and the complexity of rational choice procedures||||arXiv.org||||2018/09/25||||The structure of the environment and the complexity of rational choice procedures||||Oliva, Paulo || Zahn, Philipp||||https://arxiv.org/pdf/1809.06766||||1809.06766||||Beginning with Herbert Simon [10], the literature on bounded rationality has investigated in great detail how internal limitations affect an agent's choice process. The structure of the choice environment, deemed as important as internal limitations by Simon [11], has been mostly ignored. We introduce a model of the environment and its interaction with an agent's choice process. Focusing on online environments where an agent can use filter and sort functionality to support his decision-making, we show, a choice process relying on the environment can be rationalized. Moreover, for sufficiently many alternatives, filtering and sorting are quick ways to choose rationally.||||@arxiv||||2018/09/18||||The structure of the environment and the complexity of rational...||||Beginning with Herbert Simon [10], the literature on bounded rationality has investigated in great detail how internal limitations affect an agent's choice process. The structure of the choice...||||https://arxiv.org/abs/1809.06766v2||||econ||||
537||||None||||Nonparametric Quantile Regressions for Panel Data Models with Large T||||arXiv.org||||2019/11/05||||Nonparametric Quantile Regressions for Panel Data Models with Large T||||Chen, Liang||||https://arxiv.org/pdf/1911.01824||||1911.01824||||This paper considers panel data models where the conditional quantiles of the dependent variables are additively separable as unknown functions of the regressors and the individual effects. We propose two estimators of the quantile partial effects while controlling for the individual heterogeneity. The first estimator is based on local linear quantile regressions, and the second is based on local linear smoothed quantile regressions, both of which are easy to compute in practice. Within the large T framework, we provide sufficient conditions under which the two estimators are shown to be asymptotically normally distributed. In particular, for the first estimator, it is shown that $N<<T^{2/(d+4)}$ is needed to ignore the incidental parameter biases, where $d$ is the dimension of the regressors. For the second estimator, we are able to derive the analytical expression of the asymptotic biases under the assumption that $N\approx Th^{d}$, where $h$ is the bandwidth parameter in local linear approximations. Our theoretical results provide the basis of using split-panel jackknife for bias corrections. A Monte Carlo simulation shows that the proposed estimators and the bias-correction method perform well in finite samples.||||@arxiv||||2019/11/05||||Nonparametric Quantile Regressions for Panel Data Models with Large T||||This paper considers panel data models where the conditional quantiles of the dependent variables are additively separable as unknown functions of the regressors and the individual effects. We...||||https://arxiv.org/abs/1911.01824v1||||econ||||
538||||None||||Theories and Practice of Agent based Modeling: Some practical Implications for Economic Planners||||arXiv.org||||2019/01/23||||Theories and Practice of Agent based Modeling: Some practical Implications for Economic Planners||||Sabzian, Hossein || Shafia, Mohammad Ali || Maleki, Ali || Hashemi, Seyeed Mostapha Seyeed || Baghaei, Ali || Gharib, Hossein||||https://arxiv.org/pdf/1901.08932||||1901.08932||||Nowadays, we are surrounded by a large number of complex phenomena ranging from rumor spreading, social norms formation to rise of new economic trends and disruption of traditional businesses. To deal with such phenomena,Complex Adaptive System (CAS) framework has been found very influential among social scientists,especially economists. As the most powerful methodology of CAS modeling, Agent-based modeling (ABM) has gained a growing application among academicians and practitioners. ABMs show how simple behavioral rules of agents and local interactions among them at micro-scale can generate surprisingly complex patterns at macro-scale. Despite a growing number of ABM publications, those researchers unfamiliar with this methodology have to study a number of works to understand (1) the why and what of ABMs and (2) the ways they are rigorously developed. Therefore, the major focus of this paper is to help social sciences researchers,especially economists get a big picture of ABMs and know how to develop them both systematically and rigorously.||||@arxiv||||2019/01/23||||Theories and Practice of Agent based Modeling: Some practical...||||Nowadays, we are surrounded by a large number of complex phenomena ranging from rumor spreading, social norms formation to rise of new economic trends and disruption of traditional businesses. To...||||https://arxiv.org/abs/1901.08932v1||||econ||||
539||||None||||Estimating Network Effects Using Naturally Occurring Peer Notification Queue Counterfactuals||||arXiv.org||||2019/02/19||||Estimating Network Effects Using Naturally Occurring Peer Notification Queue Counterfactuals||||Tutterow, Craig || Saint-Jacques, Guillaume||||https://arxiv.org/pdf/1902.07133||||1902.07133||||Randomized experiments, or A/B tests are used to estimate the causal impact of a feature on the behavior of users by creating two parallel universes in which members are simultaneously assigned to treatment and control. However, in social network settings, members interact, such that the impact of a feature is not always contained within the treatment group. Researchers have developed a number of experimental designs to estimate network effects in social settings. Alternatively, naturally occurring exogenous variation, or 'natural experiments,' allow researchers to recover causal estimates of peer effects from observational data in the absence of experimental manipulation. Natural experiments trade off the engineering costs and some of the ethical concerns associated with network randomization with the search costs of finding situations with natural exogenous variation. To mitigate the search costs associated with discovering natural counterfactuals, we identify a common engineering requirement used to scale massive online systems, in which natural exogenous variation is likely to exist: notification queueing. We identify two natural experiments on the LinkedIn platform based on the order of notification queues to estimate the causal impact of a received message on the engagement of a recipient. We show that receiving a message from another member significantly increases a member's engagement, but that some popular observational specifications, such as fixed-effects estimators, overestimate this effect by as much as 2.7x. We then apply the estimated network effect coefficients to a large body of past experiments to quantify the extent to which it changes our interpretation of experimental results. The study points to the benefits of using messaging queues to discover naturally occurring counterfactuals for the estimation of causal effects without experimenter intervention.||||@arxiv||||2019/02/19||||Estimating Network Effects Using Naturally Occurring Peer...||||Randomized experiments, or A/B tests are used to estimate the causal impact of a feature on the behavior of users by creating two parallel universes in which members are simultaneously assigned to...||||https://arxiv.org/abs/1902.07133v1||||cs||||
540||||None||||How does stock market volatility react to oil shocks?||||arXiv.org||||2018/11/09||||How does stock market volatility react to oil shocks?||||Bastianin, Andrea || Manera, Matteo||||https://arxiv.org/pdf/1811.03820||||1811.03820||||We study the impact of oil price shocks on the U.S. stock market volatility. We jointly analyze three different structural oil market shocks (i.e., aggregate demand, oil supply, and oil-specific demand shocks) and stock market volatility using a structural vector autoregressive model. Identification is achieved by assuming that the price of crude oil reacts to stock market volatility only with delay. This implies that innovations to the price of crude oil are not strictly exogenous, but predetermined with respect to the stock market. We show that volatility responds significantly to oil price shocks caused by unexpected changes in aggregate and oil-specific demand, whereas the impact of supply-side shocks is negligible.||||@arxiv||||2018/11/09||||How does stock market volatility react to oil shocks?||||We study the impact of oil price shocks on the U.S. stock market volatility. We jointly analyze three different structural oil market shocks (i.e., aggregate demand, oil supply, and oil-specific...||||https://arxiv.org/abs/1811.03820v1||||econ||||
541||||None||||Shrinkage in the Time-Varying Parameter Model Framework Using the R Package shrinkTVP||||arXiv.org||||2019/10/07||||Shrinkage in the Time-Varying Parameter Model Framework Using the R Package shrinkTVP||||Bitto-Nemling, Angela || Cadonna, Annalisa || Frühwirth-Schnatter, Sylvia || Knaus, Peter||||https://arxiv.org/pdf/1907.07065||||1907.07065||||Time-varying parameter (TVP) models are widely used in time series analysis to flexibly deal with processes which gradually change over time. However, the risk of overfitting in TVP models is well known. This issue can be dealt with using appropriate global-local shrinkage priors, which pull time-varying parameters towards static ones. In this paper, we introduce the R package shrinkTVP (Knaus, Bitto-Nemling, Cadonna, and Frühwirth-Schnatter 2019), which provides a fully Bayesian implementation of shrinkage priors for TVP models, taking advantage of recent developments in the literature, in particular that of Bitto and Frühwirth-Schnatter (2019). The package shrinkTVP allows for posterior simulation of the parameters through an efficient Markov Chain Monte Carlo (MCMC) scheme. Moreover, summary and visualization methods, as well as the possibility of assessing predictive performance through log predictive density scores (LPDSs), are provided. The computationally intensive tasks have been implemented in C++ and interfaced with R. The paper includes a brief overview of the models and shrinkage priors implemented in the package. Furthermore, core functionalities are illustrated, both with simulated and real data.||||@arxiv||||2019/07/16||||Shrinkage in the Time-Varying Parameter Model Framework Using the...||||Time-varying parameter (TVP) models are widely used in time series analysis to flexibly deal with processes which gradually change over time. However, the risk of overfitting in TVP models is well...||||https://arxiv.org/abs/1907.07065v2||||econ||||
542||||None||||Does strategic commodities price respond to U.S. Partisan Conflict? Evidence from a parametric test of Granger causality in quantiles||||arXiv.org||||2018/10/19||||Does strategic commodities price respond to U.S. Partisan Conflict? Evidence from a parametric test of Granger causality in quantiles||||Jiang, Yong||||https://arxiv.org/pdf/1810.08396||||1810.08396||||Currently, U.S. politics have been characterized by a high degree of partisan conflict, which has led to increasing polarization and high policy uncertainty. Given the importance of U.S. in the global commodity market, we investigate whether U.S. partisan conflict affects the price performance (returns and volatility) of two strategic commodities (oil and gold). To this end, we employ a parametric test of Granger causality in quantiles proposed by Troster (2016), which can discriminate between causality affecting the median and the tails of the conditional distribution. Meanwhile, this approach allows us to investigate whether there exist different effects of U.S. partisan conflict index on the oil market and gold market under different market conditions. The empirical results suggest that U.S. partisan conflict can affect the returns of oil and gold, with the effects cluster around the tail of the conditional distribution of returns. More specifically, the partisan conflict mainly affects the oil returns when the crude oil market is in a bearish state (lower quantiles). By contrast, partisan conflict matters for gold returns only when the gold market is in a bullish scenario (higher quantiles). In addition, for the volatility of oil and gold, the predictability of partisan conflict index virtually covers the entire distribution of volatility. This study provides valuable implications for academics, policymakers, and investors.||||@arxiv||||2018/10/19||||Does strategic commodities price respond to U.S. Partisan...||||Currently, U.S. politics have been characterized by a high degree of partisan conflict, which has led to increasing polarization and high policy uncertainty. Given the importance of U.S. in the...||||https://arxiv.org/abs/1810.08396v1||||econ||||
543||||None||||Modular structure in labour networks reveals skill basins||||arXiv.org||||2019/09/08||||Modular structure in labour networks reveals skill basins||||O'Clery, Neave || Flaherty, Eoin || Kinsella, Stephen||||https://arxiv.org/pdf/1909.03379||||1909.03379||||Labour networks, where industries are connected based on worker transitions, have been previously deployed to study the evolution of industrial structure ('related diversification') across cities and regions. Beyond estimating skill-overlap between industry pairs, such networks characterise the structure of inter-industry labour mobility and knowledge diffusion in an economy. Here we investigate the structure of the network of inter-industry worker flows in the Irish economy, seeking to identify groups of industries exhibiting high internal mobility and skill-overlap. We argue that these industry clusters represent skill basins in which skilled labour circulate and diffuse knowledge, and delineate the size of the skilled labour force available to an industry.   Deploying a multi-scale community detection algorithm, we uncover a hierarchical modular structure composed of clusters of industries at different scales. At one end of the scale, we observe a macro division of the economy into services and manufacturing. At the other end of the scale, we detect a fine-grained partition of industries into tightly knit groupings. In particular, we find workers from finance, computing, and the public sector rarely transition into the extended economy. Hence, these industries form isolated clusters which are disconnected from the broader economy, posing a range of risks to both workers and firms. Finally, we develop a methodology based on industry growth patterns to reveal the optimal scale at which labour pooling operates in terms of skill-sharing and skill-seeking within industry clusters.||||@arxiv||||2019/09/08||||Modular structure in labour networks reveals skill basins||||Labour networks, where industries are connected based on worker transitions, have been previously deployed to study the evolution of industrial structure ('related diversification') across cities...||||https://arxiv.org/abs/1909.03379v1||||econ||||
544||||None||||Knowledge and Unanimous Acceptance of Core Payoffs: An Epistemic Foundation for Cooperative Game Theory||||arXiv.org||||2019/01/02||||Knowledge and Unanimous Acceptance of Core Payoffs: An Epistemic Foundation for Cooperative Game Theory||||Liu, Shuige||||https://arxiv.org/pdf/1802.04595||||1802.04595||||We provide an epistemic foundation for cooperative games by proof theory via studying the knowledge for players unanimously accepting only core payoffs. We first transform each cooperative game into a decision problem where a player can accept or reject any payoff vector offered to her based on her knowledge about available cooperation. Then we use a modified KD-system in epistemic logic, which can be regarded as a counterpart of the model for non-cooperative games in Bonanno (2008), (2015), to describe a player's knowledge, decision-making criterion, and reasoning process; especially, a formula called C-acceptability is defined to capture the criterion for accepting a core payoff vector. Within this syntactical framework, we characterize the core of a cooperative game in terms of players' knowledge. Based on that result, we discuss an epistemic inconsistency behind Debreu-Scarf Theorem, that is, the increase of the number of replicas has invariant requirement on each participant's knowledge from the aspect of competitive market, while requires unbounded epistemic ability players from the aspect of cooperative game.||||@arxiv||||2018/02/13||||Knowledge and Unanimous Acceptance of Core Payoffs: An Epistemic...||||We provide an epistemic foundation for cooperative games by proof theory via studying the knowledge for players unanimously accepting only core payoffs. We first transform each cooperative game...||||https://arxiv.org/abs/1802.04595v4||||econ||||
545||||None||||Evaluating Pest Management Strategies: A Robust Method and its Application to Strawberry Disease Management||||arXiv.org||||2019/08/05||||Evaluating Pest Management Strategies: A Robust Method and its Application to Strawberry Disease Management||||Soto-Caro, Ariel || Wu, Feng || Guan, Zhengfei||||https://arxiv.org/pdf/1908.01808||||1908.01808||||Farmers use pesticides to reduce yield losses. The efficacies of pesticide treatments are often evaluated by analyzing the average treatment effects and risks. The stochastic efficiency with respect to a function is often employed in such evaluations through ranking the certainty equivalents of each treatment. The main challenge of using this method is gathering an adequate number of observations to produce results with statistical power. However, in many cases, only a limited number of trials are replicated in field experiments, leaving an inadequate number of observations. In addition, this method focuses only on the farmer's profit without incorporating the impact of disease pressure on yield and profit. The objective of our study is to propose a methodology to address the issue of an insufficient number of observations using simulations and take into account the effect of disease pressure on yield through a quantile regression model. We apply this method to the case of strawberry disease management in Florida.||||@arxiv||||2019/08/05||||Evaluating Pest Management Strategies: A Robust Method and its...||||Farmers use pesticides to reduce yield losses. The efficacies of pesticide treatments are often evaluated by analyzing the average treatment effects and risks. The stochastic efficiency with...||||https://arxiv.org/abs/1908.01808v1||||econ||||
546||||None||||Why are prices proportional to embodied energies?||||arXiv.org||||2018/11/29||||Why are prices proportional to embodied energies?||||Leiva, Benjamin||||https://arxiv.org/pdf/1811.12502||||1811.12502||||The observed proportionality between nominal prices and average embodied energies cannot be interpreted with conventional economic theory. A model is presented that places energy transfers as the focal point of scarcity based on the idea that (1) goods are material rearrangements, and (2) humans can only rearrange matter with energy transfers. Modified consumer and producer problems for an autarkic agent show that the opportunity cost of goods are given by their marginal energy transfers, which depend on subjective and objective factors (e.g. consumer preferences and direct energy transfers). Allowing for exchange and under perfect competition, nominal prices arise as social manifestations of goods' marginal energy transfers. The proportionality between nominal prices and average embodied energy follows given the relation between the latter and marginal energy transfers.||||@arxiv||||2018/11/29||||Why are prices proportional to embodied energies?||||The observed proportionality between nominal prices and average embodied energies cannot be interpreted with conventional economic theory. A model is presented that places energy transfers as the...||||https://arxiv.org/abs/1811.12502v1||||econ||||
547||||None||||A Flexible Mixed-Frequency Vector Autoregression with a Steady-State Prior||||arXiv.org||||2019/11/20||||A Flexible Mixed-Frequency Vector Autoregression with a Steady-State Prior||||Ankargren, Sebastian || Unosson, Måns || Yang, Yukai||||https://arxiv.org/pdf/1911.09151||||1911.09151||||We propose a Bayesian vector autoregressive (VAR) model for mixed-frequency data. Our model is based on the mean-adjusted parametrization of the VAR and allows for an explicit prior on the 'steady states' (unconditional means) of the included variables. Based on recent developments in the literature, we discuss extensions of the model that improve the flexibility of the modeling approach. These extensions include a hierarchical shrinkage prior for the steady-state parameters, and the use of stochastic volatility to model heteroskedasticity. We put the proposed model to use in a forecast evaluation using US data consisting of 10 monthly and 3 quarterly variables. The results show that the predictive ability typically benefits from using mixed-frequency data, and that improvements can be obtained for both monthly and quarterly variables. We also find that the steady-state prior generally enhances the accuracy of the forecasts, and that accounting for heteroskedasticity by means of stochastic volatility usually provides additional improvements, although not for all variables.||||@arxiv||||2019/11/20||||A Flexible Mixed-Frequency Vector Autoregression with a Steady-State Prior||||We propose a Bayesian vector autoregressive (VAR) model for mixed-frequency data. Our model is based on the mean-adjusted parametrization of the VAR and allows for an explicit prior on the 'steady...||||https://arxiv.org/abs/1911.09151v1||||econ||||
548||||None||||The Regression Discontinuity Design||||arXiv.org||||2019/06/10||||The Regression Discontinuity Design||||Cattaneo, Matias D. || Titiunik, Rocio || Vazquez-Bare, Gonzalo||||https://arxiv.org/pdf/1906.04242||||1906.04242||||This handbook chapter gives an introduction to the sharp regression discontinuity design, covering identification, estimation, inference, and falsification methods.||||@arxiv||||2019/06/10||||The Regression Discontinuity Design||||This handbook chapter gives an introduction to the sharp regression discontinuity design, covering identification, estimation, inference, and falsification methods.||||https://arxiv.org/abs/1906.04242v1||||econ||||
549||||None||||Estimating a Large Covariance Matrix in Time-varying Factor Models||||arXiv.org||||2019/10/26||||Estimating a Large Covariance Matrix in Time-varying Factor Models||||Jung, Jaeheon||||https://arxiv.org/pdf/1910.11965||||1910.11965||||This paper deals with the time-varying high dimensional covariance matrix estimation. We propose two covariance matrix estimators corresponding with a time-varying approximate factor model and a time-varying approximate characteristic-based factor model, respectively. The models allow the factor loadings, factor covariance matrix, and error covariance matrix to change smoothly over time. We study the rate of convergence of each estimator. Our simulation and empirical study indicate that time-varying covariance matrix estimators generally perform better than time-invariant covariance matrix estimators. Also, if characteristics are available that genuinely explain true loadings, the characteristics can be used to estimate loadings more precisely in finite samples; their helpfulness increases when loadings rapidly change.||||@arxiv||||2019/10/26||||Estimating a Large Covariance Matrix in Time-varying Factor Models||||This paper deals with the time-varying high dimensional covariance matrix estimation. We propose two covariance matrix estimators corresponding with a time-varying approximate factor model and a...||||https://arxiv.org/abs/1910.11965v1||||econ||||
550||||None||||Deep Neural Networks for Choice Analysis: Extracting Complete Economic Information for Interpretation||||arXiv.org||||2019/09/16||||Deep Neural Networks for Choice Analysis: Extracting Complete Economic Information for Interpretation||||Wang, Shenhao || Wang, Qingyi || Zhao, Jinhua||||https://arxiv.org/pdf/1812.04528||||1812.04528||||While deep neural networks (DNNs) have been increasingly applied to choice analysis showing high predictive power, it is unclear to what extent researchers can interpret economic information from DNNs. This paper demonstrates that DNNs can provide economic information as complete as classical discrete choice models (DCMs). The economic information includes choice predictions, choice probabilities, market shares, substitution patterns of alternatives, social welfare, probability derivatives, elasticities, marginal rates of substitution (MRS), and heterogeneous values of time (VOT). Unlike DCMs, DNNs can automatically learn the utility function and reveal behavioral patterns that are not prespecified by domain experts. However, the economic information obtained from DNNs can be unreliable because of the three challenges associated with the automatic learning capacity: high sensitivity to hyperparameters, model non-identification, and local irregularity. To demonstrate the strength and challenges of DNNs, we estimated the DNNs using a stated preference survey, extracted the full list of economic information from the DNNs, and compared them with those from the DCMs. We found that the economic information either aggregated over trainings or population is more reliable than the disaggregate information of the individual observations or trainings, and that even simple hyperparameter searching can significantly improve the reliability of the economic information extracted from the DNNs. Future studies should investigate other regularizations and DNN architectures, better optimization algorithms, and robust DNN training methods to address DNNs' three challenges, to provide more reliable economic information from DNN-based choice models.||||@arxiv||||2018/12/11||||Deep Neural Networks for Choice Analysis: Extracting Complete...||||While deep neural networks (DNNs) have been increasingly applied to choice analysis showing high predictive power, it is unclear to what extent researchers can interpret economic information from...||||https://arxiv.org/abs/1812.04528v2||||cs||||
551||||None||||Normal Approximation in Large Network Models||||arXiv.org||||2019/04/24||||Normal Approximation in Large Network Models||||Leung, Michael P. || Moon, Hyungsik Roger||||https://arxiv.org/pdf/1904.11060||||1904.11060||||We prove central limit theorems for models of network formation and network processes with homophilous agents. The results hold under large-network asymptotics, enabling inference in the typical setting where the sample consists of a small set of large networks. We first establish a general central limit theorem under high-level `stabilization' conditions that provide a useful formulation of weak dependence, particularly in models with strategic interactions. The result delivers a square root n rate of convergence and a closed-form expression for the asymptotic variance. Then using techniques in branching process theory, we derive primitive conditions for stabilization in the following applications: static and dynamic models of strategic network formation, network regressions, and treatment effects with network spillovers. Finally, we suggest some practical methods for inference.||||@arxiv||||2019/04/24||||Normal Approximation in Large Network Models||||We prove central limit theorems for models of network formation and network processes with homophilous agents. The results hold under large-network asymptotics, enabling inference in the typical...||||https://arxiv.org/abs/1904.11060v1||||econ||||
552||||None||||A Factor-Augmented Markov Switching (FAMS) Model||||arXiv.org||||2019/05/03||||A Factor-Augmented Markov Switching (FAMS) Model||||Zens, Gregor || Böck, Maximilian||||https://arxiv.org/pdf/1904.13194||||1904.13194||||This paper investigates the role of high-dimensional information sets in the context of Markov switching models with time varying transition probabilities. Markov switching models are commonly employed in empirical macroeconomic research and policy work. However, the information used to model the switching process is usually limited drastically to ensure stability of the model. Increasing the number of included variables to enlarge the information set might even result in decreasing precision of the model. Moreover, it is often not clear a priori which variables are actually relevant when it comes to informing the switching behavior. Building strongly on recent contributions in the field of factor analysis, we introduce a general type of Markov switching autoregressive models for non-linear time series analysis. Large numbers of time series are allowed to inform the switching process through a factor structure. This factor-augmented Markov switching (FAMS) model overcomes estimation issues that are likely to arise in previous assessments of the modeling framework. More accurate estimates of the switching behavior as well as improved model fit result. The performance of the FAMS model is illustrated in a simulated data example as well as in an US business cycle application.||||@arxiv||||2019/04/30||||A Factor-Augmented Markov Switching (FAMS) Model||||This paper investigates the role of high-dimensional information sets in the context of Markov switching models with time varying transition probabilities. Markov switching models are commonly...||||https://arxiv.org/abs/1904.13194v2||||econ||||
553||||None||||Dynamic Assortment Optimization with Changing Contextual Information||||arXiv.org||||2019/01/18||||Dynamic Assortment Optimization with Changing Contextual Information||||Chen, Xi || Wang, Yining || Zhou, Yuan||||https://arxiv.org/pdf/1810.13069||||1810.13069||||In this paper, we study the dynamic assortment optimization problem under a finite selling season of length $T$. At each time period, the seller offers an arriving customer an assortment of substitutable products under a cardinality constraint, and the customer makes the purchase among offered products according to a discrete choice model. Most existing work associates each product with a real-valued fixed mean utility and assumes a multinomial logit choice (MNL) model. In many practical applications, feature/contexutal information of products is readily available. In this paper, we incorporate the feature information by assuming a linear relationship between the mean utility and the feature. In addition, we allow the feature information of products to change over time so that the underlying choice model can also be non-stationary. To solve the dynamic assortment optimization under this changing contextual MNL model, we need to simultaneously learn the underlying unknown coefficient and makes the decision on the assortment. To this end, we develop an upper confidence bound (UCB) based policy and establish the regret bound on the order of $\widetilde O(d\sqrt{T})$, where $d$ is the dimension of the feature and $\widetilde O$ suppresses logarithmic dependence. We further established the lower bound $Ω(d\sqrt{T}/K)$ where $K$ is the cardinality constraint of an offered assortment, which is usually small. When $K$ is a constant, our policy is optimal up to logarithmic factors. In the exploitation phase of the UCB algorithm, we need to solve a combinatorial optimization for assortment optimization based on the learned information. We further develop an approximation algorithm and an efficient greedy heuristic. The effectiveness of the proposed policy is further demonstrated by our numerical studies.||||@arxiv||||2018/10/31||||Dynamic Assortment Optimization with Changing Contextual Information||||In this paper, we study the dynamic assortment optimization problem under a finite selling season of length $T$. At each time period, the seller offers an arriving customer an assortment of...||||https://arxiv.org/abs/1810.13069v2||||cs||||
554||||None||||Group Average Treatment Effects for Observational Studies||||arXiv.org||||2019/12/20||||Group Average Treatment Effects for Observational Studies||||Jacob, Daniel || Härdle, Wolfgang Karl || Lessmann, Stefan||||https://arxiv.org/pdf/1911.02688||||1911.02688||||The paper proposes an estimator to make inference of heterogeneous treatment effects sorted by impact groups (GATES) for non-randomised experiments. Observational studies are standard in policy evaluation from labour markets, educational surveys and other empirical studies. To control for a potential selection-bias we implement a doubly-robust estimator in the first stage. Keeping the flexibility, we can use any machine learning method to learn the conditional mean functions as well as the propensity score. We also use machine learning methods to learn a function for the conditional average treatment effect. The group average treatment effect, is then estimated via a parametric linear model to provide p-values and confidence intervals. To control for confounding in the linear model we use Neyman-orthogonal moments to partial out the effect that covariates have on both, the treatment assignment and the outcome. The result is a best linear predictor for effect heterogeneity based on impact groups. We introduce inclusion-probability weighting as a form of cross-splitting and averaging for each observation to avoid biases through sample splitting. The advantage of the proposed method is a robust linear estimation of heterogeneous group treatment effects in observational studies.||||@arxiv||||2019/11/07||||Group Average Treatment Effects for Observational Studies||||The paper proposes an estimator to make inference of heterogeneous treatment effects sorted by impact groups (GATES) for non-randomised experiments. Observational studies are standard in policy...||||https://arxiv.org/abs/1911.02688v4||||econ||||
555||||None||||Pricing Engine: Estimating Causal Impacts in Real World Business Settings||||arXiv.org||||2018/06/12||||Pricing Engine: Estimating Causal Impacts in Real World Business Settings||||Goldman, Matt || Quistorff, Brian||||https://arxiv.org/pdf/1806.03285||||1806.03285||||We introduce the Pricing Engine package to enable the use of Double ML estimation techniques in general panel data settings. Customization allows the user to specify first-stage models, first-stage featurization, second stage treatment selection and second stage causal-modeling. We also introduce a DynamicDML class that allows the user to generate dynamic treatment-aware forecasts at a range of leads and to understand how the forecasts will vary as a function of causally estimated treatment parameters. The Pricing Engine is built on Python 3.5 and can be run on an Azure ML Workbench environment with the addition of only a few Python packages. This note provides high-level discussion of the Double ML method, describes the packages intended use and includes an example Jupyter notebook demonstrating application to some publicly available data. Installation of the package and additional technical documentation is available at $\href{https://github.com/bquistorff/pricingengine}{github.com/bquistorff/pricingengine}$.||||@arxiv||||2018/06/08||||Pricing Engine: Estimating Causal Impacts in Real World Business Settings||||We introduce the Pricing Engine package to enable the use of Double ML estimation techniques in general panel data settings. Customization allows the user to specify first-stage models,...||||https://arxiv.org/abs/1806.03285v2||||econ||||
556||||None||||Principled estimation of regression discontinuity designs with covariates: a machine learning approach||||arXiv.org||||2019/10/14||||Principled estimation of regression discontinuity designs with covariates: a machine learning approach||||Anastasopoulos, Jason||||https://arxiv.org/pdf/1910.06381||||1910.06381||||The regression discontinuity design (RDD) has become the "gold standard" for causal inference with observational data. Local average treatment effects (LATE) for RDDs are often estimated using local linear regressions with pre-treatment covariates typically added to increase the efficiency of treatment effect estimates, but their inclusion can have large impacts on LATE point estimates and standard errors, particularly in small samples. In this paper, I propose a principled, efficiency-maximizing approach for covariate adjustment of LATE in RDDs. This approach allows researchers to combine context-specific, substantive insights with automated model selection via a novel adaptive lasso algorithm. When combined with currently existing robust estimation methods, this approach improves the efficiency of LATE RDD with pre-treatment covariates. The approach will be implemented in a forthcoming R package, AdaptiveRDD which can be used to estimate and compare treatment effects generated by this approach with extant approaches.||||@arxiv||||2019/10/14||||Principled estimation of regression discontinuity designs with...||||The regression discontinuity design (RDD) has become the "gold standard" for causal inference with observational data. Local average treatment effects (LATE) for RDDs are often estimated using...||||https://arxiv.org/abs/1910.06381v1||||econ||||
557||||None||||What Is the Value Added by Using Causal Machine Learning Methods in a Welfare Experiment Evaluation?||||arXiv.org||||2019/03/16||||What Is the Value Added by Using Causal Machine Learning Methods in a Welfare Experiment Evaluation?||||Strittmatter, Anthony||||https://arxiv.org/pdf/1812.06533||||1812.06533||||Recent studies have proposed causal machine learning (CML) methods to estimate conditional average treatment effects (CATEs). In this study, I investigate whether CML methods add value compared to conventional CATE estimators by re-evaluating Connecticut's Jobs First welfare experiment. This experiment entails a mix of positive and negative work incentives. Previous studies show that it is hard to tackle the effect heterogeneity of Jobs First by means of CATEs. I report evidence that CML methods can provide support for the theoretical labor supply predictions. Furthermore, I document reasons why some conventional CATE estimators fail and discuss the limitations of CML methods.||||@arxiv||||2018/12/16||||What Is the Value Added by Using Causal Machine Learning Methods...||||Recent studies have proposed causal machine learning (CML) methods to estimate conditional average treatment effects (CATEs). In this study, I investigate whether CML methods add value compared to...||||https://arxiv.org/abs/1812.06533v2||||econ||||
558||||None||||Third-degree Price Discrimination Versus Uniform Pricing||||arXiv.org||||2019/12/11||||Third-degree Price Discrimination Versus Uniform Pricing||||Bergemann, Dirk || Castro, Francisco || Weintraub, Gabriel||||https://arxiv.org/pdf/1912.05164||||1912.05164||||We compare the revenue of the optimal third-degree price discrimination policy against a uniform pricing policy. A uniform pricing policy offers the same price to all segments of the market. Our main result establishes that for a broad class of third-degree price discrimination problems with concave revenue functions and common support, a uniform price is guaranteed to achieve one half of the optimal monopoly profits. This revenue bound obtains for any arbitrary number of segments and prices that the seller would use in case he would engage in third-degree price discrimination. We further establish that these conditions are tight, and that a weakening of common support or concavity leads to arbitrarily poor revenue comparisons.||||@arxiv||||2019/12/11||||Third-degree Price Discrimination Versus Uniform Pricing||||We compare the revenue of the optimal third-degree price discrimination policy against a uniform pricing policy. A uniform pricing policy offers the same price to all segments of the market. Our...||||https://arxiv.org/abs/1912.05164v1||||cs||||
559||||None||||Quantitative analysis on the disparity of regional economic development in China and its evolution from 1952 to 2000||||arXiv.org||||2018/06/28||||Quantitative analysis on the disparity of regional economic development in China and its evolution from 1952 to 2000||||Xu, Jianhua || Ai, Nanshan || Lu, Yan || Chen, Yong || Ling, Yiying || Yue, Wenze||||https://arxiv.org/pdf/1806.10794||||1806.10794||||Domestic and foreign scholars have already done much research on regional disparity and its evolution in China, but there is a big difference in conclusions. What is the reason for this? We think it is mainly due to different analytic approaches, perspectives, spatial units, statistical indicators and different periods for studies. On the basis of previous analyses and findings, we have done some further quantitative computation and empirical study, and revealed the inter-provincial disparity and regional disparity of economic development and their evolution trends from 1952-2000. The results shows that (a) Regional disparity in economic development in China, including the inter-provincial disparity, inter-regional disparity and intra-regional disparity, has existed for years; (b) Gini coefficient and Theil coefficient have revealed a similar dynamic trend for comparative disparity in economic development between provinces in China. From 1952 to 1978, except for the "Great Leap Forward" period, comparative disparity basically assumes a upward trend and it assumed a slowly downward trend from 1979 to1990. Afterwards from 1991 to 2000 the disparity assumed a slowly upward trend again; (c) A comparison between Shanghai and Guizhou shows that absolute inter-provincial disparity has been quite big for years; and (d) The Hurst exponent (H=0.5) in the period of 1966-1978 indicates that the comparative inter-provincial disparity of economic development showed a random characteristic, and in the Hurst exponent (H>0.5) in period of 1979-2000 indicates that in this period the evolution of the comparative inter-provincial disparity of economic development in China has a long-enduring characteristic.||||@arxiv||||2018/06/28||||Quantitative analysis on the disparity of regional economic...||||Domestic and foreign scholars have already done much research on regional disparity and its evolution in China, but there is a big difference in conclusions. What is the reason for this? We think...||||https://arxiv.org/abs/1806.10794v1||||econ||||
560||||None||||Inference on weighted average value function in high-dimensional state space||||arXiv.org||||2019/08/24||||Inference on weighted average value function in high-dimensional state space||||Chernozhukov, Victor || Newey, Whitney || Semenova, Vira||||https://arxiv.org/pdf/1908.09173||||1908.09173||||This paper gives a consistent, asymptotically normal estimator of the expected value function when the state space is high-dimensional and the first-stage nuisance functions are estimated by modern machine learning tools. First, we show that value function is orthogonal to the conditional choice probability, therefore, this nuisance function needs to be estimated only at $n^{-1/4}$ rate. Second, we give a correction term for the transition density of the state variable. The resulting orthogonal moment is robust to misspecification of the transition density and does not require this nuisance function to be consistently estimated. Third, we generalize this result by considering the weighted expected value. In this case, the orthogonal moment is doubly robust in the transition density and additional second-stage nuisance functions entering the correction term. We complete the asymptotic theory by providing bounds on second-order asymptotic terms.||||@arxiv||||2019/08/24||||Inference on weighted average value function in high-dimensional...||||This paper gives a consistent, asymptotically normal estimator of the expected value function when the state space is high-dimensional and the first-stage nuisance functions are estimated by...||||https://arxiv.org/abs/1908.09173v1||||cs||||
561||||None||||The Use of Binary Choice Forests to Model and Estimate Discrete Choices||||arXiv.org||||2019/11/14||||The Use of Binary Choice Forests to Model and Estimate Discrete Choices||||Chen, Ningyuan || Gallego, Guillermo || Tang, Zhuodong||||https://arxiv.org/pdf/1908.01109||||1908.01109||||We show the equivalence of discrete choice models and the class of binary choice forests, which are random forests based on binary choice trees. This suggests that standard machine learning techniques based on random forests can serve to estimate discrete choice models with an interpretable output. This is confirmed by our data-driven theoretical results which show that random forests can predict the choice probability of any discrete choice model consistently, with its splitting criterion capable of recovering preference rank lists. The framework has unique advantages: it can capture behavioral patterns such as irrationality or sequential searches; it handles nonstandard formats of training data that result from aggregation; it can measure product importance based on how frequently a random customer would make decisions depending on the presence of the product; it can also incorporate price information and customer features. Our numerical results show that using random forests to estimate customer choices represented by binary choice forests can outperform the best parametric models in synthetic and real datasets.||||@arxiv||||2019/08/03||||The Use of Binary Choice Forests to Model and Estimate Discrete Choices||||We show the equivalence of discrete choice models and the class of binary choice forests, which are random forests based on binary choice trees. This suggests that standard machine learning...||||https://arxiv.org/abs/1908.01109v3||||cs||||
562||||None||||Estimation of Auction Models with Shape Restrictions||||arXiv.org||||2019/12/16||||Estimation of Auction Models with Shape Restrictions||||Pinkse, Joris || Schurter, Karl||||https://arxiv.org/pdf/1912.07466||||1912.07466||||We introduce several new estimation methods that leverage shape constraints in auction models to estimate various objects of interest, including the distribution of a bidder's valuations, the bidder's ex ante expected surplus, and the seller's counterfactual revenue. The basic approach applies broadly in that (unlike most of the literature) it works for a wide range of auction formats and allows for asymmetric bidders. Though our approach is not restrictive, we focus our analysis on first--price, sealed--bid auctions with independent private valuations. We highlight two nonparametric estimation strategies, one based on a least squares criterion and the other on a maximum likelihood criterion. We also provide the first direct estimator of the strategy function. We establish several theoretical properties of our methods to guide empirical analysis and inference. In addition to providing the asymptotic distributions of our estimators, we identify ways in which methodological choices should be tailored to the objects of their interest. For objects like the bidders' ex ante surplus and the seller's counterfactual expected revenue with an additional symmetric bidder, we show that our input--parameter--free estimators achieve the semiparametric efficiency bound. For objects like the bidders' inverse strategy function, we provide an easily implementable boundary--corrected kernel smoothing and transformation method in order to ensure the squared error is integrable over the entire support of the valuations. An extensive simulation study illustrates our analytical results and demonstrates the respective advantages of our least--squares and maximum likelihood estimators in finite samples. Compared to estimation strategies based on kernel density estimation, the simulations indicate that the smoothed versions of our estimators enjoy a large degree of robustness to the choice of an input parameter.||||@arxiv||||2019/12/16||||Estimation of Auction Models with Shape Restrictions||||We introduce several new estimation methods that leverage shape constraints in auction models to estimate various objects of interest, including the distribution of a bidder's valuations, the...||||https://arxiv.org/abs/1912.07466v1||||econ||||
563||||None||||The Logic of Strategic Assets: From Oil to Artificial Intelligence||||arXiv.org||||2020/01/09||||The Logic of Strategic Assets: From Oil to Artificial Intelligence||||Ding, Jeffrey || Dafoe, Allan||||https://arxiv.org/pdf/2001.03246||||2001.03246||||What resources and technologies are strategic? This question is often the focus of policy and theoretical debates, where the label "strategic" designates those assets that warrant the attention of the highest levels of the state. But these conversations are plagued by analytical confusion, flawed heuristics, and the rhetorical use of "strategic" to advance particular agendas. We aim to improve these conversations through conceptual clarification, introducing a theory based on important rivalrous externalities for which socially optimal behavior will not be produced alone by markets or individual national security entities. We distill and theorize the most important three forms of these externalities, which involve cumulative-, infrastructure-, and dependency-strategic logics. We then employ these logics to clarify three important cases: the Avon 2 engine in the 1950s, the U.S.-Japan technology rivalry in the late 1980s, and contemporary conversations about artificial intelligence.||||@arxiv||||2020/01/09||||The Logic of Strategic Assets: From Oil to Artificial Intelligence||||What resources and technologies are strategic? This question is often the focus of policy and theoretical debates, where the label "strategic" designates those assets that warrant the attention of...||||https://arxiv.org/abs/2001.03246v1||||cs||||
564||||None||||Testing for Unobserved Heterogeneity via k-means Clustering||||arXiv.org||||2019/07/17||||Testing for Unobserved Heterogeneity via k-means Clustering||||Patton, Andrew J. || Weller, Brian M.||||https://arxiv.org/pdf/1907.07582||||1907.07582||||Clustering methods such as k-means have found widespread use in a variety of applications. This paper proposes a formal testing procedure to determine whether a null hypothesis of a single cluster, indicating homogeneity of the data, can be rejected in favor of multiple clusters. The test is simple to implement, valid under relatively mild conditions (including non-normality, and heterogeneity of the data in aspects beyond those in the clustering analysis), and applicable in a range of contexts (including clustering when the time series dimension is small, or clustering on parameters other than the mean). We verify that the test has good size control in finite samples, and we illustrate the test in applications to clustering vehicle manufacturers and U.S. mutual funds.||||@arxiv||||2019/07/17||||Testing for Unobserved Heterogeneity via k-means Clustering||||Clustering methods such as k-means have found widespread use in a variety of applications. This paper proposes a formal testing procedure to determine whether a null hypothesis of a single...||||https://arxiv.org/abs/1907.07582v1||||econ||||
565||||None||||Identifying Network Ties from Panel Data: Theory and an Application to Tax Competition||||arXiv.org||||2019/10/16||||Identifying Network Ties from Panel Data: Theory and an Application to Tax Competition||||de Paula, Aureo || Rasul, Imran || Souza, Pedro||||https://arxiv.org/pdf/1910.07452||||1910.07452||||Social interactions determine many economic behaviors, but information on social ties does not exist in most publicly available and widely used datasets. We present results on the identification of social networks from observational panel data that contains no information on social ties between agents. In the context of a canonical social interactions model, we provide sufficient conditions under which the social interactions matrix, endogenous and exogenous social effect parameters are all globally identified. While this result is relevant across different estimation strategies, we then describe how high-dimensional estimation techniques can be used to estimate the interactions model based on the Adaptive Elastic Net GMM method. We employ the method to study tax competition across US states. We find the identified social interactions matrix implies tax competition differs markedly from the common assumption of competition between geographically neighboring states, providing further insights for the long-standing debate on the relative roles of factor mobility and yardstick competition in driving tax setting behavior across states. Most broadly, our identification and application show the analysis of social interactions can be extended to economic realms where no network data exists.||||@arxiv||||2019/10/16||||Identifying Network Ties from Panel Data: Theory and an...||||Social interactions determine many economic behaviors, but information on social ties does not exist in most publicly available and widely used datasets. We present results on the identification...||||https://arxiv.org/abs/1910.07452v1||||econ||||
566||||None||||Inference in Differences-in-Differences: How Much Should We Trust in Independent Clusters?||||arXiv.org||||2019/09/24||||Inference in Differences-in-Differences: How Much Should We Trust in Independent Clusters?||||Ferman, Bruno||||https://arxiv.org/pdf/1909.01782||||1909.01782||||We analyze the conditions in which ignoring spatial correlation is problematic for inference in differences-in-differences (DID) models. Assuming that the spatial correlation structure follows a linear factor model, we show that inference ignoring such correlation remains reliable when either (i) the second moment of the difference between the pre- and post-treatment averages of common factors is low, or (ii) the distribution of factor loadings has the same expected values for treated and control groups, and do not exhibit significant spatial correlation. We present simulations with real datasets that corroborate these conclusions. Our results provide important guidelines on how to minimize inference problems due to spatial correlation in DID applications.||||@arxiv||||2019/09/04||||Inference in Differences-in-Differences: How Much Should We Trust...||||We analyze the conditions in which ignoring spatial correlation is problematic for inference in differences-in-differences (DID) models. Assuming that the spatial correlation structure follows a...||||https://arxiv.org/abs/1909.01782v2||||econ||||
567||||None||||Bias and Consistency in Three-way Gravity Models||||arXiv.org||||2020/01/27||||Bias and Consistency in Three-way Gravity Models||||Weidner, Martin || Zylkin, Thomas||||https://arxiv.org/pdf/1909.01327||||1909.01327||||We study the incidental parameter problem in "three-way" Poisson Pseudo-Maximum Likelihood ("PPML") gravity models recently recommended for identifying the effects of trade policies and in other network panel data settings. Despite the number and variety of fixed effects this model entails, we confirm it is consistent for small $T$ and we show it is in fact the only estimator among a wide range of PML gravity estimators that is generally consistent in this context when $T$ is small. At the same time, asymptotic confidence intervals in fixed-$T$ panels are not correctly centered at the true point estimates, and cluster-robust variance estimates used to construct standard errors are generally biased as well. We characterize each of these biases analytically and show both numerically and empirically that they are salient even for real-data settings with a large number of countries. We also offer practical remedies that can be used to obtain more reliable inferences of the effects of trade policies and other time-varying gravity variables.||||@arxiv||||2019/09/03||||Bias and Consistency in Three-way Gravity Models||||We study the incidental parameter problem in "three-way" Poisson Pseudo-Maximum Likelihood ("PPML") gravity models recently recommended for identifying the effects of trade policies and in other...||||https://arxiv.org/abs/1909.01327v3||||econ||||
568||||None||||Brexit: The Belated Threat||||arXiv.org||||2018/08/14||||Brexit: The Belated Threat||||Petróczy, Dóra Gréta || Rogers, Mark Francis || Kóczy, László Á.||||https://arxiv.org/pdf/1808.05142||||1808.05142||||Debates on an EU-leaving referendum arose in several member states after Brexit. We want to highlight how the exit of an additional country affects the power distribution in the Council of the European Union. We inspect the power indices of the member states both with and without the country which might leave the union. Our results show a pattern connected to a change in the threshold of the number of member states required for a decision. An exit that modifies this threshold benefits the countries with high population, while an exit that does not cause such a change benefits the small member states. According to our calculations, the threat of Brexit would have worked differently before the entry of Croatia.||||@arxiv||||2018/08/14||||Brexit: The Belated Threat||||Debates on an EU-leaving referendum arose in several member states after Brexit. We want to highlight how the exit of an additional country affects the power distribution in the Council of the...||||https://arxiv.org/abs/1808.05142v1||||econ||||
569||||None||||Rational Groupthink||||arXiv.org||||2019/12/04||||Rational Groupthink||||Harel, Matan || Mossel, Elchanan || Strack, Philipp || Tamuz, Omer||||https://arxiv.org/pdf/1412.7172||||1412.7172||||We study how long-lived rational agents learn from repeatedly observing a private signal and each others' actions. With normal signals, a group of any size learns more slowly than just four agents who directly observe each others' private signals in each period. Similar results apply to general signal structures. We identify rational groupthink---in which agents ignore their private signals and choose the same action for long periods of time---as the cause of this failure of information aggregation.||||@arxiv||||2014/12/22||||Rational Groupthink||||We study how long-lived rational agents learn from repeatedly observing a private signal and each others' actions. With normal signals, a group of any size learns more slowly than just four agents...||||https://arxiv.org/abs/1412.7172v6||||cs||||
570||||None||||Risk Fluctuation Characteristics of Internet Finance: Combining Industry Characteristics with Ecological Value||||arXiv.org||||2020/01/27||||Risk Fluctuation Characteristics of Internet Finance: Combining Industry Characteristics with Ecological Value||||Xu, Runjie || Mi, Chuanmin || Ye, Nan || Marshall, Tom || Xiao, Yadong || Shuai, Hefan||||https://arxiv.org/pdf/2001.09798||||2001.09798||||The Internet plays a key role in society and is vital to economic development. Due to the pressure of competition, most technology companies, including Internet finance companies, continue to explore new markets and new business. Funding subsidies and resource inputs have led to significant business income tendencies in financial statements. This tendency of business income is often manifested as part of the business loss or long-term unprofitability. We propose a risk change indicator (RFR) and compare the risk indicator of fourteen representative companies. This model combines extreme risk value with slope, and the combination method is simple and effective. The results of experiment show the potential of this model. The risk volatility of technology enterprises including Internet finance enterprises is highly cyclical, and the risk volatility of emerging Internet fintech companies is much higher than that of other technology companies.||||@arxiv||||2020/01/27||||Risk Fluctuation Characteristics of Internet Finance: Combining...||||The Internet plays a key role in society and is vital to economic development. Due to the pressure of competition, most technology companies, including Internet finance companies, continue to...||||https://arxiv.org/abs/2001.09798v1||||econ||||
571||||None||||CAP and Monetary Policy||||arXiv.org||||2018/07/25||||CAP and Monetary Policy||||Duisberg, Carl||||https://arxiv.org/pdf/1807.09475||||1807.09475||||Despite the importance of CAP-related agricultural market regulation mechanisms within Europe, the agricultural sectors in European countries retain a degree of sensitivity to macroeconomic activity and policies. This reality now raises the question of the effects to be expected from the implementation of the single monetary policy on these agricultural sectors within the Monetary Union.||||@arxiv||||2018/07/25||||CAP and Monetary Policy||||Despite the importance of CAP-related agricultural market regulation mechanisms within Europe, the agricultural sectors in European countries retain a degree of sensitivity to macroeconomic...||||https://arxiv.org/abs/1807.09475v1||||econ||||
572||||None||||A long short-term memory stochastic volatility model||||arXiv.org||||2019/09/30||||A long short-term memory stochastic volatility model||||Nguyen, Nghia || Tran, Minh-Ngoc || Gunawan, David || Kohn, R.||||https://arxiv.org/pdf/1906.02884||||1906.02884||||Stochastic Volatility (SV) models are widely used in the financial sector while Long Short-Term Memory (LSTM) models are successfully used in many large-scale industrial applications of Deep Learning. Our article combines these two methods in a non-trivial way and proposes a model, which we call the LSTM-SV model, to capture the dynamics of stochastic volatility. The proposed model overcomes the short-term memory problem in conventional SV models, is able to capture non-linear dependence in the latent volatility process, and often has a better out-of-sample forecast performance than SV models. These properties are illustrated through simulation study and applications to three financial time series datasets: The US stock market weekly index SP500, the Australian stock weekly index ASX200 and the Australian-US dollar daily exchange rates. Based on our analysis, we argue that there are significant differences in the underlying dynamics between the volatility process of the SP500 and ASX200 datasets and that of the exchange rate dataset. For the stock index data, there is strong evidence of long-term memory and non-linear dependence in the volatility process, while this is not the case for the exchange rates. An user-friendly software package together with the examples reported in the paper are available at https://github.com/vbayeslab.||||@arxiv||||2019/06/07||||A long short-term memory stochastic volatility model||||Stochastic Volatility (SV) models are widely used in the financial sector while Long Short-Term Memory (LSTM) models are successfully used in many large-scale industrial applications of Deep...||||https://arxiv.org/abs/1906.02884v2||||econ||||
573||||None||||Artificial Intelligence Alter Egos: Who benefits from Robo-investing?||||arXiv.org||||2019/07/08||||Artificial Intelligence Alter Egos: Who benefits from Robo-investing?||||D'Hondt, Catherine || De Winne, Rudy || Ghysels, Eric || Raymond, Steve||||https://arxiv.org/pdf/1907.03370||||1907.03370||||Artificial intelligence, or AI, enhancements are increasingly shaping our daily lives. Financial decision-making is no exception to this. We introduce the notion of AI Alter Egos, which are shadow robo-investors, and use a unique data set covering brokerage accounts for a large cross-section of investors over a sample from January 2003 to March 2012, which includes the 2008 financial crisis, to assess the benefits of robo-investing. We have detailed investor characteristics and records of all trades. Our data set consists of investors typically targeted for robo-advising. We explore robo-investing strategies commonly used in the industry, including some involving advanced machine learning methods. The man versus machine comparison allows us to shed light on potential benefits the emerging robo-advising industry may provide to certain segments of the population, such as low income and/or high risk averse investors.||||@arxiv||||2019/07/08||||Artificial Intelligence Alter Egos: Who benefits from Robo-investing?||||Artificial intelligence, or AI, enhancements are increasingly shaping our daily lives. Financial decision-making is no exception to this. We introduce the notion of AI Alter Egos, which are shadow...||||https://arxiv.org/abs/1907.03370v1||||econ||||
574||||None||||Political Openness and Armed Conflict: Evidence from Local Councils in Colombia||||arXiv.org||||2019/10/08||||Political Openness and Armed Conflict: Evidence from Local Councils in Colombia||||Galindo-Silva, Hector||||https://arxiv.org/pdf/1910.03712||||1910.03712||||In this paper, I empirically investigate how the openness of political institutions to diverse representation can impact conflict-related violence. By exploiting plausibly exogenous variations in the number of councillors in Colombian municipalities, I develop two sets of results. First, regression discontinuity estimates show that larger municipal councils have a considerably greater number of political parties with at least one elected representative. I interpret this result as evidence that larger municipal councils are more open to diverse political participation. The estimates also reveal that non-traditional parties are the main beneficiaries of this greater political openness. Second, regression discontinuity estimates show that political openness substantially decreases conflict-related violence, namely the killing of civilian non-combatants. By exploiting plausibly exogenous variations in local election results, I show that the lower level of political violence stems from greater participation by parties with close links to armed groups. Using data about the types of violence employed by these groups, and representation at higher levels of government, I argue that armed violence has decreased not because of power-sharing arrangements involving armed groups linked to the parties with more political representation, but rather because armed groups with less political power and visibility are deterred from initiating certain types of violence.||||@arxiv||||2019/10/08||||Political Openness and Armed Conflict: Evidence from Local...||||In this paper, I empirically investigate how the openness of political institutions to diverse representation can impact conflict-related violence. By exploiting plausibly exogenous variations in...||||https://arxiv.org/abs/1910.03712v1||||econ||||
575||||None||||Averaging estimation for instrumental variables quantile regression||||arXiv.org||||2019/10/09||||Averaging estimation for instrumental variables quantile regression||||Liu, Xin||||https://arxiv.org/pdf/1910.04245||||1910.04245||||This paper proposes averaging estimation methods to improve the finite-sample efficiency of the instrumental variables quantile regression (IVQR) estimation. First, I apply Cheng, Liao, Shi's (2019) averaging GMM framework to the IVQR model. I propose using the usual quantile regression moments for averaging to take advantage of cases when endogeneity is not too strong. I also propose using two-stage least squares slope moments to take advantage of cases when heterogeneity is not too strong. The empirical optimal weight formula of Cheng et al. (2019) helps optimize the bias-variance tradeoff, ensuring uniformly better (asymptotic) risk of the averaging estimator over the standard IVQR estimator under certain conditions. My implementation involves many computational considerations and builds on recent developments in the quantile literature. Second, I propose a bootstrap method that directly averages among IVQR, quantile regression, and two-stage least squares estimators. More specifically, I find the optimal weights in the bootstrap world and then apply the bootstrap-optimal weights to the original sample. The bootstrap method is simpler to compute and generally performs better in simulations, but it lacks the formal uniform dominance results of Cheng et al. (2019). Simulation results demonstrate that in the multiple-regressors/instruments case, both the GMM averaging and bootstrap estimators have uniformly smaller risk than the IVQR estimator across data-generating processes (DGPs) with all kinds of combinations of different endogeneity levels and heterogeneity levels. In DGPs with a single endogenous regressor and instrument, where averaging estimation is known to have least opportunity for improvement, the proposed averaging estimators outperform the IVQR estimator in some cases but not others.||||@arxiv||||2019/10/09||||Averaging estimation for instrumental variables quantile regression||||This paper proposes averaging estimation methods to improve the finite-sample efficiency of the instrumental variables quantile regression (IVQR) estimation. First, I apply Cheng, Liao, Shi's...||||https://arxiv.org/abs/1910.04245v1||||econ||||
576||||None||||Hipsters and the Cool: A Game Theoretic Analysis of Social Identity, Trends and Fads||||arXiv.org||||2019/10/29||||Hipsters and the Cool: A Game Theoretic Analysis of Social Identity, Trends and Fads||||Golman, Russell || Jain, Aditi || Saraf, Sonica||||https://arxiv.org/pdf/1910.13385||||1910.13385||||Cultural trends and popularity cycles can be observed all around us, yet our theories of social influence and identity expression do not explain what perpetuates these complex, often unpredictable social dynamics. We propose a theory of social identity expression based on the opposing, but not mutually exclusive, motives to conform and to be unique among one's neighbors in a social network. We then model the social dynamics that arise from these motives. We find that the dynamics typically enter random walks or stochastic limit cycles rather than converging to a static equilibrium. We also prove that without social network structure or, alternatively, without the uniqueness motive, reasonable adaptive dynamics would necessarily converge to equilibrium. Thus, we show that nuanced psychological assumptions (recognizing preferences for uniqueness along with conformity) and realistic social network structure are both necessary for explaining how complex, unpredictable cultural trends emerge.||||@arxiv||||2019/10/29||||Hipsters and the Cool: A Game Theoretic Analysis of Social...||||Cultural trends and popularity cycles can be observed all around us, yet our theories of social influence and identity expression do not explain what perpetuates these complex, often unpredictable...||||https://arxiv.org/abs/1910.13385v1||||econ||||
577||||None||||Evaluating regulatory reform of network industries: a survey of empirical models based on categorical proxies||||arXiv.org||||2018/10/08||||Evaluating regulatory reform of network industries: a survey of empirical models based on categorical proxies||||Bastianin, Andrea || Castelnovo, Paolo || Florio, Massimo||||https://arxiv.org/pdf/1810.03348||||1810.03348||||Proxies for regulatory reforms based on categorical variables are increasingly used in empirical evaluation models. We surveyed 63 studies that rely on such indices to analyze the effects of entry liberalization, privatization, unbundling, and independent regulation of the electricity, natural gas, and telecommunications sectors. We highlight methodological issues related to the use of these proxies. Next, taking stock of the literature, we provide practical advice for the design of the empirical strategy and discuss the selection of control and instrumental variables to attenuate endogeneity problems undermining identification of the effects of regulatory reforms.||||@arxiv||||2018/10/08||||Evaluating regulatory reform of network industries: a survey of...||||Proxies for regulatory reforms based on categorical variables are increasingly used in empirical evaluation models. We surveyed 63 studies that rely on such indices to analyze the effects of entry...||||https://arxiv.org/abs/1810.03348v1||||econ||||
578||||None||||Electronic markets with multiple submodular buyers||||arXiv.org||||2019/09/23||||Electronic markets with multiple submodular buyers||||Borodin, Allan || Rakheja, Akash||||https://arxiv.org/pdf/1907.11915||||1907.11915||||We discuss the problem of setting prices in an electronic market that has more than one buyer. We assume that there are self-interested sellers each selling a distinct item that has an associated cost. Each buyer has a submodular valuation for purchasing any subset of items. The goal of the sellers is to set a price for their item such that their profit from possibly selling their item to the buyers is maximized. Our most comprehensive results concern a multi copy setting where each seller has m copies of their item and there are m buyers. In this setting, we give a necessary and sufficient condition for the existence of market clearing pure Nash equilibrium. We also show that not all equilibria are market clearing even when this condition is satisfied contrary to what was shown in the case of a single buyer in [2]. Finally, we investigate the pricing problem for multiple buyers in the limited supply setting when each seller only has a single copy of their item.||||@arxiv||||2019/07/27||||Electronic markets with multiple submodular buyers||||We discuss the problem of setting prices in an electronic market that has more than one buyer. We assume that there are self-interested sellers each selling a distinct item that has an associated...||||https://arxiv.org/abs/1907.11915v2||||cs||||
579||||None||||Nonparametric identification of an interdependent value model with buyer covariates from first-price auction bids||||arXiv.org||||2019/10/23||||Nonparametric identification of an interdependent value model with buyer covariates from first-price auction bids||||Gimenes, Nathalie || Guerre, Emmanuel||||https://arxiv.org/pdf/1910.10646||||1910.10646||||This paper introduces a version of the interdependent value model of Milgrom and Weber (1982), where the signals are given by an index gathering signal shifters observed by the econometrician and private ones specific to each bidders. The model primitives are shown to be nonparametrically identified from first-price auction bids under a testable mild rank condition. Identification holds for all possible signal values. This allows to consider a wide range of counterfactuals where this is important, as expected revenue in second-price auction. An estimation procedure is briefly discussed.||||@arxiv||||2019/10/23||||Nonparametric identification of an interdependent value model with...||||This paper introduces a version of the interdependent value model of Milgrom and Weber (1982), where the signals are given by an index gathering signal shifters observed by the econometrician and...||||https://arxiv.org/abs/1910.10646v1||||econ||||
580||||None||||A supreme test for periodic explosive GARCH||||arXiv.org||||2018/12/09||||A supreme test for periodic explosive GARCH||||Richter, Stefan || Wang, Weining || Wu, Wei Biao||||https://arxiv.org/pdf/1812.03475||||1812.03475||||We develop a uniform test for detecting and dating explosive behavior of a strictly stationary GARCH$(r,s)$ (generalized autoregressive conditional heteroskedasticity) process. Namely, we test the null hypothesis of a globally stable GARCH process with constant parameters against an alternative where there is an 'abnormal' period with changed parameter values. During this period, the change may lead to an explosive behavior of the volatility process. It is assumed that both the magnitude and the timing of the breaks are unknown. We develop a double supreme test for the existence of a break, and then provide an algorithm to identify the period of change. Our theoretical results hold under mild moment assumptions on the innovations of the GARCH process. Technically, the existing properties for the QMLE in the GARCH model need to be reinvestigated to hold uniformly over all possible periods of change. The key results involve a uniform weak Bahadur representation for the estimated parameters, which leads to weak convergence of the test statistic to the supreme of a Gaussian Process. In simulations we show that the test has good size and power for reasonably large time series lengths. We apply the test to Apple asset returns and Bitcoin returns.||||@arxiv||||2018/12/09||||A supreme test for periodic explosive GARCH||||We develop a uniform test for detecting and dating explosive behavior of a strictly stationary GARCH$(r,s)$ (generalized autoregressive conditional heteroskedasticity) process. Namely, we test the...||||https://arxiv.org/abs/1812.03475v1||||econ||||
581||||None||||Analyzing China's Consumer Price Index Comparatively with that of United States||||arXiv.org||||2019/10/29||||Analyzing China's Consumer Price Index Comparatively with that of United States||||Wang, Zhenzhong || Tu, Yundong || Chen, Song Xi||||https://arxiv.org/pdf/1910.13301||||1910.13301||||This paper provides a thorough analysis on the dynamic structures and predictability of China's Consumer Price Index (CPI-CN), with a comparison to those of the United States. Despite the differences in the two leading economies, both series can be well modeled by a class of Seasonal Autoregressive Integrated Moving Average Model with Covariates (S-ARIMAX). The CPI-CN series possess regular patterns of dynamics with stable annual cycles and strong Spring Festival effects, with fitting and forecasting errors largely comparable to their US counterparts. Finally, for the CPI-CN, the diffusion index (DI) approach offers improved predictions than the S-ARIMAX models.||||@arxiv||||2019/10/29||||Analyzing China's Consumer Price Index Comparatively with that...||||This paper provides a thorough analysis on the dynamic structures and predictability of China's Consumer Price Index (CPI-CN), with a comparison to those of the United States. Despite the...||||https://arxiv.org/abs/1910.13301v1||||econ||||
582||||None||||Perceived Advantage in Perspective Application of Integrated Choice and Latent Variable Model to Capture Electric Vehicles Perceived Advantage from Consumers Perspective||||arXiv.org||||2019/11/28||||Perceived Advantage in Perspective Application of Integrated Choice and Latent Variable Model to Capture Electric Vehicles Perceived Advantage from Consumers Perspective||||Ghasri, Milad || Ardeshiri, Ali || Rashidi, Taha||||https://arxiv.org/pdf/1905.11606||||1905.11606||||Relative advantage, or the degree to which a new technology is perceived to be better over the existing technology it supersedes, has a significant impact on individuals decision of adopting to the new technology. This paper investigates the impact of electric vehicles perceived advantage over the conventional internal combustion engine vehicles, from consumers perspective, on their decision to select electric vehicles. Data is obtained from a stated preference survey from 1176 residents in New South Wales, Australia. The collected data is used to estimate an integrated choice and latent variable model of electric vehicle choice, which incorporates the perceived advantage of electric vehicles in the form of latent variables in the utility function. The design of the electric vehicle, impact on the environment, and safety are three identified advantages from consumers point of view. The model is used to simulate the effectiveness of various policies to promote electric vehicles on different cohorts. Rebate on the purchase price is found to be the most effective strategy to promote electric vehicles adoption.||||@arxiv||||2019/05/28||||Perceived Advantage in Perspective Application of Integrated...||||Relative advantage, or the degree to which a new technology is perceived to be better over the existing technology it supersedes, has a significant impact on individuals decision of adopting to...||||https://arxiv.org/abs/1905.11606v2||||econ||||
583||||None||||Testing for Sample Selection||||arXiv.org||||2019/08/26||||Testing for Sample Selection||||Corradi, Valentina || Gutknecht, Daniel||||https://arxiv.org/pdf/1907.07412||||1907.07412||||This paper provides a unified approach for detecting sample selection in nonparametric conditional quantile \textit{and} mean functions. Our testing strategy consists of a two-step procedure: the first test is an omitted predictor test with the propensity score as omitted variable. This test has power against $\sqrt{n}-$alternatives. While failure to reject the null implies no selection, we cannot, as any omnibus test, distinguish between rejection due to genuine selection or to misspecification. Since differentiation of the latter has implications for nonparametric (point) identification and estimation of the conditional quantile function, our second test is designed to detect misspecification. Using only individuals with propensity score close to one, this test relies on an `identification at infinity' argument, but accommodates cases of irregular identification. Finally, our testing procedure does not require any parametric assumptions on the selection equation, and all our results in the quantile case hold uniformly across quantile ranks in a compact set. We apply our procedure to test for selection in log hourly wages using UK Family Expenditure Survey data.||||@arxiv||||2019/07/17||||Testing for Sample Selection||||This paper provides a unified approach for detecting sample selection in nonparametric conditional quantile \textit{and} mean functions. Our testing strategy consists of a two-step procedure: the...||||https://arxiv.org/abs/1907.07412v2||||econ||||
584||||None||||Can GDP measurement be further improved? Data revision and reconciliation||||arXiv.org||||2018/08/15||||Can GDP measurement be further improved? Data revision and reconciliation||||Jacobs, Jan P. A. M. || Sarferaz, Samad || Sturm, Jan-Egbert || van Norden, Simon||||https://arxiv.org/pdf/1808.04970||||1808.04970||||Recent years have seen many attempts to combine expenditure-side estimates of U.S. real output (GDE) growth with income-side estimates (GDI) to improve estimates of real GDP growth. We show how to incorporate information from multiple releases of noisy data to provide more precise estimates while avoiding some of the identifying assumptions required in earlier work. This relies on a new insight: using multiple data releases allows us to distinguish news and noise measurement errors in situations where a single vintage does not.   Our new measure, GDP++, fits the data better than GDP+, the GDP growth measure of Aruoba et al. (2016) published by the Federal Reserve Bank of Philadephia. Historical decompositions show that GDE releases are more informative than GDI, while the use of multiple data releases is particularly important in the quarters leading up to the Great Recession.||||@arxiv||||2018/08/15||||Can GDP measurement be further improved? Data revision and reconciliation||||Recent years have seen many attempts to combine expenditure-side estimates of U.S. real output (GDE) growth with income-side estimates (GDI) to improve estimates of real GDP growth. We show how to...||||https://arxiv.org/abs/1808.04970v1||||econ||||
585||||None||||An Economic Topology of the Brexit vote||||arXiv.org||||2019/09/08||||An Economic Topology of the Brexit vote||||Dlotko, Pawel || Rudkin, Simon || Qiu, Wanling||||https://arxiv.org/pdf/1909.03490||||1909.03490||||A quest to understand the decision of the UK to leave the European Union, Brexit, in the referendum of June 2016 has occupied academics, the media and politicians alike. As the debate about what the future relationship will look like rages, the referendum is given renewed importance as an indicator of the likely success, or otherwise, of any forward plans. Topological data analysis offers an ability to faithfully extract maximal information from complex multi-dimensional datasets of the type that have been gathered on Brexit voting. Within the complexity it is shown that support for Leave drew from a far more similar demographic than Remain. Obtaining votes from this concise set was more straightforward for Leave campaigners than was Remain's task of mobilising a diverse group to oppose Brexit. Broad patterns are consistent with extant empirical work, but the strength of TDA Ball Mapper means that evidence is offered to enrich the narrative on immobility, and being ``left-behind'' by EU membership, that could not be found before. A detailed understanding emerges which comments robustly on why Britain voted as it did. A start point for the policy development that must follow is given.||||@arxiv||||2019/09/08||||An Economic Topology of the Brexit vote||||A quest to understand the decision of the UK to leave the European Union, Brexit, in the referendum of June 2016 has occupied academics, the media and politicians alike. As the debate about what...||||https://arxiv.org/abs/1909.03490v1||||econ||||
586||||None||||Systemic Cascades On Inhomogeneous Random Financial Networks||||arXiv.org||||2019/09/19||||Systemic Cascades On Inhomogeneous Random Financial Networks||||Hurd, T. R.||||https://arxiv.org/pdf/1909.09239||||1909.09239||||This systemic risk paper introduces inhomogeneous random financial networks (IRFNs). Such models are intended to describe parts, or the entirety, of a highly heterogeneous network of banks and their interconnections, in the global financial system. Both the balance sheets and the stylized crisis behaviour of banks are ingredients of the network model. A systemic crisis is pictured as triggered by a shock to banks' balance sheets, which then leads to the propagation of damaging shocks and the potential for amplification of the crisis, ending with the system in a cascade equilibrium. Under some conditions the model has ``locally tree-like independence (LTI)'', where a general percolation theoretic argument leads to an analytic fixed point equation describing the cascade equilibrium when the number of banks $N$ in the system is taken to infinity. This paper focusses on mathematical properties of the framework in the context of Eisenberg-Noe solvency cascades generalized to account for fractional bankruptcy charges. New results including a definition and proof of the ``LTI property'' of the Eisenberg-Noe solvency cascade mechanism lead to explicit $N=\infty$ fixed point equations that arise under very general model specifications. The essential formulas are shown to be implementable via well-defined approximation schemes, but numerical exploration of some of the wide range of potential applications of the method is left for future work.||||@arxiv||||2019/09/19||||Systemic Cascades On Inhomogeneous Random Financial Networks||||This systemic risk paper introduces inhomogeneous random financial networks (IRFNs). Such models are intended to describe parts, or the entirety, of a highly heterogeneous network of banks and...||||https://arxiv.org/abs/1909.09239v1||||econ||||
587||||None||||Proportionality and the Limits of Welfarism||||arXiv.org||||2019/11/26||||Proportionality and the Limits of Welfarism||||Peters, Dominik || Skowron, Piotr||||https://arxiv.org/pdf/1911.11747||||1911.11747||||We study two influential voting rules proposed in the 1890s by Phragmén and Thiele, which elect a committee or parliament of k candidates which proportionally represents the voters. Voters provide their preferences by approving an arbitrary number of candidates. Previous work has proposed proportionality axioms satisfied by Thiele's rule (now known as Proportional Approval Voting, PAV) but not by Phragmén's rule. By proposing two new proportionality axioms (laminar proportionality and priceability) satisfied by Phragmén but not Thiele, we show that the two rules achieve two distinct forms of proportional representation. Phragmén's rule ensures that all voters have a similar amount of influence on the committee, and Thiele's rule ensures a fair utility distribution.   Thiele's rule is a welfarist voting rule (one that maximizes a function of voter utilities). We show that no welfarist rule can satisfy our new axioms, and we prove that no such rule can satisfy the core. Conversely, some welfarist fairness properties cannot be guaranteed by Phragmén-type rules. This formalizes the difference between the two types of proportionality. We then introduce an attractive committee rule which satisfies a property intermediate between the core and extended justified representation (EJR). It satisfies laminar proportionality, priceability, and is computable in polynomial time. We show that our new rule provides a logarithmic approximation to the core. On the other hand, PAV provides a factor-2 approximation to the core, and this factor is optimal for rules that are fair in the sense of the Pigou--Dalton principle.||||@arxiv||||2019/11/26||||Proportionality and the Limits of Welfarism||||We study two influential voting rules proposed in the 1890s by Phragmén and Thiele, which elect a committee or parliament of k candidates which proportionally represents the voters. Voters...||||https://arxiv.org/abs/1911.11747v1||||cs||||
588||||None||||Constructing a Social Accounting Matrix Framework to Analyse the Impact of Public Expenditure on Income Distribution in Malaysia||||arXiv.org||||2020/01/07||||Constructing a Social Accounting Matrix Framework to Analyse the Impact of Public Expenditure on Income Distribution in Malaysia||||Harun, Mukaramah || Zakariah, A. R. || Azali, M.||||https://arxiv.org/pdf/2001.03488||||2001.03488||||The use of the social accounting matrix (SAM) in income distribution analysis is a method recommended by economists. However, until now, there have only been a few SAM developed in Malaysia. The last SAM produced for Malaysia was developed in 1984 based upon data from 1970 and has not been updated since this time despite the significance changes in the structure of the Malaysian economy. The paper proposes a new Malaysian SAM framework to analyse public expenditure impact on income distribution in Malaysia. The SAM developed in the present paper is based on more recent data, providing an up-to date and coherent picture of the complexity of the Malaysian economy. The paper describes the structure of the SAM framework with a detailed aggregation and disaggregation of accounts related to public expenditure and income distribution issues. In the SAM utilized in the present study, the detailed framework of the different components of public expenditure in the production sectors and household groups is essential in the analysis of the different effects of the various public expenditure programmes on the incomes of households among different groups.||||@arxiv||||2020/01/07||||Constructing a Social Accounting Matrix Framework to Analyse the...||||The use of the social accounting matrix (SAM) in income distribution analysis is a method recommended by economists. However, until now, there have only been a few SAM developed in Malaysia. The...||||https://arxiv.org/abs/2001.03488v1||||econ||||
589||||None||||An Econometric Perspective of Algorithmic Subsampling||||arXiv.org||||2020/01/29||||An Econometric Perspective of Algorithmic Subsampling||||Lee, Sokbae || Ng, Serena||||https://arxiv.org/pdf/1907.01954||||1907.01954||||Datasets that are terabytes in size are increasingly common, but computer bottlenecks often frustrate a complete analysis of the data. While more data are better than less, diminishing returns suggest that we may not need terabytes of data to estimate a parameter or test a hypothesis. But which rows of data should we analyze, and might an arbitrary subset of rows preserve the features of the original data? This paper reviews a line of work that is grounded in theoretical computer science and numerical linear algebra, and which finds that an algorithmically desirable sketch, which is a randomly chosen subset of the data, must preserve the eigenstructure of the data, a property known as a subspace embedding. Building on this work, we study how prediction and inference can be affected by data sketching within a linear regression setup. We show that the sketching error is small compared to the sample size effect which a researcher can control. As a sketch size that is algorithmically optimal may not be suitable for prediction and inference, we use statistical arguments to provide 'inference conscious' guides to the sketch size. When appropriately implemented, an estimator that pools over different sketches can be nearly as efficient as the infeasible one using the full sample.||||@arxiv||||2019/07/03||||An Econometric Perspective of Algorithmic Subsampling||||Datasets that are terabytes in size are increasingly common, but computer bottlenecks often frustrate a complete analysis of the data. While more data are better than less, diminishing returns...||||https://arxiv.org/abs/1907.01954v3||||econ||||
590||||None||||Optimal Search Segmentation Mechanisms for Online Platform Markets||||arXiv.org||||2019/08/20||||Optimal Search Segmentation Mechanisms for Online Platform Markets||||Zheng, Zhenzhe || Srikant, R.||||https://arxiv.org/pdf/1908.07489||||1908.07489||||Online platforms, such as Airbnb, hotels.com, Amazon, Uber and Lyft, can control and optimize many aspects of product search to improve the efficiency of marketplaces. Here we focus on a common model, called the discriminatory control model, where the platform chooses to display a subset of sellers who sell products at prices determined by the market and a buyer is interested in buying a single product from one of the sellers. Under the commonly-used model for single product selection by a buyer, called the multinomial logit model, and the Bertrand game model for competition among sellers, we show the following result: to maximize social welfare, the optimal strategy for the platform is to display all products; however, to maximize revenue, the optimal strategy is to only display a subset of the products whose qualities are above a certain threshold. We extend our results to Cournot competition model, and show that the optimal search segmentation mechanisms for both social welfare maximization and revenue maximization also have simple threshold structures. The threshold in each case depends on the quality of all products, the platform's objective and seller's competition model, and can be computed in linear time in the number of products.||||@arxiv||||2019/08/20||||Optimal Search Segmentation Mechanisms for Online Platform Markets||||Online platforms, such as Airbnb, hotels.com, Amazon, Uber and Lyft, can control and optimize many aspects of product search to improve the efficiency of marketplaces. Here we focus on a common...||||https://arxiv.org/abs/1908.07489v1||||cs||||
591||||None||||Extrapolating Treatment Effects in Multi-Cutoff Regression Discontinuity Designs||||arXiv.org||||2019/02/18||||Extrapolating Treatment Effects in Multi-Cutoff Regression Discontinuity Designs||||Cattaneo, Matias D. || Keele, Luke || Titiunik, Rocio || Vazquez-Bare, Gonzalo||||https://arxiv.org/pdf/1808.04416||||1808.04416||||Regression discontinuity (RD) designs are viewed as one of the most credible identification strategies for program evaluation and causal inference. However, RD treatment effect estimands are necessarily local, making statistical methods for the extrapolation of these effects a key area for development. We introduce a new method for extrapolation of RD effects that relies on the presence of multiple cutoffs, and is therefore design-based. Our approach relies on an easy-to-interpret identifying assumption that mimics the idea of "common trends" in differences-in-differences designs. We use our methods to study the effect of a subsidized loan program on post-education attendance in Colombia, and offer new empirical evidence on the program effects for students with test scores away from the cutoff that determined program eligibility.||||@arxiv||||2018/08/13||||Extrapolating Treatment Effects in Multi-Cutoff Regression...||||Regression discontinuity (RD) designs are viewed as one of the most credible identification strategies for program evaluation and causal inference. However, RD treatment effect estimands are...||||https://arxiv.org/abs/1808.04416v2||||econ||||
592||||None||||A dynamic factor model approach to incorporate Big Data in state space models for official statistics||||arXiv.org||||2019/01/31||||A dynamic factor model approach to incorporate Big Data in state space models for official statistics||||Schiavoni, Caterina || Palm, Franz || Smeekes, Stephan || Brakel, Jan van den||||https://arxiv.org/pdf/1901.11355||||1901.11355||||In this paper we consider estimation of unobserved components in state space models using a dynamic factor approach to incorporate auxiliary information from high-dimensional data sources. We apply the methodology to unemployment estimation as done by Statistics Netherlands, who uses a multivariate state space model to produce monthly figures for the unemployment using series observed with the labour force survey (LFS). We extend the model by including auxiliary series about job search behaviour from Google Trends and claimant counts, partially observed at higher frequencies. Our factor model allows for nowcasting the variable of interest, providing reliable unemployment estimates in real time before LFS data become available.||||@arxiv||||2019/01/31||||A dynamic factor model approach to incorporate Big Data in state...||||In this paper we consider estimation of unobserved components in state space models using a dynamic factor approach to incorporate auxiliary information from high-dimensional data sources. We...||||https://arxiv.org/abs/1901.11355v1||||econ||||
593||||None||||Improving Regression-based Event Study Analysis Using a Topological Machine-learning Method||||arXiv.org||||2019/05/16||||Improving Regression-based Event Study Analysis Using a Topological Machine-learning Method||||Yamashita, Takashi || Miura, Ryozo||||https://arxiv.org/pdf/1905.06536||||1905.06536||||This paper introduces a new correction scheme to a conventional regression-based event study method: a topological machine-learning approach with a self-organizing map (SOM).We use this new scheme to analyze a major market event in Japan and find that the factors of abnormal stock returns can be easily can be easily identified and the event-cluster can be depicted.We also find that a conventional event study method involves an empirical analysis mechanism that tends to derive bias due to its mechanism, typically in an event-clustered market situation. We explain our new correction scheme and apply it to an event in the Japanese market --- the holding disclosure of the Government Pension Investment Fund (GPIF) on July 31, 2015.||||@arxiv||||2019/05/16||||Improving Regression-based Event Study Analysis Using a...||||This paper introduces a new correction scheme to a conventional regression-based event study method: a topological machine-learning approach with a self-organizing map (SOM).We use this new scheme...||||https://arxiv.org/abs/1905.06536v1||||econ||||
594||||None||||Extended opportunity cost model to find near equilibrium electricity prices under non-convexities||||arXiv.org||||2018/09/25||||Extended opportunity cost model to find near equilibrium electricity prices under non-convexities||||Shavandi, Hassan || Pirnia, Mehrdad || Fuller, J. David||||https://arxiv.org/pdf/1809.09734||||1809.09734||||This paper finds near equilibrium prices for electricity markets with nonconvexities due to binary variables, in order to reduce the market participants' opportunity costs, such as generators' unrecovered costs. The opportunity cost is defined as the difference between the profit when the instructions of the market operator are followed and when the market participants can freely make their own decisions based on the market prices. We use the minimum complementarity approximation to the minimum total opportunity cost (MTOC) model, from previous research, with tests on a much more realistic unit commitment (UC) model than in previous research, including features such as reserve requirements, ramping constraints, and minimum up and down times. The developed model incorporates flexible price responsive demand, as in previous research, but since not all demand is price responsive, we consider the more realistic case that total demand is a mixture of fixed and flexible. Another improvement over previous MTOC research is computational: whereas the previous research had nonconvex terms among the objective function's continuous variables, we convert the objective to an equivalent form that contains only linear and convex quadratic terms in the continuous variables. We compare the unit commitment model with the standard social welfare optimization version of UC, in a series of sensitivity analyses, varying flexible demand to represent varying degrees of future penetration of electric vehicles and smart appliances, different ratios of generation availability, and different values of transmission line capacities to consider possible congestion. The minimum total opportunity cost and social welfare solutions are mostly very close in different scenarios, except in some extreme cases.||||@arxiv||||2018/09/25||||Extended opportunity cost model to find near equilibrium...||||This paper finds near equilibrium prices for electricity markets with nonconvexities due to binary variables, in order to reduce the market participants' opportunity costs, such as generators'...||||https://arxiv.org/abs/1809.09734v1||||econ||||
595||||None||||Forecasting the Remittances of the Overseas Filipino Workers in the Philippines||||arXiv.org||||2019/06/25||||Forecasting the Remittances of the Overseas Filipino Workers in the Philippines||||Manayaga, Merry Christ E. || Ceballos, Roel F.||||https://arxiv.org/pdf/1906.10422||||1906.10422||||This study aims to find a Box-Jenkins time series model for the monthly OFW's remittance in the Philippines. Forecasts of OFW's remittance for the years 2018 and 2019 will be generated using the appropriate time series model. The data were retrieved from the official website of Bangko Sentral ng Pilipinas. There are 108 observations, 96 of which were used in model building and the remaining 12 observations were used in forecast evaluation. ACF and PACF were used to examine the stationarity of the series. Augmented Dickey Fuller test was used to confirm the stationarity of the series. The data was found to have a seasonal component, thus, seasonality has been considered in the final model which is SARIMA (2,1,0)x(0,0,2)_12. There are no significant spikes in the ACF and PACF of residuals of the final model and the L-jung Box Q* test confirms further that the residuals of the model are uncorrelated. Also, based on the result of the Shapiro-Wilk test for the forecast errors, the forecast errors can be considered a Gaussian white noise. Considering the results of diagnostic checking and forecast evaluation, SARIMA (2,1,0)x(0,0,2)_12 is an appropriate model for the series. All necessary computations were done using the R statistical software.||||@arxiv||||2019/06/25||||Forecasting the Remittances of the Overseas Filipino Workers in...||||This study aims to find a Box-Jenkins time series model for the monthly OFW's remittance in the Philippines. Forecasts of OFW's remittance for the years 2018 and 2019 will be generated using the...||||https://arxiv.org/abs/1906.10422v1||||econ||||
596||||None||||An alternative quality of life ranking on the basis of remittances||||arXiv.org||||2019/11/12||||An alternative quality of life ranking on the basis of remittances||||Petróczy, Dóra Gréta||||https://arxiv.org/pdf/1809.03977||||1809.03977||||Remittances mean an important connection between people working abroad and their home countries. This paper considers these transfers as a measure of preferences revealed by the workers, underlying a ranking of countries around the world. We use the World Bank bilateral remittances data of international salaries and interpersonal transfers between 2010 and 2015 to compare European countries. The suggested least squares method implies that the ranking is invariant to country sizes and satisfies the axiom of bridge country independence. Our ranking reveals a crucial aspect of quality of life and may become an alternative to various composite indices.||||@arxiv||||2018/09/11||||An alternative quality of life ranking on the basis of remittances||||Remittances mean an important connection between people working abroad and their home countries. This paper considers these transfers as a measure of preferences revealed by the workers,...||||https://arxiv.org/abs/1809.03977v5||||econ||||
597||||None||||Generalized Poisson Difference Autoregressive Processes||||arXiv.org||||2020/02/11||||Generalized Poisson Difference Autoregressive Processes||||Carallo, Giulia || Casarin, Roberto || Robert, Christian P.||||https://arxiv.org/pdf/2002.04470||||2002.04470||||This paper introduces a new stochastic process with values in the set Z of integers with sign. The increments of process are Poisson differences and the dynamics has an autoregressive structure. We study the properties of the process and exploit the thinning representation to derive stationarity conditions and the stationary distribution of the process. We provide a Bayesian inference method and an efficient posterior approximation procedure based on Monte Carlo. Numerical illustrations on both simulated and real data show the effectiveness of the proposed inference.||||@arxiv||||2020/02/11||||Generalized Poisson Difference Autoregressive Processes||||This paper introduces a new stochastic process with values in the set Z of integers with sign. The increments of process are Poisson differences and the dynamics has an autoregressive structure....||||https://arxiv.org/abs/2002.04470v1||||econ||||
598||||None||||The Mittag-Leffler Fitting of the Phillips Curve||||arXiv.org||||2019/09/30||||The Mittag-Leffler Fitting of the Phillips Curve||||Skovranek, Tomas||||https://arxiv.org/pdf/1604.00369||||1604.00369||||In this paper, a mathematical model based on the one-parameter Mittag-Leffler function is proposed to be used for the first time to describe the relation between unemployment rate and inflation rate, also known as the Phillips curve. The Phillips curve is in the literature often represented by an exponential-like shape. On the other hand, Phillips in his fundamental paper used a power function in the model definition. Considering that the ordinary as well as generalised Mittag-Leffler function behaves between a purely exponential function and a power function it is natural to implement it in the definition of the model used to describe the relation between the data representing the Phillips curve. For the modelling purposes the data of two different European economies, France and Switzerland, were used and an "out-of-sample" forecast was done to compare the performance of the Mittag-Leffler model to the performance of the power-type and exponential-type model. The results demonstrate that the ability of the Mittag-Leffler function to fit data that manifest signs of stretched exponentials, oscillations or even damped oscillations can be of use when describing economic relations and phenomenons, such as the Phillips curve.||||@arxiv||||2016/04/01||||The Mittag-Leffler Fitting of the Phillips Curve||||In this paper, a mathematical model based on the one-parameter Mittag-Leffler function is proposed to be used for the first time to describe the relation between unemployment rate and inflation...||||https://arxiv.org/abs/1604.00369v3||||econ||||
599||||None||||Knowledge Graphs for Innovation Ecosystems||||arXiv.org||||2020/01/09||||Knowledge Graphs for Innovation Ecosystems||||Tejero, Alberto || Rodriguez-Doncel, Victor || Pau, Ivan||||https://arxiv.org/pdf/2001.08615||||2001.08615||||Innovation ecosystems can be naturally described as a collection of networked entities, such as experts, institutions, projects, technologies and products. Representing in a machine-readable form these entities and their relations is not entirely attainable, due to the existence of abstract concepts such as knowledge and due to the confidential, non-public nature of this information, but even its partial depiction is of strong interest. The representation of innovation ecosystems incarnated as knowledge graphs would enable the generation of reports with new insights, the execution of advanced data analysis tasks. An ontology to capture the essential entities and relations is presented, as well as the description of data sources, which can be used to populate innovation knowledge graphs. Finally, the application case of the Universidad Politecnica de Madrid is presented, as well as an insight of future applications.||||@arxiv||||2020/01/09||||Knowledge Graphs for Innovation Ecosystems||||Innovation ecosystems can be naturally described as a collection of networked entities, such as experts, institutions, projects, technologies and products. Representing in a machine-readable form...||||https://arxiv.org/abs/2001.08615v1||||cs||||
600||||None||||A lava attack on the recovery of sums of dense and sparse signals||||arXiv.org||||2015/03/25||||A lava attack on the recovery of sums of dense and sparse signals||||Chernozhukov, Victor || Hansen, Christian || Liao, Yuan||||https://arxiv.org/pdf/1502.03155||||1502.03155||||Common high-dimensional methods for prediction rely on having either a sparse signal model, a model in which most parameters are zero and there are a small number of non-zero parameters that are large in magnitude, or a dense signal model, a model with no large parameters and very many small non-zero parameters. We consider a generalization of these two basic models, termed here a "sparse+dense" model, in which the signal is given by the sum of a sparse signal and a dense signal. Such a structure poses problems for traditional sparse estimators, such as the lasso, and for traditional dense estimation methods, such as ridge estimation. We propose a new penalization-based method, called lava, which is computationally efficient. With suitable choices of penalty parameters, the proposed method strictly dominates both lasso and ridge. We derive analytic expressions for the finite-sample risk function of the lava estimator in the Gaussian sequence model. We also provide an deviation bound for the prediction risk in the Gaussian regression model with fixed design. In both cases, we provide Stein's unbiased estimator for lava's prediction risk. A simulation example compares the performance of lava to lasso, ridge, and elastic net in a regression example using feasible, data-dependent penalty parameters and illustrates lava's improved performance relative to these benchmarks.||||@arxiv||||2015/02/11||||A lava attack on the recovery of sums of dense and sparse signals||||Common high-dimensional methods for prediction rely on having either a sparse signal model, a model in which most parameters are zero and there are a small number of non-zero parameters that are...||||https://arxiv.org/abs/1502.03155v2||||cs||||
601||||None||||Consistent Approval-Based Multi-Winner Rules||||arXiv.org||||2019/10/23||||Consistent Approval-Based Multi-Winner Rules||||Lackner, Martin || Skowron, Piotr||||https://arxiv.org/pdf/1704.02453||||1704.02453||||This paper is an axiomatic study of consistent approval-based multi-winner rules, i.e., voting rules that select a fixed-size group of candidates based on approval ballots. We introduce the class of counting rules and provide an axiomatic characterization of this class based on the consistency axiom. Building upon this result, we axiomatically characterize three important consistent multi-winner rules: Proportional Approval Voting, Multi-Winner Approval Voting and the Approval Chamberlin--Courant rule. Our results demonstrate the variety of multi-winner rules and illustrate three different, orthogonal principles that multi-winner voting rules may represent: individual excellence, diversity, and proportionality.||||@arxiv||||2017/04/08||||Consistent Approval-Based Multi-Winner Rules||||This paper is an axiomatic study of consistent approval-based multi-winner rules, i.e., voting rules that select a fixed-size group of candidates based on approval ballots. We introduce the class...||||https://arxiv.org/abs/1704.02453v6||||cs||||
602||||None||||Battling Antibiotic Resistance: Can Machine Learning Improve Prescribing?||||arXiv.org||||2019/06/05||||Battling Antibiotic Resistance: Can Machine Learning Improve Prescribing?||||Ribers, Michael Allan || Ullrich, Hannes||||https://arxiv.org/pdf/1906.03044||||1906.03044||||Antibiotic resistance constitutes a major health threat. Predicting bacterial causes of infections is key to reducing antibiotic misuse, a leading driver of antibiotic resistance. We train a machine learning algorithm on administrative and microbiological laboratory data from Denmark to predict diagnostic test outcomes for urinary tract infections. Based on predictions, we develop policies to improve prescribing in primary care, highlighting the relevance of physician expertise and policy implementation when patient distributions vary over time. The proposed policies delay antibiotic prescriptions for some patients until test results are known and give them instantly to others. We find that machine learning can reduce antibiotic use by 7.42 percent without reducing the number of treated bacterial infections. As Denmark is one of the most conservative countries in terms of antibiotic use, this result is likely to be a lower bound of what can be achieved elsewhere.||||@arxiv||||2019/06/05||||Battling Antibiotic Resistance: Can Machine Learning Improve Prescribing?||||Antibiotic resistance constitutes a major health threat. Predicting bacterial causes of infections is key to reducing antibiotic misuse, a leading driver of antibiotic resistance. We train a...||||https://arxiv.org/abs/1906.03044v1||||cs||||
603||||None||||NAPLES;Mining the lead-lag Relationship from Non-synchronous and High-frequency Data||||arXiv.org||||2020/02/03||||NAPLES;Mining the lead-lag Relationship from Non-synchronous and High-frequency Data||||Ito, Katsuya || Nakagawa, Kei||||https://arxiv.org/pdf/2002.00724||||2002.00724||||In time-series analysis, the term "lead-lag effect" is used to describe a delayed effect on a given time series caused by another time series. lead-lag effects are ubiquitous in practice and are specifically critical in formulating investment strategies in high-frequency trading. At present, there are three major challenges in analyzing the lead-lag effects. First, in practical applications, not all time series are observed synchronously. Second, the size of the relevant dataset and rate of change of the environment is increasingly faster, and it is becoming more difficult to complete the computation within a particular time limit. Third, some lead-lag effects are time-varying and only last for a short period, and their delay lengths are often affected by external factors. In this paper, we propose NAPLES (Negative And Positive lead-lag EStimator), a new statistical measure that resolves all these problems. Through experiments on artificial and real datasets, we demonstrate that NAPLES has a strong correlation with the actual lead-lag effects, including those triggered by significant macroeconomic announcements.||||@arxiv||||2020/02/03||||NAPLES;Mining the lead-lag Relationship from Non-synchronous and...||||In time-series analysis, the term "lead-lag effect" is used to describe a delayed effect on a given time series caused by another time series. lead-lag effects are ubiquitous in practice and are...||||https://arxiv.org/abs/2002.00724v1||||econ||||
604||||None||||Quantifying Health Shocks Over the Life Cycle||||arXiv.org||||2018/01/26||||Quantifying Health Shocks Over the Life Cycle||||Fukai, Taiyo || Ichimura, Hidehiko || Kanazawa, Kyogo||||https://arxiv.org/pdf/1801.08746||||1801.08746||||We first show (1) the importance of investigating health expenditure process using the order two Markov chain model, rather than the standard order one model, which is widely used in the literature. Markov chain of order two is the minimal framework that is capable of distinguishing those who experience a certain health expenditure level for the first time from those who have been experiencing that or other levels for some time. In addition, using the model we show (2) that the probability of encountering a health shock first de- creases until around age 10, and then increases with age, particularly, after age 40, (3) that health shock distributions among different age groups do not differ until their percentiles reach the median range, but that above the median the health shock distributions of older age groups gradually start to first-order dominate those of younger groups, and (4) that the persistency of health shocks also shows a U-shape in relation to age.||||@arxiv||||2018/01/26||||Quantifying Health Shocks Over the Life Cycle||||We first show (1) the importance of investigating health expenditure process using the order two Markov chain model, rather than the standard order one model, which is widely used in the...||||https://arxiv.org/abs/1801.08746v1||||econ||||
605||||None||||Duesenberry's Theory of Consumption: Habit, Learning, and Ratcheting||||arXiv.org||||2018/12/25||||Duesenberry's Theory of Consumption: Habit, Learning, and Ratcheting||||Choi, Kyoung Jin || Jeon, Junkee || Koo, Hyeng Keun||||https://arxiv.org/pdf/1812.10038||||1812.10038||||This paper investigates the consumption and risk taking decision of an economic agent with partial irreversibility of consumption decision by formalizing the theory proposed by Duesenberry (1949). The optimal policies exhibit a type of the (s, S) policy: there are two wealth thresholds within which consumption stays constant. Consumption increases or decreases at the thresholds and after the adjustment new thresholds are set. The share of risky investment in the agent's total investment is inversely U-shaped within the (s, S) band, which generates time-varying risk aversion that can fluctuate widely over time. This property can explain puzzles and questions on asset pricing and households' portfolio choices, e.g., why aggregate consumption is so smooth whereas the high equity premium is high and the equity return has high volatility, why the risky share is so low whereas the estimated risk aversion by the micro-level data is small, and whether and when an increase in wealth has an impact on the risky share. Also, the partial irreversibility model can explain both the excess sensitivity and the excess smoothness of consumption.||||@arxiv||||2018/12/25||||Duesenberry's Theory of Consumption: Habit, Learning, and Ratcheting||||This paper investigates the consumption and risk taking decision of an economic agent with partial irreversibility of consumption decision by formalizing the theory proposed by Duesenberry (1949)....||||https://arxiv.org/abs/1812.10038v1||||econ||||
606||||None||||Journal ranking should depend on the level of aggregation||||arXiv.org||||2019/09/02||||Journal ranking should depend on the level of aggregation||||Csató, László||||https://arxiv.org/pdf/1904.06300||||1904.06300||||Journal ranking is becoming more important in assessing the quality of academic research. Several indices have been suggested for this purpose, typically on the basis of a citation graph between the journals. We follow an axiomatic approach and find an impossibility theorem: any self-consistent ranking method, which satisfies a natural monotonicity property, should depend on the level of aggregation. Our result presents a trade-off between two axiomatic properties and reveals a dilemma of aggregation.||||@arxiv||||2019/04/08||||Journal ranking should depend on the level of aggregation||||Journal ranking is becoming more important in assessing the quality of academic research. Several indices have been suggested for this purpose, typically on the basis of a citation graph between...||||https://arxiv.org/abs/1904.06300v4||||cs||||
607||||None||||Inefficiencies in Digital Advertising Markets||||arXiv.org||||2019/12/19||||Inefficiencies in Digital Advertising Markets||||Gordon, Brett R || Jerath, Kinshuk || Katona, Zsolt || Narayanan, Sridhar || Shin, Jiwoong || Wilbur, Kenneth C||||https://arxiv.org/pdf/1912.09012||||1912.09012||||Digital advertising markets are growing and attracting increased scrutiny. This paper explores four market inefficiencies that remain poorly understood: ad effect measurement, frictions between and within advertising channel members, ad blocking and ad fraud. These topics are not unique to digital advertising, but each manifests in new ways in markets for digital ads. We focus on relevant findings in the academic literature, recent developments in practice, and promising topics for future research.||||@arxiv||||2019/12/19||||Inefficiencies in Digital Advertising Markets||||Digital advertising markets are growing and attracting increased scrutiny. This paper explores four market inefficiencies that remain poorly understood: ad effect measurement, frictions between...||||https://arxiv.org/abs/1912.09012v1||||econ||||
608||||None||||Two-way fixed effects estimators with heterogeneous treatment effects||||arXiv.org||||2019/10/30||||Two-way fixed effects estimators with heterogeneous treatment effects||||de Chaisemartin, Clément || D'Haultfœuille, Xavier||||https://arxiv.org/pdf/1803.08807||||1803.08807||||Linear regressions with period and group fixed effects are widely used to estimate treatment effects. We show that they estimate weighted sums of the average treatment effects (ATE) in each group and period, with weights that may be negative. Due to the negative weights, the linear regression coefficient may for instance be negative while all the ATEs are positive. We propose another estimator that solves this issue. In the two applications we revisit, it is significantly different from the linear regression estimator.||||@arxiv||||2018/03/21||||Two-way fixed effects estimators with heterogeneous treatment effects||||Linear regressions with period and group fixed effects are widely used to estimate treatment effects. We show that they estimate weighted sums of the average treatment effects (ATE) in each group...||||https://arxiv.org/abs/1803.08807v6||||econ||||
609||||None||||Prices, Profits, and Production: Identification and Counterfactuals||||arXiv.org||||2019/01/21||||Prices, Profits, and Production: Identification and Counterfactuals||||Aguiar, Victor H. || Allen, Roy || Kashaev, Nail||||https://arxiv.org/pdf/1810.04697||||1810.04697||||This paper studies nonparametric identification and counterfactual bounds for heterogeneous firms that can be ranked in terms of productivity. We require observation of profits or other optimizing-values such as costs or revenues, and either prices or attributes that determine prices. We extend classical duality results for price-taking firms to a setup with rich heterogeneity, and with limited variation in prices. We characterize the identified set for production sets, and provide conditions that ensure point identification. We present a general computationally-feasible framework for sharp counterfactual bounds, such as bounds on quantities at a counterfactual price. We show that existing convergence results for quantile estimators may be directly converted to convergence results for production sets, which facilitates statistical inference.||||@arxiv||||2018/10/10||||Prices, Profits, and Production: Identification and Counterfactuals||||This paper studies nonparametric identification and counterfactual bounds for heterogeneous firms that can be ranked in terms of productivity. We require observation of profits or other...||||https://arxiv.org/abs/1810.04697v2||||econ||||
610||||None||||Machine Learning for Set-Identified Linear Models||||arXiv.org||||2019/12/06||||Machine Learning for Set-Identified Linear Models||||Semenova, Vira||||https://arxiv.org/pdf/1712.10024||||1712.10024||||This paper provides estimation and inference methods for an identified set where the selection among a very large number of covariates is based on modern machine learning tools. I characterize the boundary of the identified set (i.e., support function) using a semiparametric moment condition. Combining Neyman-orthogonality and sample splitting ideas, I construct a root-N consistent, uniformly asymptotically Gaussian estimator of the support function and propose a weighted bootstrap procedure to conduct inference about the identified set. I provide a general method to construct a Neyman-orthogonal moment condition for the support function. Applying my method to Lee (2008)'s endogenous selection model, I provide the asymptotic theory for the sharp (i.e., the tightest possible) bounds on the Average Treatment Effect in the presence of high-dimensional covariates. Furthermore, I relax the conventional monotonicity assumption and allow the sign of the treatment effect on the selection (e.g., employment) to be determined by covariates. Using JobCorps data set with very rich baseline characteristics, I substantially tighten the bounds on the JobCorps effect on wages under weakened monotonicity assumption.||||@arxiv||||2017/12/28||||Machine Learning for Set-Identified Linear Models||||This paper provides estimation and inference methods for an identified set where the selection among a very large number of covariates is based on modern machine learning tools. I characterize the...||||https://arxiv.org/abs/1712.10024v3||||cs||||
611||||None||||Killer Technologies: the destructive creation in the technical change||||arXiv.org||||2019/07/29||||Killer Technologies: the destructive creation in the technical change||||Coccia, Mario||||https://arxiv.org/pdf/1907.12406||||1907.12406||||Killer technology is a radical innovation, based on new products and/or processes, that with high technical and/or economic performance destroys the usage value of established techniques previously sold and used. Killer technology is a new concept in economics of innovation that may be useful for bringing a new perspective to explain and generalize the behavior and characteristics of innovations that generate a destructive creation for sustaining technical change. To explore the behavior of killer technologies, a simple model is proposed to analyze and predict how killer technologies destroy and substitute established technologies. Empirical evidence of this theoretical framework is based on historical data on the evolution of some example technologies. Theoretical framework and empirical evidence hint at general properties of the behavior of killer technologies to explain corporate, industrial, economic and social change and to support best practices for technology management of firms and innovation policy of nations. Overall, then, the proposed theoretical framework can lay a foundation for the development of more sophisticated concepts to explain the behavior of vital technologies that generate technological and industrial change in society.||||@arxiv||||2019/07/29||||Killer Technologies: the destructive creation in the technical change||||Killer technology is a radical innovation, based on new products and/or processes, that with high technical and/or economic performance destroys the usage value of established techniques...||||https://arxiv.org/abs/1907.12406v1||||econ||||
612||||None||||A Residual Bootstrap for Conditional Expected Shortfall||||arXiv.org||||2018/11/26||||A Residual Bootstrap for Conditional Expected Shortfall||||Heinemann, Alexander || Telg, Sean||||https://arxiv.org/pdf/1811.11557||||1811.11557||||This paper studies a fixed-design residual bootstrap method for the two-step estimator of Francq and Zakoïan (2015) associated with the conditional Expected Shortfall. For a general class of volatility models the bootstrap is shown to be asymptotically valid under the conditions imposed by Beutner et al. (2018). A simulation study is conducted revealing that the average coverage rates are satisfactory for most settings considered. There is no clear evidence to have a preference for any of the three proposed bootstrap intervals. This contrasts results in Beutner et al. (2018) for the VaR, for which the reversed-tails interval has a superior performance.||||@arxiv||||2018/11/26||||A Residual Bootstrap for Conditional Expected Shortfall||||This paper studies a fixed-design residual bootstrap method for the two-step estimator of Francq and Zakoïan (2015) associated with the conditional Expected Shortfall. For a general class of...||||https://arxiv.org/abs/1811.11557v1||||econ||||
613||||None||||The Role of the Propensity Score in Fixed Effect Models||||arXiv.org||||2019/04/14||||The Role of the Propensity Score in Fixed Effect Models||||Arkhangelsky, Dmitry || Imbens, Guido||||https://arxiv.org/pdf/1807.02099||||1807.02099||||We develop a new approach for estimating average treatment effects in the observational studies with unobserved group-level heterogeneity. A common approach in such settings is to use linear fixed effect specifications estimated by least squares regression. Such methods severely limit the extent of the heterogeneity between groups by making the restrictive assumption that linearly adjusting for differences between groups in average covariate values addresses all concerns with cross-group comparisons. We start by making two observations. First we note that the fixed effect method in effect adjusts only for differences between groups by adjusting for the average of covariate values and average treatment. Second, we note that weighting by the inverse of the propensity score would remove biases for comparisons between treated and control units under the fixed effect set up. We then develop three generalizations of the fixed effect approach based on these two observations. First, we suggest more general, nonlinear, adjustments for the average covariate values. Second, we suggest robustifying the estimators by using propensity score weighting. Third, we motivate and develop implementations for adjustments that also adjust for group characteristics beyond the average covariate values.||||@arxiv||||2018/07/05||||The Role of the Propensity Score in Fixed Effect Models||||We develop a new approach for estimating average treatment effects in the observational studies with unobserved group-level heterogeneity. A common approach in such settings is to use linear fixed...||||https://arxiv.org/abs/1807.02099v6||||econ||||
614||||None||||Capital Structure and Speed of Adjustment in U.S. Firms. A Comparative Study in Microeconomic and Macroeconomic Conditions - A Quantille Regression Approach||||arXiv.org||||2018/11/11||||Capital Structure and Speed of Adjustment in U.S. Firms. A Comparative Study in Microeconomic and Macroeconomic Conditions - A Quantille Regression Approach||||Kaloudis, Andreas || Tsolis, Dimitrios||||https://arxiv.org/pdf/1811.04473||||1811.04473||||The major perspective of this paper is to provide more evidence regarding how "quickly", in different macroeconomic states, companies adjust their capital structure to their leverage targets. This study extends the empirical research on the topic of capital structure by focusing on a quantile regression method to investigate the behavior of firm-specific characteristics and macroeconomic factors across all quantiles of distribution of leverage (book leverage and market leverage). Therefore, depending on a partial adjustment model, we find that the adjustment speed fluctuated in different stages of book versus market leverage. Furthermore, while macroeconomic states change, we detect clear differentiations of the contribution and the effects of the firm-specific and the macroeconomic variables between market leverage and book leverage debt ratios. Consequently, we deduce that across different macroeconomic states the nature and maturity of borrowing influence the persistence and endurance of the relation between determinants and borrowing.||||@arxiv||||2018/11/11||||Capital Structure and Speed of Adjustment in U.S. Firms. A...||||The major perspective of this paper is to provide more evidence regarding how "quickly", in different macroeconomic states, companies adjust their capital structure to their leverage targets. This...||||https://arxiv.org/abs/1811.04473v1||||econ||||
615||||None||||Efficient Difference-in-Differences Estimation with High-Dimensional Common Trend Confounding||||arXiv.org||||2019/07/16||||Efficient Difference-in-Differences Estimation with High-Dimensional Common Trend Confounding||||Zimmert, Michael||||https://arxiv.org/pdf/1809.01643||||1809.01643||||In this study, we consider difference-in-differences models with a common trend assumption that is only valid after conditioning on covariates. We suggest novel two-step estimators that allow the covariates to enter the model in a very flexible form. In particular, the first stages can be estimated using supervised machine learning methods. We derive asymptotic results for new semiparametric and linear model based estimators for repeated cross-sections and panel data and show that they have desirable statistical properties like asymptotic normality and double robustness. Further, we establish semiparametric efficiency bounds for difference-in-differences estimation with cross-sectional and panel data. The proposed semiparametric estimators attain the bounds. The usability of the methods is assessed by replicating a study on an employment protection reform. We demonstrate that the notion of high-dimensional common trend confounding has implications for the economic interpretation of the policy evaluation results. Notably, measured reform effects are substantially decreased or even reversed when covariates are included in a data-driven manner.||||@arxiv||||2018/09/05||||Efficient Difference-in-Differences Estimation with...||||In this study, we consider difference-in-differences models with a common trend assumption that is only valid after conditioning on covariates. We suggest novel two-step estimators that allow the...||||https://arxiv.org/abs/1809.01643v4||||econ||||
616||||None||||Income growth with high inequality implies loss of well-being: A mathematical model||||arXiv.org||||2019/11/25||||Income growth with high inequality implies loss of well-being: A mathematical model||||Córdova-Lepe, Fernando||||https://arxiv.org/pdf/1911.11205||||1911.11205||||A mathematical model of measurement of the perception of well-being for groups with increasing incomes, but proportionally unequal is proposed. Assuming that welfare grows with own income and decreases with relative inequality (income of the other concerning one's own), possible scenarios for long-term behavior in welfare functions are concluded. Also, it is proved that a high relative inequality (parametric definition) always implies the loss of the self-perception of the well-being of the most disadvantaged group.||||@arxiv||||2019/11/25||||Income growth with high inequality implies loss of well-being: A...||||A mathematical model of measurement of the perception of well-being for groups with increasing incomes, but proportionally unequal is proposed. Assuming that welfare grows with own income and...||||https://arxiv.org/abs/1911.11205v1||||econ||||
617||||None||||Forecasting the impact of state pension reforms in post-Brexit England and Wales using microsimulation and deep learning||||arXiv.org||||2018/04/21||||Forecasting the impact of state pension reforms in post-Brexit England and Wales using microsimulation and deep learning||||Werpachowska, Agnieszka||||https://arxiv.org/pdf/1802.09427||||1802.09427||||We employ stochastic dynamic microsimulations to analyse and forecast the pension cost dependency ratio for England and Wales from 1991 to 2061, evaluating the impact of the ongoing state pension reforms and changes in international migration patterns under different Brexit scenarios. To fully account for the recently observed volatility in life expectancies, we propose mortality rate model based on deep learning techniques, which discovers complex patterns in data and extrapolated trends. Our results show that the recent reforms can effectively stave off the "pension crisis" and bring back the system on a sounder fiscal footing. At the same time, increasingly more workers can expect to spend greater share of their lifespan in retirement, despite the eligibility age rises. The population ageing due to the observed postponement of death until senectitude often occurs with the compression of morbidity, and thus will not, perforce, intrinsically strain healthcare costs. To a lesser degree, the future pension cost dependency ratio will depend on the post-Brexit relations between the UK and the EU, with "soft" alignment on the free movement lowering the relative cost of the pension system compared to the "hard" one. In the long term, however, the ratio has a rising tendency.||||@arxiv||||2018/02/06||||Forecasting the impact of state pension reforms in post-Brexit...||||We employ stochastic dynamic microsimulations to analyse and forecast the pension cost dependency ratio for England and Wales from 1991 to 2061, evaluating the impact of the ongoing state pension...||||https://arxiv.org/abs/1802.09427v2||||econ||||
618||||None||||Speculative Trading, Prospect Theory and Transaction Costs||||arXiv.org||||2019/11/22||||Speculative Trading, Prospect Theory and Transaction Costs||||Tse, Alex S. L. || Zheng, Harry||||https://arxiv.org/pdf/1911.10106||||1911.10106||||A speculative agent with Prospect Theory preference chooses the optimal time to purchase and then to sell an indivisible risky asset as to maximize the expected utility of the round-trip profit net of transaction costs. The optimization problem is formulated as a sequential optimal stopping problem and we provide a complete characterization of the solution. Depending on the preference and market parameters as well as the initial price of the asset, the optimal strategy can be "buy and hold", "buy low sell high", "buy high sell higher" or "no trading". Transaction costs do not necessarily curb speculative trading. For example, while a large proportional transaction cost on sale can unambiguously suppress trading participation, introducing a fixed market entry fee will indeed encourage trading when the asset price level is high.||||@arxiv||||2019/11/22||||Speculative Trading, Prospect Theory and Transaction Costs||||A speculative agent with Prospect Theory preference chooses the optimal time to purchase and then to sell an indivisible risky asset as to maximize the expected utility of the round-trip profit...||||https://arxiv.org/abs/1911.10106v1||||econ||||
619||||None||||Finite Time Identification in Unstable Linear Systems||||arXiv.org||||2018/06/05||||Finite Time Identification in Unstable Linear Systems||||Faradonbeh, Mohamad Kazem Shirani || Tewari, Ambuj || Michailidis, George||||https://arxiv.org/pdf/1710.01852||||1710.01852||||Identification of the parameters of stable linear dynamical systems is a well-studied problem in the literature, both in the low and high-dimensional settings. However, there are hardly any results for the unstable case, especially regarding finite time bounds. For this setting, classical results on least-squares estimation of the dynamics parameters are not applicable and therefore new concepts and technical approaches need to be developed to address the issue. Unstable linear systems arise in key real applications in control theory, econometrics, and finance. This study establishes finite time bounds for the identification error of the least-squares estimates for a fairly large class of heavy-tailed noise distributions, and transition matrices of such systems. The results relate the time length (samples) required for estimation to a function of the problem dimension and key characteristics of the true underlying transition matrix and the noise distribution. To establish them, appropriate concentration inequalities for random matrices and for sequences of martingale differences are leveraged.||||@arxiv||||2017/10/05||||Finite Time Identification in Unstable Linear Systems||||Identification of the parameters of stable linear dynamical systems is a well-studied problem in the literature, both in the low and high-dimensional settings. However, there are hardly any...||||https://arxiv.org/abs/1710.01852v2||||cs||||
620||||None||||Optimal Dynamic Auctions are Virtual Welfare Maximizers||||arXiv.org||||2018/12/07||||Optimal Dynamic Auctions are Virtual Welfare Maximizers||||Mirrokni, Vahab || Leme, Renato Paes || Tang, Pingzhong || Zuo, Song||||https://arxiv.org/pdf/1812.02993||||1812.02993||||We are interested in the setting where a seller sells sequentially arriving items, one per period, via a dynamic auction. At the beginning of each period, each buyer draws a private valuation for the item to be sold in that period and this valuation is independent across buyers and periods. The auction can be dynamic in the sense that the auction at period $t$ can be conditional on the bids in that period and all previous periods, subject to certain appropriately defined incentive compatible and individually rational conditions. Perhaps not surprisingly, the revenue optimal dynamic auctions are computationally hard to find and existing literatures that aim to approximate the optimal auctions are all based on solving complex dynamic programs. It remains largely open on the structural interpretability of the optimal dynamic auctions.   In this paper, we show that any optimal dynamic auction is a virtual welfare maximizer subject to some monotone allocation constraints. In particular, the explicit definition of the virtual value function above arises naturally from the primal-dual analysis by relaxing the monotone constraints. We further develop an ironing technique that gets rid of the monotone allocation constraints. Quite different from Myerson's ironing approach, our technique is more technically involved due to the interdependence of the virtual value functions across buyers. We nevertheless show that ironing can be done approximately and efficiently, which in turn leads to a Fully Polynomial Time Approximation Scheme of the optimal dynamic auction.||||@arxiv||||2018/12/07||||Optimal Dynamic Auctions are Virtual Welfare Maximizers||||We are interested in the setting where a seller sells sequentially arriving items, one per period, via a dynamic auction. At the beginning of each period, each buyer draws a private valuation for...||||https://arxiv.org/abs/1812.02993v1||||cs||||
621||||None||||A Unified Approach on the Local Power of Panel Unit Root Tests||||arXiv.org||||2017/10/09||||A Unified Approach on the Local Power of Panel Unit Root Tests||||Liang, Zhongwen||||https://arxiv.org/pdf/1710.02944||||1710.02944||||In this paper, a unified approach is proposed to derive the exact local asymptotic power for panel unit root tests, which is one of the most important issues in nonstationary panel data literature. Two most widely used panel unit root tests known as Levin-Lin-Chu (LLC, Levin, Lin and Chu (2002)) and Im-Pesaran-Shin (IPS, Im, Pesaran and Shin (2003)) tests are systematically studied for various situations to illustrate our method. Our approach is characteristic function based, and can be used directly in deriving the moments of the asymptotic distributions of these test statistics under the null and the local-to-unity alternatives. For the LLC test, the approach provides an alternative way to obtain the results that can be derived by the existing method. For the IPS test, the new results are obtained, which fills the gap in the literature where few results exist, since the IPS test is non-admissible. Moreover, our approach has the advantage in deriving Edgeworth expansions of these tests, which are also given in the paper. The simulations are presented to illustrate our theoretical findings.||||@arxiv||||2017/10/09||||A Unified Approach on the Local Power of Panel Unit Root Tests||||In this paper, a unified approach is proposed to derive the exact local asymptotic power for panel unit root tests, which is one of the most important issues in nonstationary panel data...||||https://arxiv.org/abs/1710.02944v1||||econ||||
622||||None||||Nonparametric Identification and Estimation with Independent, Discrete Instruments||||arXiv.org||||2019/06/12||||Nonparametric Identification and Estimation with Independent, Discrete Instruments||||Loh, Isaac||||https://arxiv.org/pdf/1906.05231||||1906.05231||||In a nonparametric instrumental regression model, we strengthen the conventional moment independence assumption towards full statistical independence between instrument and error term. This allows us to prove identification results and develop estimators for a structural function of interest when the instrument is discrete, and in particular binary. When the regressor of interest is also discrete with more mass points than the instrument, we state straightforward conditions under which the structural function is partially identified, and give modified assumptions which imply point identification. These stronger assumptions are shown to hold outside of a small set of conditional moments of the error term. Estimators for the identified set are given when the structural function is either partially or point identified. When the regressor is continuously distributed, we prove that if the instrument induces a sufficiently rich variation in the joint distribution of the regressor and error term then point identification of the structural function is still possible. This approach is relatively tractable, and under some standard conditions we demonstrate that our point identifying assumption holds on a topologically generic set of density functions for the joint distribution of regressor, error, and instrument. Our method also applies to a well-known nonparametric quantile regression framework, and we are able to state analogous point identification results in that context.||||@arxiv||||2019/06/12||||Nonparametric Identification and Estimation with Independent,...||||In a nonparametric instrumental regression model, we strengthen the conventional moment independence assumption towards full statistical independence between instrument and error term. This allows...||||https://arxiv.org/abs/1906.05231v1||||econ||||
623||||None||||RNN-based counterfactual prediction||||arXiv.org||||2019/04/13||||RNN-based counterfactual prediction||||Poulos, Jason||||https://arxiv.org/pdf/1712.03553||||1712.03553||||This paper proposes an alternative to the synthetic control method (SCM) for estimating the effect of a policy intervention on an outcome over time. Recurrent neural networks (RNNs) are used to predict the counterfactual outcomes of treated units using only the outcomes of control units as predictors. This approach is less susceptible to $p$-hacking because it does not require the researcher to choose predictors or pre-intervention covariates to construct the synthetic control. RNNs do not assume a functional form, can learn nonconvex combinations of control units, and are specifically structured to exploit temporal dependencies in sequential data. I apply the approach to the problem of estimating the long-run impacts of U.S. homestead policy on public school spending.||||@arxiv||||2017/12/10||||RNN-based counterfactual prediction||||This paper proposes an alternative to the synthetic control method (SCM) for estimating the effect of a policy intervention on an outcome over time. Recurrent neural networks (RNNs) are used to...||||https://arxiv.org/abs/1712.03553v5||||econ||||
624||||None||||Robust Reserve Pricing in Auctions under Mean Constraints||||arXiv.org||||2019/11/16||||Robust Reserve Pricing in Auctions under Mean Constraints||||Che, Ethan||||https://arxiv.org/pdf/1911.07103||||1911.07103||||We study a seller who sets a reserve price in a second price auction with uncertainty over the joint distribution of bidders' valuations. The seller only knows the mean of the marginal distribution of each bidder's valuation and the range, and an adversarial nature chooses the worst-case distribution within this ambiguity set. We use a dual characterization to solve for this distribution. We find that the seller's optimal reserve price tends to be low and converges to zero in probability as the number of bidders increases.||||@arxiv||||2019/11/16||||Robust Reserve Pricing in Auctions under Mean Constraints||||We study a seller who sets a reserve price in a second price auction with uncertainty over the joint distribution of bidders' valuations. The seller only knows the mean of the marginal...||||https://arxiv.org/abs/1911.07103v1||||econ||||
625||||None||||Inference on Counterfactual Distributions||||arXiv.org||||2013/09/18||||Inference on Counterfactual Distributions||||Chernozhukov, Victor || Fernandez-Val, Ivan || Melly, Blaise||||https://arxiv.org/pdf/0904.0951||||0904.0951||||Counterfactual distributions are important ingredients for policy analysis and decomposition analysis in empirical economics. In this article we develop modeling and inference tools for counterfactual distributions based on regression methods. The counterfactual scenarios that we consider consist of ceteris paribus changes in either the distribution of covariates related to the outcome of interest or the conditional distribution of the outcome given covariates. For either of these scenarios we derive joint functional central limit theorems and bootstrap validity results for regression-based estimators of the status quo and counterfactual outcome distributions. These results allow us to construct simultaneous confidence sets for function-valued effects of the counterfactual changes, including the effects on the entire distribution and quantile functions of the outcome as well as on related functionals. These confidence sets can be used to test functional hypotheses such as no-effect, positive effect, or stochastic dominance. Our theory applies to general counterfactual changes and covers the main regression methods including classical, quantile, duration, and distribution regressions. We illustrate the results with an empirical application to wage decompositions using data for the United States.   As a part of developing the main results, we introduce distribution regression as a comprehensive and flexible tool for modeling and estimating the \textit{entire} conditional distribution. We show that distribution regression encompasses the Cox duration regression and represents a useful alternative to quantile regression. We establish functional central limit theorems and bootstrap validity results for the empirical distribution regression process and various related functionals.||||@arxiv||||2009/04/06||||Inference on Counterfactual Distributions||||Counterfactual distributions are important ingredients for policy analysis and decomposition analysis in empirical economics. In this article we develop modeling and inference tools for...||||https://arxiv.org/abs/0904.0951v6||||econ||||
626||||None||||Evidence for Gross Domestic Product growth time delay dependence over Foreign Direct Investment. A time-lag dependent correlation study||||arXiv.org||||2019/05/05||||Evidence for Gross Domestic Product growth time delay dependence over Foreign Direct Investment. A time-lag dependent correlation study||||Ausloos, Marcel || Eskandary, Ali || Kaur, Parmjit || Dhesi, Gurjeet||||https://arxiv.org/pdf/1905.01617||||1905.01617||||This paper considers an often forgotten relationship, the time delay between a cause and its effect in economies and finance. We treat the case of Foreign Direct Investment (FDI) and economic growth, - measured through a country Gross Domestic Product (GDP). The pertinent data refers to 43 countries, over 1970-2015, - for a total of 4278 observations. When countries are grouped   according to the Inequality-Adjusted Human Development Index (IHDI), it is found that a time lag dependence effect exists in FDI-GDP correlations.   This is established through a time-dependent Pearson 's product-moment correlation coefficient matrix.   Moreover, such a Pearson correlation coefficient is observed to evolve from positive   to negative values depending on the IHDI, from low to high. It is "politically and policy   "relevant" that   the correlation is statistically significant providing the time lag is less than 3 years. A "rank-size" law is demonstrated.   It is recommended to reconsider such a time lag effect when discussing previous analyses whence conclusions on international business, and thereafter on forecasting.||||@arxiv||||2019/05/05||||Evidence for Gross Domestic Product growth time delay dependence...||||This paper considers an often forgotten relationship, the time delay between a cause and its effect in economies and finance. We treat the case of Foreign Direct Investment (FDI) and economic...||||https://arxiv.org/abs/1905.01617v1||||econ||||
627||||None||||Sparse Models and Methods for Optimal Instruments with an Application to Eminent Domain||||arXiv.org||||2015/04/19||||Sparse Models and Methods for Optimal Instruments with an Application to Eminent Domain||||Belloni, Alexandre || Chen, Daniel || Chernozhukov, Victor || Hansen, Christian||||https://arxiv.org/pdf/1010.4345||||1010.4345||||We develop results for the use of Lasso and Post-Lasso methods to form first-stage predictions and estimate optimal instruments in linear instrumental variables (IV) models with many instruments, $p$. Our results apply even when $p$ is much larger than the sample size, $n$. We show that the IV estimator based on using Lasso or Post-Lasso in the first stage is root-n consistent and asymptotically normal when the first-stage is approximately sparse; i.e. when the conditional expectation of the endogenous variables given the instruments can be well-approximated by a relatively small set of variables whose identities may be unknown. We also show the estimator is semi-parametrically efficient when the structural error is homoscedastic. Notably our results allow for imperfect model selection, and do not rely upon the unrealistic "beta-min" conditions that are widely used to establish validity of inference following model selection. In simulation experiments, the Lasso-based IV estimator with a data-driven penalty performs well compared to recently advocated many-instrument-robust procedures. In an empirical example dealing with the effect of judicial eminent domain decisions on economic outcomes, the Lasso-based IV estimator outperforms an intuitive benchmark.   In developing the IV results, we establish a series of new results for Lasso and Post-Lasso estimators of nonparametric conditional expectation functions which are of independent theoretical and practical interest. We construct a modification of Lasso designed to deal with non-Gaussian, heteroscedastic disturbances which uses a data-weighted $\ell_1$-penalty function. Using moderate deviation theory for self-normalized sums, we provide convergence rates for the resulting Lasso and Post-Lasso estimators that are as sharp as the corresponding rates in the homoscedastic Gaussian case under the condition that $\log p = o(n^{1/3})$.||||@arxiv||||2010/10/21||||Sparse Models and Methods for Optimal Instruments with an...||||We develop results for the use of Lasso and Post-Lasso methods to form first-stage predictions and estimate optimal instruments in linear instrumental variables (IV) models with many instruments,...||||https://arxiv.org/abs/1010.4345v5||||econ||||
628||||None||||On Existence of Equilibrium Under Social Coalition Structures||||arXiv.org||||2019/10/10||||On Existence of Equilibrium Under Social Coalition Structures||||Caskurlu, Bugra || Ekici, Ozgun || Kizilkaya, Fatih Erdem||||https://arxiv.org/pdf/1910.04648||||1910.04648||||In a strategic form game a strategy profile is an equilibrium if no viable coalition of agents (or players) benefits (in the Pareto sense) from jointly changing their strategies. Weaker or stronger equilibrium notions can be defined by considering various restrictions on coalition formation. In a Nash equilibrium, for instance, the assumption is that viable coalitions are singletons, and in a super strong equilibrium, every coalition is viable. Restrictions on coalition formation can be justified by communication limitations, coordination problems or institutional constraints. In this paper, inspired by social structures in various real-life scenarios, we introduce certain restrictions on coalition formation, and on their basis we introduce a number of equilibrium notions. As an application we study our equilibrium notions in resource selection games (RSGs), and we present a complete set of existence and non-existence results for general RSGs and their important special cases.||||@arxiv||||2019/10/10||||On Existence of Equilibrium Under Social Coalition Structures||||In a strategic form game a strategy profile is an equilibrium if no viable coalition of agents (or players) benefits (in the Pareto sense) from jointly changing their strategies. Weaker or...||||https://arxiv.org/abs/1910.04648v1||||cs||||
629||||None||||Validating Weak-form Market Efficiency in United States Stock Markets with Trend Deterministic Price Data and Machine Learning||||arXiv.org||||2019/09/11||||Validating Weak-form Market Efficiency in United States Stock Markets with Trend Deterministic Price Data and Machine Learning||||Showalter, Samuel || Gropp, Jeffrey||||https://arxiv.org/pdf/1909.05151||||1909.05151||||The Efficient Market Hypothesis has been a staple of economics research for decades. In particular, weak-form market efficiency -- the notion that past prices cannot predict future performance -- is strongly supported by econometric evidence. In contrast, machine learning algorithms implemented to predict stock price have been touted, to varying degrees, as successful. Moreover, some data scientists boast the ability to garner above-market returns using price data alone. This study endeavors to connect existing econometric research on weak-form efficient markets with data science innovations in algorithmic trading. First, a traditional exploration of stationarity in stock index prices over the past decade is conducted with Augmented Dickey-Fuller and Variance Ratio tests. Then, an algorithmic trading platform is implemented with the use of five machine learning algorithms. Econometric findings identify potential stationarity, hinting technical evaluation may be possible, though algorithmic trading results find little predictive power in any machine learning model, even when using trend-specific metrics. Accounting for transaction costs and risk, no system achieved above-market returns consistently. Our findings reinforce the validity of weak-form market efficiency.||||@arxiv||||2019/09/11||||Validating Weak-form Market Efficiency in United States Stock...||||The Efficient Market Hypothesis has been a staple of economics research for decades. In particular, weak-form market efficiency -- the notion that past prices cannot predict future performance --...||||https://arxiv.org/abs/1909.05151v1||||cs||||
630||||None||||Testing Ambiguity and Machina Preferences Within a Quantum-theoretic Framework for Decision-making||||arXiv.org||||2017/06/06||||Testing Ambiguity and Machina Preferences Within a Quantum-theoretic Framework for Decision-making||||Aerts, Diederik || Geriente, Suzette || Moreira, Catarina || Sozzo, Sandro||||https://arxiv.org/pdf/1706.02168||||1706.02168||||The Machina thought experiments pose to major non-expected utility models challenges that are similar to those posed by the Ellsberg thought experiments to subjective expected utility theory (SEUT). We test human choices in the `Ellsberg three-color example', confirming typical ambiguity aversion patterns, and the `Machina 50/51 and reflection examples', partially confirming the preferences hypothesized by Machina. Then, we show that a quantum-theoretic framework for decision-making under uncertainty recently elaborated by some of us allows faithful modeling of all data on the Ellsberg and Machina paradox situations. In the quantum-theoretic framework subjective probabilities are represented by quantum probabilities, while quantum state transformations enable representations of ambiguity aversion and subjective attitudes toward it.||||@arxiv||||2017/06/06||||Testing Ambiguity and Machina Preferences Within a...||||The Machina thought experiments pose to major non-expected utility models challenges that are similar to those posed by the Ellsberg thought experiments to subjective expected utility theory...||||https://arxiv.org/abs/1706.02168v1||||econ||||
631||||None||||Time-consistent decisions and rational expectation equilibrium existence in DSGE models||||arXiv.org||||2019/11/18||||Time-consistent decisions and rational expectation equilibrium existence in DSGE models||||Kim, Minseong||||https://arxiv.org/pdf/1909.10915||||1909.10915||||We demonstrate that if all agents in an economy make time-consistent decisions and policies, then there exists no rational expectation equilibrium in a dynamic stochastic general equilibrium (DSGE) model, unless under very restrictive and special circumstances. Some time-consistent interest rate rules, such as Taylor rule, worsen the equilibrium non-existence issue in general circumstances. Monetary policy needs to be lagged in order to avoid equilibrium non-existence due to agents making time-consistent decisions. We also show that due to the transversality condition issue, either fiscal-monetary coordination may need to be modeled, or it may be necessary to write a model such that bonds or money provides utility as medium of exchange or has liquidity roles.||||@arxiv||||2019/09/23||||Time-consistent decisions and rational expectation equilibrium...||||We demonstrate that if all agents in an economy make time-consistent decisions and policies, then there exists no rational expectation equilibrium in a dynamic stochastic general equilibrium...||||https://arxiv.org/abs/1909.10915v3||||econ||||
632||||None||||Evolutionary dynamics in heterogeneous populations: a general framework for an arbitrary type distribution||||arXiv.org||||2019/05/03||||Evolutionary dynamics in heterogeneous populations: a general framework for an arbitrary type distribution||||Zusai, Dai||||https://arxiv.org/pdf/1805.04897||||1805.04897||||A general framework of evolutionary dynamics under heterogeneous populations is presented. The framework allows continuously many types of heterogeneous agents, heterogeneity both in payoff functions and in revision protocols and the entire joint distribution of strategies and types to influence the payoffs of agents. We clarify regularity conditions for the unique existence of a solution trajectory and for the existence of equilibrium. We confirm that equilibrium stationarity in general and equilibrium stability in potential games are extended from the homogeneous setting to the heterogeneous setting. In particular, a wide class of admissible dynamics share the same set of locally stable equilibria in a potential game through local maximization of the potential.||||@arxiv||||2018/05/13||||Evolutionary dynamics in heterogeneous populations: a general...||||A general framework of evolutionary dynamics under heterogeneous populations is presented. The framework allows continuously many types of heterogeneous agents, heterogeneity both in payoff...||||https://arxiv.org/abs/1805.04897v2||||cs||||
633||||None||||Leave-out estimation of variance components||||arXiv.org||||2019/08/26||||Leave-out estimation of variance components||||Kline, Patrick || Saggio, Raffaele || Sølvsten, Mikkel||||https://arxiv.org/pdf/1806.01494||||1806.01494||||We propose leave-out estimators of quadratic forms designed for the study of linear models with unrestricted heteroscedasticity. Applications include analysis of variance and tests of linear restrictions in models with many regressors. An approximation algorithm is provided that enables accurate computation of the estimator in very large datasets. We study the large sample properties of our estimator allowing the number of regressors to grow in proportion to the number of observations. Consistency is established in a variety of settings where plug-in methods and estimators predicated on homoscedasticity exhibit first-order biases. For quadratic forms of increasing rank, the limiting distribution can be represented by a linear combination of normal and non-central $χ^2$ random variables, with normality ensuing under strong identification. Standard error estimators are proposed that enable tests of linear restrictions and the construction of uniformly valid confidence intervals for quadratic forms of interest. We find in Italian social security records that leave-out estimates of a variance decomposition in a two-way fixed effects model of wage determination yield substantially different conclusions regarding the relative contribution of workers, firms, and worker-firm sorting to wage inequality than conventional methods. Monte Carlo exercises corroborate the accuracy of our asymptotic approximations, with clear evidence of non-normality emerging when worker mobility between blocks of firms is limited.||||@arxiv||||2018/06/05||||Leave-out estimation of variance components||||We propose leave-out estimators of quadratic forms designed for the study of linear models with unrestricted heteroscedasticity. Applications include analysis of variance and tests of linear...||||https://arxiv.org/abs/1806.01494v2||||econ||||
634||||None||||Reversals of signal-posterior monotonicity imply a bias of screening||||arXiv.org||||2019/11/03||||Reversals of signal-posterior monotonicity imply a bias of screening||||Heinsalu, Sander||||https://arxiv.org/pdf/1910.03117||||1910.03117||||This note strengthens the main result of Lagziel and Lehrer (2019) (LL) "A bias in screening" using Chambers Healy (2011) (CH) "Reversals of signal-posterior monotonicity for any bounded prior". LL show that the conditional expectation of an unobserved variable of interest, given that a noisy signal of it exceeds a cutoff, may decrease in the cutoff. CH prove that the distribution of a variable conditional on a lower signal may first order stochastically dominate the distribution conditional on a higher signal.   The nonmonotonicity result is also extended to the empirically relevant exponential and Pareto distributions, and to a wide range of signals.||||@arxiv||||2019/10/07||||Reversals of signal-posterior monotonicity imply a bias of screening||||This note strengthens the main result of Lagziel and Lehrer (2019) (LL) "A bias in screening" using Chambers Healy (2011) (CH) "Reversals of signal-posterior monotonicity for any bounded prior"....||||https://arxiv.org/abs/1910.03117v6||||econ||||
635||||None||||Simple Inference on Functionals of Set-Identified Parameters Defined by Linear Moments||||arXiv.org||||2019/09/08||||Simple Inference on Functionals of Set-Identified Parameters Defined by Linear Moments||||Cho, JoonHwan || Russell, Thomas M.||||https://arxiv.org/pdf/1810.03180||||1810.03180||||This paper considers uniformly valid (over a class of data generating processes) inference for linear functionals of partially identified parameters in cases where the identified set is defined by linear (in the parameter) moment inequalities. We propose a bootstrap procedure for constructing uniformly valid confidence sets for a linear functional of a partially identified parameter. The proposed method amounts to bootstrapping the value functions of a linear optimization problem, and subsumes subvector inference as a special case. In other words, this paper shows the conditions under which ``naively'' bootstrapping a linear program can be used to construct a confidence set with uniform correct coverage for a partially identified linear functional. Unlike other proposed subvector inference procedures, our procedure does not require the researcher to repeatedly invert a hypothesis test, and is extremely computationally efficient. In addition to the new procedure, the paper also discusses connections between the literature on optimization and the literature on subvector inference in partially identified models.||||@arxiv||||2018/10/07||||Simple Inference on Functionals of Set-Identified Parameters...||||This paper considers uniformly valid (over a class of data generating processes) inference for linear functionals of partially identified parameters in cases where the identified set is defined by...||||https://arxiv.org/abs/1810.03180v7||||econ||||
636||||None||||High-Dimensional Econometrics and Regularized GMM||||arXiv.org||||2018/06/10||||High-Dimensional Econometrics and Regularized GMM||||Belloni, Alexandre || Chernozhukov, Victor || Chetverikov, Denis || Hansen, Christian || Kato, Kengo||||https://arxiv.org/pdf/1806.01888||||1806.01888||||This chapter presents key concepts and theoretical results for analyzing estimation and inference in high-dimensional models. High-dimensional models are characterized by having a number of unknown parameters that is not vanishingly small relative to the sample size. We first present results in a framework where estimators of parameters of interest may be represented directly as approximate means. Within this context, we review fundamental results including high-dimensional central limit theorems, bootstrap approximation of high-dimensional limit distributions, and moderate deviation theory. We also review key concepts underlying inference when many parameters are of interest such as multiple testing with family-wise error rate or false discovery rate control. We then turn to a general high-dimensional minimum distance framework with a special focus on generalized method of moments problems where we present results for estimation and inference about model parameters. The presented results cover a wide array of econometric applications, and we discuss several leading special cases including high-dimensional linear regression and linear instrumental variables models to illustrate the general results.||||@arxiv||||2018/06/05||||High-Dimensional Econometrics and Regularized GMM||||This chapter presents key concepts and theoretical results for analyzing estimation and inference in high-dimensional models. High-dimensional models are characterized by having a number of...||||https://arxiv.org/abs/1806.01888v2||||econ||||
637||||None||||From Local to Global: External Validity in a Fertility Natural Experiment||||arXiv.org||||2019/06/19||||From Local to Global: External Validity in a Fertility Natural Experiment||||Dehejia, Rajeev || Pop-Eleches, Cristian || Samii, Cyrus||||https://arxiv.org/pdf/1906.08096||||1906.08096||||We study issues related to external validity for treatment effects using over 100 replications of the Angrist and Evans (1998) natural experiment on the effects of sibling sex composition on fertility and labor supply. The replications are based on census data from around the world going back to 1960. We decompose sources of error in predicting treatment effects in external contexts in terms of macro and micro sources of variation. In our empirical setting, we find that macro covariates dominate over micro covariates for reducing errors in predicting treatments, an issue that past studies of external validity have been unable to evaluate. We develop methods for two applications to evidence-based decision-making, including determining where to locate an experiment and whether policy-makers should commission new experiments or rely on an existing evidence base for making a policy decision.||||@arxiv||||2019/06/19||||From Local to Global: External Validity in a Fertility Natural Experiment||||We study issues related to external validity for treatment effects using over 100 replications of the Angrist and Evans (1998) natural experiment on the effects of sibling sex composition on...||||https://arxiv.org/abs/1906.08096v1||||econ||||
638||||None||||The converse envelope theorem||||arXiv.org||||2019/09/24||||The converse envelope theorem||||Sinander, Ludvig||||https://arxiv.org/pdf/1909.11219||||1909.11219||||I prove an envelope theorem with a converse: the envelope formula is equivalent to a first-order condition. Like Milgrom and Segal's (2002) envelope theorem, my result requires no structure on the choice set. I use the converse envelope theorem to extend to abstract outcomes the canonical result in mechanism design that any increasing allocation is implementable, and apply this to selling information.||||@arxiv||||2019/09/24||||The converse envelope theorem||||I prove an envelope theorem with a converse: the envelope formula is equivalent to a first-order condition. Like Milgrom and Segal's (2002) envelope theorem, my result requires no structure on the...||||https://arxiv.org/abs/1909.11219v1||||econ||||
639||||None||||Model Selection in Time Series Analysis: Using Information Criteria as an Alternative to Hypothesis Testing||||arXiv.org||||2018/05/23||||Model Selection in Time Series Analysis: Using Information Criteria as an Alternative to Hypothesis Testing||||Hacker, R. Scott || Hatemi-J, Abdulnasser||||https://arxiv.org/pdf/1805.08991||||1805.08991||||The issue of model selection in applied research is of vital importance. Since the true model in such research is not known, which model should be used from among various potential ones is an empirical question. There might exist several competitive models. A typical approach to dealing with this is classic hypothesis testing using an arbitrarily chosen significance level based on the underlying assumption that a true null hypothesis exists. In this paper we investigate how successful this approach is in determining the correct model for different data generating processes using time series data. An alternative approach based on more formal model selection techniques using an information criterion or cross-validation is suggested and evaluated in the time series environment via Monte Carlo experiments. This paper also explores the effectiveness of deciding what type of general relation exists between two variables (e.g. relation in levels or relation in first differences) using various strategies based on hypothesis testing and on information criteria with the presence or absence of unit roots.||||@arxiv||||2018/05/23||||Model Selection in Time Series Analysis: Using Information...||||The issue of model selection in applied research is of vital importance. Since the true model in such research is not known, which model should be used from among various potential ones is an...||||https://arxiv.org/abs/1805.08991v1||||econ||||
640||||None||||Semiparametric Quantile Models for Ascending Auctions with Asymmetric Bidders||||arXiv.org||||2019/11/29||||Semiparametric Quantile Models for Ascending Auctions with Asymmetric Bidders||||Bhattacharya, Jayeeta || Gimenes, Nathalie || Guerre, Emmanuel||||https://arxiv.org/pdf/1911.13063||||1911.13063||||The paper proposes a parsimonious and flexible semiparametric quantile regression specification for asymmetric bidders within the independent private value framework. Asymmetry is parameterized using powers of a parent private value distribution, which is generated by a quantile regression specification. As noted in Cantillon (2008), this covers and extends models used for efficient collusion, joint bidding and mergers among homogeneous bidders. The specification can be estimated for ascending auctions using the winning bids and the winner's identity. The estimation is two stage. The asymmetry parameters are estimated from the winner's identity using a simple maximum likelihood procedure. The parent quantile regression specification can be estimated using simple modifications of Gimenes (2017). A timber application reveals that weaker bidders have 30\% less chances to win the auction than stronger ones. It is also found that increasing participation in an asymmetric ascending auction may not be as beneficial as using an optimal reserve price as would have been expected from a result of Bulow and Klemperer (1996) valid under symmetry.||||@arxiv||||2019/11/29||||Semiparametric Quantile Models for Ascending Auctions with...||||The paper proposes a parsimonious and flexible semiparametric quantile regression specification for asymmetric bidders within the independent private value framework. Asymmetry is parameterized...||||https://arxiv.org/abs/1911.13063v1||||econ||||
641||||None||||Bootstrap-Assisted Unit Root Testing With Piecewise Locally Stationary Errors||||arXiv.org||||2018/02/14||||Bootstrap-Assisted Unit Root Testing With Piecewise Locally Stationary Errors||||Rho, Yeonwoo || Shao, Xiaofeng||||https://arxiv.org/pdf/1802.05333||||1802.05333||||In unit root testing, a piecewise locally stationary process is adopted to accommodate nonstationary errors that can have both smooth and abrupt changes in second- or higher-order properties. Under this framework, the limiting null distributions of the conventional unit root test statistics are derived and shown to contain a number of unknown parameters. To circumvent the difficulty of direct consistent estimation, we propose to use the dependent wild bootstrap to approximate the non-pivotal limiting null distributions and provide a rigorous theoretical justification for bootstrap consistency. The proposed method is compared through finite sample simulations with the recolored wild bootstrap procedure, which was developed for errors that follow a heteroscedastic linear process. Further, a combination of autoregressive sieve recoloring with the dependent wild bootstrap is shown to perform well. The validity of the dependent wild bootstrap in a nonstationary setting is demonstrated for the first time, showing the possibility of extensions to other inference problems associated with locally stationary processes.||||@arxiv||||2018/02/14||||Bootstrap-Assisted Unit Root Testing With Piecewise Locally...||||In unit root testing, a piecewise locally stationary process is adopted to accommodate nonstationary errors that can have both smooth and abrupt changes in second- or higher-order properties....||||https://arxiv.org/abs/1802.05333v1||||econ||||
642||||None||||Limit Theorems for Network Dependent Random Variables||||arXiv.org||||2019/03/04||||Limit Theorems for Network Dependent Random Variables||||Kojevnikov, Denis || Marmer, Vadim || Song, Kyungchul||||https://arxiv.org/pdf/1903.01059||||1903.01059||||This paper considers a general form of network dependence where dependence between two sets of random variables becomes weaker as their distance in a network gets longer. We show that such network dependence cannot be embedded as a random field on a lattice in a Euclidean space with a fixed dimension when the maximum clique increases in size as the network grows. This paper applies Doukhan and Louhichi (1999)'s weak dependence notion to network dependence by measuring dependence strength by the covariance between nonlinearly transformed random variables. While this approach covers examples such as strong mixing random fields on a graph and conditional dependency graphs, it is most useful when dependence arises through a large functional-causal system of equations. The main results of our paper include the law of large numbers, and the central limit theorem. We also propose a heteroskedasticity-autocorrelation consistent variance estimator and prove its consistency under regularity conditions. The finite sample performance of this latter estimator is investigated through a Monte Carlo simulation study.||||@arxiv||||2019/03/04||||Limit Theorems for Network Dependent Random Variables||||This paper considers a general form of network dependence where dependence between two sets of random variables becomes weaker as their distance in a network gets longer. We show that such network...||||https://arxiv.org/abs/1903.01059v1||||econ||||
643||||None||||E-commerce in Hungary: A Market Analysis||||arXiv.org||||2018/12/30||||E-commerce in Hungary: A Market Analysis||||Nagy, Szabolcs||||https://arxiv.org/pdf/1812.11488||||1812.11488||||E-commerce is on the rise in Hungary, with significantly growing numbers of customers shopping online. This paper aims to identify the direct and indirect drivers of the double-digit growth rate, including the related macroeconomic indicators and the Digital Economy and Society Index (DESI). Moreover, this study provides a deep insight into industry trends and outlooks, including high industry concentration and top industrial players. It also draws the profile of the typical online shopper and the dominant characteristics of online purchases. Development of e-commerce is robust, but there is still plenty of potential for growth and progress in Hungary.||||@arxiv||||2018/12/30||||E-commerce in Hungary: A Market Analysis||||E-commerce is on the rise in Hungary, with significantly growing numbers of customers shopping online. This paper aims to identify the direct and indirect drivers of the double-digit growth rate,...||||https://arxiv.org/abs/1812.11488v1||||econ||||
644||||None||||Saddlepoint approximations for spatial panel data models||||arXiv.org||||2020/01/31||||Saddlepoint approximations for spatial panel data models||||Jiang, Chaonan || La Vecchia, Davide || Ronchetti, Elvezio || Scaillet, Olivier||||https://arxiv.org/pdf/2001.10377||||2001.10377||||We develop new higher-order asymptotic techniques for the Gaussian maximum likelihood estimator in a spatial panel data model, with fixed effects, time-varying covariates, and spatially correlated errors. Our saddlepoint density and tail area approximation feature relative error of order $O(m^{-1})$ for $m=n(T-1)$ with $n$ being the cross-sectional dimension and $T$ the time-series dimension. The main theoretical tool is the tilted-Edgeworth technique in a non-identically distributed setting. The density approximation is always non-negative, does not need resampling, and is accurate in the tails. We provide an algorithm and Monte Carlo experiments illustrating its good performance over first-order asymptotics and Edgeworth expansions, while preserving analytical tractability. An empirical application on the investment-saving relationship in OECD countries shows disagreement between testing results based on first-order asymptotics and saddlepoint techniques.||||@arxiv||||2020/01/22||||Saddlepoint approximations for spatial panel data models||||We develop new higher-order asymptotic techniques for the Gaussian maximum likelihood estimator in a spatial panel data model, with fixed effects, time-varying covariates, and spatially correlated...||||https://arxiv.org/abs/2001.10377v2||||econ||||
645||||None||||Multivariate Fractional Components Analysis||||arXiv.org||||2019/01/29||||Multivariate Fractional Components Analysis||||Hartl, Tobias || Weigand, Roland||||https://arxiv.org/pdf/1812.09149||||1812.09149||||We propose a setup for fractionally cointegrated time series which is formulated in terms of latent integrated and short-memory components. It accommodates nonstationary processes with different fractional orders and cointegration of different strengths and is applicable in high-dimensional settings. In an application to realized covariance matrices, we find that orthogonal short- and long-memory components provide a reasonable fit and competitive out-of-sample performance compared to several competing methods.||||@arxiv||||2018/12/21||||Multivariate Fractional Components Analysis||||We propose a setup for fractionally cointegrated time series which is formulated in terms of latent integrated and short-memory components. It accommodates nonstationary processes with different...||||https://arxiv.org/abs/1812.09149v2||||econ||||
646||||None||||A Peek into the Unobservable: Hidden States and Bayesian Inference for the Bitcoin and Ether Price Series||||arXiv.org||||2019/09/24||||A Peek into the Unobservable: Hidden States and Bayesian Inference for the Bitcoin and Ether Price Series||||Koki, Constandina || Leonardos, Stefanos || Piliouras, Georgios||||https://arxiv.org/pdf/1909.10957||||1909.10957||||Conventional financial models fail to explain the economic and monetary properties of cryptocurrencies due to the latter's dual nature: their usage as financial assets on the one side and their tight connection to the underlying blockchain structure on the other. In an effort to examine both components via a unified approach, we apply a recently developed Non-Homogeneous Hidden Markov (NHHM) model with an extended set of financial and blockchain specific covariates on the Bitcoin (BTC) and Ether (ETH) price data. Based on the observable series, the NHHM model offers a novel perspective on the underlying microstructure of the cryptocurrency market and provides insight on unobservable parameters such as the behavior of investors, traders and miners. The algorithm identifies two alternating periods (hidden states) of inherently different activity -- fundamental versus uninformed or noise traders -- in the Bitcoin ecosystem and unveils differences in both the short/long run dynamics and in the financial characteristics of the two states, such as significant explanatory variables, extreme events and varying series autocorrelation. In a somewhat unexpected result, the Bitcoin and Ether markets are found to be influenced by markedly distinct indicators despite their perceived correlation. The current approach backs earlier findings that cryptocurrencies are unlike any conventional financial asset and makes a first step towards understanding cryptocurrency markets via a more comprehensive lens.||||@arxiv||||2019/09/24||||A Peek into the Unobservable: Hidden States and Bayesian Inference...||||Conventional financial models fail to explain the economic and monetary properties of cryptocurrencies due to the latter's dual nature: their usage as financial assets on the one side and their...||||https://arxiv.org/abs/1909.10957v1||||econ||||
647||||None||||Averaging plus Learning in financial markets||||arXiv.org||||2019/06/04||||Averaging plus Learning in financial markets||||Popescu, Ionel || Vaidya, Tushar||||https://arxiv.org/pdf/1904.08131||||1904.08131||||This paper develops original models to study interacting agents in financial markets. The key feature of these models is how interactions are formulated and analysed. Agents learn from their observations and learning ability to interpret news or private information. Central limit theorems are developed but they arise rather unexpectedly. Under certain type of conditions governing the learning, agents beliefs converge in distribution that can be even fractal. The underlying randomness in the systems is not restricted to be of a certain class. Fresh insights are gained not only from developing new non-linear social learning models but also from using different techniques to study discrete time random linear dynamical systems.||||@arxiv||||2019/04/17||||Averaging plus Learning in financial markets||||This paper develops original models to study interacting agents in financial markets. The key feature of these models is how interactions are formulated and analysed. Agents learn from their...||||https://arxiv.org/abs/1904.08131v2||||econ||||
648||||None||||Statistical Inference on Partially Linear Panel Model under Unobserved Linearity||||arXiv.org||||2019/11/20||||Statistical Inference on Partially Linear Panel Model under Unobserved Linearity||||Liu, Ruiqi || Boukai, Ben || Shang, Zuofeng||||https://arxiv.org/pdf/1911.08830||||1911.08830||||A new statistical procedure, based on a modified spline basis, is proposed to identify the linear components in the panel data model with fixed effects. Under some mild assumptions, the proposed procedure is shown to consistently estimate the underlying regression function, correctly select the linear components, and effectively conduct the statistical inference. When compared to existing methods for detection of linearity in the panel model, our approach is demonstrated to be theoretically justified as well as practically convenient. We provide a computational algorithm that implements the proposed procedure along with a path-based solution method for linearity detection, which avoids the burden of selecting the tuning parameter for the penalty term. Monte Carlo simulations are conducted to examine the finite sample performance of our proposed procedure with detailed findings that confirm our theoretical results in the paper. Applications to Aggregate Production and Environmental Kuznets Curve data also illustrate the necessity for detecting linearity in the partially linear panel model.||||@arxiv||||2019/11/20||||Statistical Inference on Partially Linear Panel Model under...||||A new statistical procedure, based on a modified spline basis, is proposed to identify the linear components in the panel data model with fixed effects. Under some mild assumptions, the proposed...||||https://arxiv.org/abs/1911.08830v1||||econ||||
649||||None||||Spatial competition with unit-demand functions||||arXiv.org||||2020/01/30||||Spatial competition with unit-demand functions||||Fournier, Gaëtan || Van Der Straeten, Karine || Weibull, Jörgen||||https://arxiv.org/pdf/2001.11422||||2001.11422||||This paper studies a spatial competition game between two firms that sell a homogeneous good at some pre-determined fixed price. A population of consumers is spread out over the real line, and the two firms simultaneously choose location in this same space. When buying from one of the firms, consumers incur the fixed price plus some transportation costs, which are increasing with their distance to the firm. Under the assumption that each consumer is ready to buy one unit of the good whatever the locations of the firms, firms converge to the median location: there is "minimal differentiation". In this article, we relax this assumption and assume that there is an upper limit to the distance a consumer is ready to cover to buy the good. We show that the game always has at least one Nash equilibrium in pure strategy. Under this more general assumption, the "minimal differentiation principle" no longer holds in general. At equilibrium, firms choose "minimal", "intermediate" or "full" differentiation, depending on this critical distance a consumer is ready to cover and on the shape of the distribution of consumers' locations.||||@arxiv||||2020/01/30||||Spatial competition with unit-demand functions||||This paper studies a spatial competition game between two firms that sell a homogeneous good at some pre-determined fixed price. A population of consumers is spread out over the real line, and the...||||https://arxiv.org/abs/2001.11422v1||||econ||||
650||||None||||Inference under random limit bootstrap measures||||arXiv.org||||2019/12/05||||Inference under random limit bootstrap measures||||Cavaliere, Giuseppe || Georgiev, Iliyan||||https://arxiv.org/pdf/1911.12779||||1911.12779||||Asymptotic bootstrap validity is usually understood as consistency of the distribution of a bootstrap statistic, conditional on the data, for the unconditional limit distribution of a statistic of interest. From this perspective, randomness of the limit bootstrap measure is regarded as a failure of the bootstrap. We show that such limiting randomness does not necessarily invalidate bootstrap inference if validity is understood as control over the frequency of correct inferences in large samples. We first establish sufficient conditions for asymptotic bootstrap validity in cases where the unconditional limit distribution of a statistic can be obtained by averaging a (random) limiting bootstrap distribution. Further, we provide results ensuring the asymptotic validity of the bootstrap as a tool for conditional inference, the leading case being that where a bootstrap distribution estimates consistently a conditional (and thus, random) limit distribution of a statistic. We apply our framework to several inference problems in econometrics, including linear models with possibly non-stationary regressors, functional CUSUM statistics, conditional Kolmogorov-Smirnov specification tests, the `parameter on the boundary' problem and tests for constancy of parameters in dynamic econometric models.||||@arxiv||||2019/11/28||||Inference under random limit bootstrap measures||||Asymptotic bootstrap validity is usually understood as consistency of the distribution of a bootstrap statistic, conditional on the data, for the unconditional limit distribution of a statistic of...||||https://arxiv.org/abs/1911.12779v2||||econ||||
651||||None||||Statistical and Economic Evaluation of Time Series Models for Forecasting Arrivals at Call Centers||||arXiv.org||||2018/04/23||||Statistical and Economic Evaluation of Time Series Models for Forecasting Arrivals at Call Centers||||Bastianin, Andrea || Galeotti, Marzio || Manera, Matteo||||https://arxiv.org/pdf/1804.08315||||1804.08315||||Call centers' managers are interested in obtaining accurate point and distributional forecasts of call arrivals in order to achieve an optimal balance between service quality and operating costs. We present a strategy for selecting forecast models of call arrivals which is based on three pillars: (i) flexibility of the loss function; (ii) statistical evaluation of forecast accuracy; (iii) economic evaluation of forecast performance using money metrics. We implement fourteen time series models and seven forecast combination schemes on three series of daily call arrivals. Although we focus mainly on point forecasts, we also analyze density forecast evaluation. We show that second moments modeling is important both for point and density forecasting and that the simple Seasonal Random Walk model is always outperformed by more general specifications. Our results suggest that call center managers should invest in the use of forecast models which describe both first and second moments of call arrivals.||||@arxiv||||2018/04/23||||Statistical and Economic Evaluation of Time Series Models for...||||Call centers' managers are interested in obtaining accurate point and distributional forecasts of call arrivals in order to achieve an optimal balance between service quality and operating costs....||||https://arxiv.org/abs/1804.08315v1||||econ||||
652||||None||||Counterfactual Inference for Consumer Choice Across Many Product Categories||||arXiv.org||||2019/06/06||||Counterfactual Inference for Consumer Choice Across Many Product Categories||||Donnelly, Rob || Ruiz, Francisco R. || Blei, David || Athey, Susan||||https://arxiv.org/pdf/1906.02635||||1906.02635||||This paper proposes a method for estimating consumer preferences among discrete choices, where the consumer chooses at most one product in a category, but selects from multiple categories in parallel. The consumer's utility is additive in the different categories. Her preferences about product attributes as well as her price sensitivity vary across products and are in general correlated across products. We build on techniques from the machine learning literature on probabilistic models of matrix factorization, extending the methods to account for time-varying product attributes and products going out of stock. We evaluate the performance of the model using held-out data from weeks with price changes or out of stock products. We show that our model improves over traditional modeling approaches that consider each category in isolation. One source of the improvement is the ability of the model to accurately estimate heterogeneity in preferences (by pooling information across categories); another source of improvement is its ability to estimate the preferences of consumers who have rarely or never made a purchase in a given category in the training data. Using held-out data, we show that our model can accurately distinguish which consumers are most price sensitive to a given product. We consider counterfactuals such as personally targeted price discounts, showing that using a richer model such as the one we propose substantially increases the benefits of personalization in discounts.||||@arxiv||||2019/06/06||||Counterfactual Inference for Consumer Choice Across Many Product Categories||||This paper proposes a method for estimating consumer preferences among discrete choices, where the consumer chooses at most one product in a category, but selects from multiple categories in...||||https://arxiv.org/abs/1906.02635v1||||cs||||
653||||None||||Stochastic Dominance Under Independent Noise||||arXiv.org||||2019/05/20||||Stochastic Dominance Under Independent Noise||||Pomatto, Luciano || Strack, Philipp || Tamuz, Omer||||https://arxiv.org/pdf/1807.06927||||1807.06927||||Stochastic dominance is a crucial tool for the analysis of choice under risk. It is typically analyzed as a property of two gambles that are taken in isolation. We study how additional independent sources of risk (e.g. uninsurable labor risk, house price risk, etc.) can affect the ordering of gambles. We show that, perhaps surprisingly, background risk can be strong enough to render lotteries that are ranked by their expectation ranked in terms of first-order stochastic dominance. We extend our results to second order stochastic dominance, and show how they lead to a novel, and elementary, axiomatization of mean-variance preferences.||||@arxiv||||2018/07/18||||Stochastic Dominance Under Independent Noise||||Stochastic dominance is a crucial tool for the analysis of choice under risk. It is typically analyzed as a property of two gambles that are taken in isolation. We study how additional independent...||||https://arxiv.org/abs/1807.06927v5||||econ||||
654||||None||||Local Polynomial Estimation of Time-Varying Parameters in Nonlinear Models||||arXiv.org||||2019/04/10||||Local Polynomial Estimation of Time-Varying Parameters in Nonlinear Models||||Kristensen, Dennis || Lee, Young Jun||||https://arxiv.org/pdf/1904.05209||||1904.05209||||We develop a novel asymptotic theory for local polynomial (quasi-) maximum-likelihood estimators of time-varying parameters in a broad class of nonlinear time series models. Under weak regularity conditions, we show the proposed estimators are consistent and follow normal distributions in large samples. Our conditions impose weaker smoothness and moment conditions on the data-generating process and its likelihood compared to existing theories. Furthermore, the bias terms of the estimators take a simpler form. We demonstrate the usefulness of our general results by applying our theory to local (quasi-)maximum-likelihood estimators of a time-varying VAR's, ARCH and GARCH, and Poisson autogressions. For the first three models, we are able to substantially weaken the conditions found in the existing literature. For the Poisson autogression, existing theories cannot be be applied while our novel approach allows us to analyze it.||||@arxiv||||2019/04/10||||Local Polynomial Estimation of Time-Varying Parameters in Nonlinear Models||||We develop a novel asymptotic theory for local polynomial (quasi-) maximum-likelihood estimators of time-varying parameters in a broad class of nonlinear time series models. Under weak regularity...||||https://arxiv.org/abs/1904.05209v1||||econ||||
655||||None||||Ancillarity-Sufficiency Interweaving Strategy (ASIS) for Boosting MCMC Estimation of Stochastic Volatility Models||||arXiv.org||||2017/06/16||||Ancillarity-Sufficiency Interweaving Strategy (ASIS) for Boosting MCMC Estimation of Stochastic Volatility Models||||Kastner, Gregor || Frühwirth-Schnatter, Sylvia||||https://arxiv.org/pdf/1706.05280||||1706.05280||||Bayesian inference for stochastic volatility models using MCMC methods highly depends on actual parameter values in terms of sampling efficiency. While draws from the posterior utilizing the standard centered parameterization break down when the volatility of volatility parameter in the latent state equation is small, non-centered versions of the model show deficiencies for highly persistent latent variable series. The novel approach of ancillarity-sufficiency interweaving has recently been shown to aid in overcoming these issues for a broad class of multilevel models. In this paper, we demonstrate how such an interweaving strategy can be applied to stochastic volatility models in order to greatly improve sampling efficiency for all parameters and throughout the entire parameter range. Moreover, this method of "combining best of different worlds" allows for inference for parameter constellations that have previously been infeasible to estimate without the need to select a particular parameterization beforehand.||||@arxiv||||2017/06/16||||Ancillarity-Sufficiency Interweaving Strategy (ASIS) for Boosting...||||Bayesian inference for stochastic volatility models using MCMC methods highly depends on actual parameter values in terms of sampling efficiency. While draws from the posterior utilizing the...||||https://arxiv.org/abs/1706.05280v1||||econ||||
656||||None||||Edgeworth trading on networks||||arXiv.org||||2019/01/08||||Edgeworth trading on networks||||Cassese, Daniele || Pin, Paolo||||https://arxiv.org/pdf/1803.08836||||1803.08836||||We define a class of pure exchange Edgeworth trading processes that under minimal assumptions converge to a stable set in the space of allocations, and characterise the Pareto set of these processes. Choosing a specific process belonging to this class, that we define fair trading, we analyse the trade dynamics between agents located on a weighted network. We determine the conditions under which there always exists a one-to-one map between the set of networks and the set of stable equilibria. This result is used to understand what is the effect of the network topology on the trade dynamics and on the final allocation. We find that the positions in the network affect the distribution of the utility gains, given the initial allocations||||@arxiv||||2018/03/23||||Edgeworth trading on networks||||We define a class of pure exchange Edgeworth trading processes that under minimal assumptions converge to a stable set in the space of allocations, and characterise the Pareto set of these...||||https://arxiv.org/abs/1803.08836v2||||cs||||
657||||None||||$k$th price auctions and Catalan numbers||||arXiv.org||||2018/08/17||||$k$th price auctions and Catalan numbers||||Nawar, Abdel-Hameed || Sen, Debapriya||||https://arxiv.org/pdf/1808.05996||||1808.05996||||This paper establishes an interesting link between $k$th price auctions and Catalan numbers by showing that for distributions that have linear density, the bid function at any symmetric, increasing equilibrium of a $k$th price auction with $k\geq 3$ can be represented as a finite series of $k-2$ terms whose $\ell$th term involves the $\ell$th Catalan number. Using an integral representation of Catalan numbers, together with some classical combinatorial identities, we derive the closed form of the unique symmetric, increasing equilibrium of a $k$th price auction for a non-uniform distribution.||||@arxiv||||2018/08/17||||$k$th price auctions and Catalan numbers||||This paper establishes an interesting link between $k$th price auctions and Catalan numbers by showing that for distributions that have linear density, the bid function at any symmetric,...||||https://arxiv.org/abs/1808.05996v1||||econ||||
658||||None||||Measuring the Time-Varying Market Efficiency in the Prewar Japanese Stock Market||||arXiv.org||||2019/11/11||||Measuring the Time-Varying Market Efficiency in the Prewar Japanese Stock Market||||Noda, Akihiko||||https://arxiv.org/pdf/1911.04059||||1911.04059||||This study explores the time-varying structure of market efficiency on the prewar Japanese stock market based on Lo's (2004) adaptive market hypothesis (AMH). In particular, we measure the time-varying degree of market efficiency using new datasets of the stock price index estimated by Hirayama (2017a,b, 2018, 2019a). The empirical results show that (1) the degree of market efficiency in the prewar Japanese stock market varied with time and that its variation corresponded with major historical events, (2) Lo's (2004) AMH is supported in the prewar Japanese stock market, (3) the difference in market efficiency between the old/new TSE shares and the EQPI depends on the manner in which the price index is constructed, and (4) the price control policy beginning in the early 1930s suppressed price volatility and improved market efficiency.||||@arxiv||||2019/11/11||||Measuring the Time-Varying Market Efficiency in the Prewar...||||This study explores the time-varying structure of market efficiency on the prewar Japanese stock market based on Lo's (2004) adaptive market hypothesis (AMH). In particular, we measure the...||||https://arxiv.org/abs/1911.04059v1||||econ||||
659||||None||||Principal Components and Regularized Estimation of Factor Models||||arXiv.org||||2017/11/13||||Principal Components and Regularized Estimation of Factor Models||||Bai, Jushan || Ng, Serena||||https://arxiv.org/pdf/1708.08137||||1708.08137||||It is known that the common factors in a large panel of data can be consistently estimated by the method of principal components, and principal components can be constructed by iterative least squares regressions. Replacing least squares with ridge regressions turns out to have the effect of shrinking the singular values of the common component and possibly reducing its rank. The method is used in the machine learning literature to recover low-rank matrices. We study the procedure from the perspective of estimating a minimum-rank approximate factor model. We show that the constrained factor estimates are biased but can be more efficient in terms of mean-squared errors. Rank consideration suggests a data-dependent penalty for selecting the number of factors. The new criterion is more conservative in cases when the nominal number of factors is inflated by the presence of weak factors or large measurement noise. The framework is extended to incorporate a priori linear constraints on the loadings. We provide asymptotic results that can be used to test economic hypotheses.||||@arxiv||||2017/08/27||||Principal Components and Regularized Estimation of Factor Models||||It is known that the common factors in a large panel of data can be consistently estimated by the method of principal components, and principal components can be constructed by iterative least...||||https://arxiv.org/abs/1708.08137v2||||econ||||
660||||None||||Decision Making with Machine Learning and ROC Curves||||arXiv.org||||2019/05/05||||Decision Making with Machine Learning and ROC Curves||||Feng, Kai || Hong, Han || Tang, Ke || Wang, Jingyuan||||https://arxiv.org/pdf/1905.02810||||1905.02810||||The Receiver Operating Characteristic (ROC) curve is a representation of the statistical information discovered in binary classification problems and is a key concept in machine learning and data science. This paper studies the statistical properties of ROC curves and its implication on model selection. We analyze the implications of different models of incentive heterogeneity and information asymmetry on the relation between human decisions and the ROC curves. Our theoretical discussion is illustrated in the context of a large data set of pregnancy outcomes and doctor diagnosis from the Pre-Pregnancy Checkups of reproductive age couples in Henan Province provided by the Chinese Ministry of Health.||||@arxiv||||2019/05/05||||Decision Making with Machine Learning and ROC Curves||||The Receiver Operating Characteristic (ROC) curve is a representation of the statistical information discovered in binary classification problems and is a key concept in machine learning and data...||||https://arxiv.org/abs/1905.02810v1||||cs||||
661||||None||||On Long Memory Origins and Forecast Horizons||||arXiv.org||||2017/12/21||||On Long Memory Origins and Forecast Horizons||||Vera-Valdés, J. Eduardo||||https://arxiv.org/pdf/1712.08057||||1712.08057||||Most long memory forecasting studies assume that the memory is generated by the fractional difference operator. We argue that the most cited theoretical arguments for the presence of long memory do not imply the fractional difference operator, and assess the performance of the autoregressive fractionally integrated moving average $(ARFIMA)$ model when forecasting series with long memory generated by nonfractional processes. We find that high-order autoregressive $(AR)$ models produce similar or superior forecast performance than $ARFIMA$ models at short horizons. Nonetheless, as the forecast horizon increases, the $ARFIMA$ models tend to dominate in forecast performance. Hence, $ARFIMA$ models are well suited for forecasts of long memory processes regardless of the long memory generating mechanism, particularly for medium and long forecast horizons. Additionally, we analyse the forecasting performance of the heterogeneous autoregressive ($HAR$) model which imposes restrictions on high-order $AR$ models. We find that the structure imposed by the $HAR$ model produces better long horizon forecasts than $AR$ models of the same order, at the price of inferior short horizon forecasts in some cases. Our results have implications for, among others, Climate Econometrics and Financial Econometrics models dealing with long memory series at different forecast horizons. We show in an example that while a short memory autoregressive moving average $(ARMA)$ model gives the best performance when forecasting the Realized Variance of the S\&P 500 up to a month ahead, the $ARFIMA$ model gives the best performance for longer forecast horizons.||||@arxiv||||2017/12/21||||On Long Memory Origins and Forecast Horizons||||Most long memory forecasting studies assume that the memory is generated by the fractional difference operator. We argue that the most cited theoretical arguments for the presence of long memory...||||https://arxiv.org/abs/1712.08057v1||||econ||||
662||||None||||Regional economic convergence and spatial quantile regression||||arXiv.org||||2019/06/11||||Regional economic convergence and spatial quantile regression||||Cartone, Alfredo || Hewings, Geoffrey JD || Postiglione, Paolo||||https://arxiv.org/pdf/1906.04613||||1906.04613||||The presence of \b{eta}-convergence in European regions is an important issue to be analyzed. In this paper, we adopt a quantile regression approach in analyzing economic convergence. While previous work has performed quantile regression at the national level, we focus on 187 European NUTS2 regions for the period 1981-2009 and use spatial quantile regression to account for spatial dependence.||||@arxiv||||2019/06/11||||Regional economic convergence and spatial quantile regression||||The presence of \b{eta}-convergence in European regions is an important issue to be analyzed. In this paper, we adopt a quantile regression approach in analyzing economic convergence. While...||||https://arxiv.org/abs/1906.04613v1||||econ||||
663||||None||||Practical volume computation of structured convex bodies, and an application to modeling portfolio dependencies and financial crises||||arXiv.org||||2018/03/15||||Practical volume computation of structured convex bodies, and an application to modeling portfolio dependencies and financial crises||||Cales, Ludovic || Chalkis, Apostolos || Emiris, Ioannis Z. || Fisikopoulos, Vissarion||||https://arxiv.org/pdf/1803.05861||||1803.05861||||We examine volume computation of general-dimensional polytopes and more general convex bodies, defined as the intersection of a simplex by a family of parallel hyperplanes, and another family of parallel hyperplanes or a family of concentric ellipsoids. Such convex bodies appear in modeling and predicting financial crises. The impact of crises on the economy (labor, income, etc.) makes its detection of prime interest. Certain features of dependencies in the markets clearly identify times of turmoil. We describe the relationship between asset characteristics by means of a copula; each characteristic is either a linear or quadratic form of the portfolio components, hence the copula can be constructed by computing volumes of convex bodies. We design and implement practical algorithms in the exact and approximate setting, we experimentally juxtapose them and study the tradeoff of exactness and accuracy for speed. We analyze the following methods in order of increasing generality: rejection sampling relying on uniformly sampling the simplex, which is the fastest approach, but inaccurate for small volumes; exact formulae based on the computation of integrals of probability distribution functions; an optimized Lawrence sign decomposition method, since the polytopes at hand are shown to be simple; Markov chain Monte Carlo algorithms using random walks based on the hit-and-run paradigm generalized to nonlinear convex bodies and relying on new methods for computing a ball enclosed; the latter is experimentally extended to non-convex bodies with very encouraging results. Our C++ software, based on CGAL and Eigen and available on github, is shown to be very effective in up to 100 dimensions. Our results offer novel, effective means of computing portfolio dependencies and an indicator of financial crises, which is shown to correctly identify past crises.||||@arxiv||||2018/03/15||||Practical volume computation of structured convex bodies, and an...||||We examine volume computation of general-dimensional polytopes and more general convex bodies, defined as the intersection of a simplex by a family of parallel hyperplanes, and another family of...||||https://arxiv.org/abs/1803.05861v1||||cs||||
664||||None||||Identification of Noncausal Models by Quantile Autoregressions||||arXiv.org||||2019/04/11||||Identification of Noncausal Models by Quantile Autoregressions||||Hecq, Alain || Sun, Li||||https://arxiv.org/pdf/1904.05952||||1904.05952||||We propose a model selection criterion to detect purely causal from purely noncausal models in the framework of quantile autoregressions (QAR). We also present asymptotics for the i.i.d. case with regularly varying distributed innovations in QAR. This new modelling perspective is appealing for investigating the presence of bubbles in economic and financial time series, and is an alternative to approximate maximum likelihood methods. We illustrate our analysis using hyperinflation episodes in Latin American countries.||||@arxiv||||2019/04/11||||Identification of Noncausal Models by Quantile Autoregressions||||We propose a model selection criterion to detect purely causal from purely noncausal models in the framework of quantile autoregressions (QAR). We also present asymptotics for the i.i.d. case with...||||https://arxiv.org/abs/1904.05952v1||||econ||||
665||||None||||Optimal mitigation with endogenous learning and a cumulative constraint: with application to negative emissions of carbon dioxide||||arXiv.org||||2018/08/27||||Optimal mitigation with endogenous learning and a cumulative constraint: with application to negative emissions of carbon dioxide||||Seshadri, Ashwin K||||https://arxiv.org/pdf/1808.08717||||1808.08717||||Large-scale extraction of carbon dioxide (CO2) from Earth's atmosphere ("negative emissions") is important for stringent climate change mitigation scenarios, and we examine optimal (i.e. least-cost) pathways of negative emissions in the presence of learning by doing ("endogenous learning"). Optimal pathways solve a variational problem involving minimization of discounted costs subject to a constraint on total negative emissions across time. A minimum pathway exists if the marginal cost curve of negative emissions is increasing with annual rate of emissions reduction. In the absence of endogenous learning, the optimal pathway has annual negative emissions increasing with time: with more rapid increase in emissions rate occurring in case of large discount rate and slower increase of the cost curve. Endogenous learning can have contrary effects depending on how it is included in models. This paper identifies a basic distinction, between additive and multiplicative effects on marginal costs of endogenous learning, which governs its qualitative effects in such models. If endogenous learning is best modeled as a negative addition to the cost function, shifting the cost curve downward, the optimal pathway has higher emissions rate early on when compared to the no-learning case, however with emissions increasing with time. In contrast if endogenous learning is a multiplicative effect, scaling down marginal cost uniformly, then benefits of learning are slowly manifest as marginal cost rises and the optimal pathway begins at lower emissions rates that increase more rapidly as compared to if endogenous learning were absent.||||@arxiv||||2018/08/27||||Optimal mitigation with endogenous learning and a cumulative...||||Large-scale extraction of carbon dioxide (CO2) from Earth's atmosphere ("negative emissions") is important for stringent climate change mitigation scenarios, and we examine optimal (i.e....||||https://arxiv.org/abs/1808.08717v1||||econ||||
666||||None||||Subvector Inference in Partially Identified Models with Many Moment Inequalities||||arXiv.org||||2018/06/29||||Subvector Inference in Partially Identified Models with Many Moment Inequalities||||Belloni, Alexandre || Bugni, Federico || Chernozhukov, Victor||||https://arxiv.org/pdf/1806.11466||||1806.11466||||This paper considers inference for a function of a parameter vector in a partially identified model with many moment inequalities. This framework allows the number of moment conditions to grow with the sample size, possibly at exponential rates. Our main motivating application is subvector inference, i.e., inference on a single component of the partially identified parameter vector associated with a treatment effect or a policy variable of interest.   Our inference method compares a MinMax test statistic (minimum over parameters satisfying $H_0$ and maximum over moment inequalities) against critical values that are based on bootstrap approximations or analytical bounds. We show that this method controls asymptotic size uniformly over a large class of data generating processes despite the partially identified many moment inequality setting. The finite sample analysis allows us to obtain explicit rates of convergence on the size control. Our results are based on combining non-asymptotic approximations and new high-dimensional central limit theorems for the MinMax of the components of random matrices. Unlike the previous literature on functional inference in partially identified models, our results do not rely on weak convergence results based on Donsker's class assumptions and, in fact, our test statistic may not even converge in distribution. Our bootstrap approximation requires the choice of a tuning parameter sequence that can avoid the excessive concentration of our test statistic. To this end, we propose an asymptotically valid data-driven method to select this tuning parameter sequence. This method generalizes the selection of tuning parameter sequences to problems outside the Donsker's class assumptions and may also be of independent interest. Our procedures based on self-normalized moderate deviation bounds are relatively more conservative but easier to implement.||||@arxiv||||2018/06/29||||Subvector Inference in Partially Identified Models with Many...||||This paper considers inference for a function of a parameter vector in a partially identified model with many moment inequalities. This framework allows the number of moment conditions to grow...||||https://arxiv.org/abs/1806.11466v1||||econ||||
667||||None||||A comment on 'Testing Goodwin: growth cycles in ten OECD countries'||||arXiv.org||||2018/03/05||||A comment on 'Testing Goodwin: growth cycles in ten OECD countries'||||Grasselli, Matheus R. || Maheshwari, Aditya||||https://arxiv.org/pdf/1803.01527||||1803.01527||||We revisit the results of Harvie (2000) and show how correcting for a reporting mistake in some of the estimated parameter values leads to significantly different conclusions, including realistic parameter values for the Philips curve and estimated equilibrium employment rates exhibiting on average one tenth of the relative error of those obtained in Harvie (2000).||||@arxiv||||2018/03/05||||A comment on 'Testing Goodwin: growth cycles in ten OECD countries'||||We revisit the results of Harvie (2000) and show how correcting for a reporting mistake in some of the estimated parameter values leads to significantly different conclusions, including realistic...||||https://arxiv.org/abs/1803.01527v1||||econ||||
668||||None||||On monitoring development indicators using high resolution satellite images||||arXiv.org||||2018/06/25||||On monitoring development indicators using high resolution satellite images||||Suraj, Potnuru Kishen || Gupta, Ankesh || Sharma, Makkunda || Paul, Sourabh Bikas || Banerjee, Subhashis||||https://arxiv.org/pdf/1712.02282||||1712.02282||||We develop a machine learning based tool for accurate prediction of socio-economic indicators from daytime satellite imagery. The diverse set of indicators are often not intuitively related to observable features in satellite images, and are not even always well correlated with each other. Our predictive tool is more accurate than using night light as a proxy, and can be used to predict missing data, smooth out noise in surveys, monitor development progress of a region, and flag potential anomalies. Finally, we use predicted variables to do robustness analysis of a regression study of high rate of stunting in India.||||@arxiv||||2017/12/06||||On monitoring development indicators using high resolution satellite images||||We develop a machine learning based tool for accurate prediction of socio-economic indicators from daytime satellite imagery. The diverse set of indicators are often not intuitively related to...||||https://arxiv.org/abs/1712.02282v3||||cs||||
669||||None||||Building social networks under consent: A survey||||arXiv.org||||2019/10/25||||Building social networks under consent: A survey||||Gilles, Robert P.||||https://arxiv.org/pdf/1910.11693||||1910.11693||||This survey explores the literature on game-theoretic models of network formation under the hypothesis of mutual consent in link formation. The introduction of consent in link formation imposes a coordination problem in the network formation process. This survey explores the conclusions from this theory and the various methodologies to avoid the main pitfalls. The main insight originates from Myerson's work on mutual consent in link formation and his main conclusion that the empty network (the network without any links) always emerges as a strong Nash equilibrium in any game-theoretic model of network formation under mutual consent and positive link formation costs. Jackson and Wolinsky introduced a cooperative framework to avoid this main pitfall. They devised the notion of a pairwise stable network to arrive at equilibrium networks that are mainly non-trivial. Unfortunately, this notion of pairwise stability requires coordinated action by pairs of decision makers in link formation. I survey the possible solutions in a purely non-cooperative framework of network formation under mutual consent by exploring potential refinements of the standard Nash equilibrium concept to explain the emergence of non-trivial networks. This includes the notions of unilateral and monadic stability. The first one is founded on advanced rational reasoning of individuals about how others would respond to one's efforts to modify the network. The latter incorporates trusting, boundedly rational behaviour into the network formation process. The survey is concluded with an initial exploration of external correlation devices as an alternative framework to address mutual consent in network formation.||||@arxiv||||2019/10/25||||Building social networks under consent: A survey||||This survey explores the literature on game-theoretic models of network formation under the hypothesis of mutual consent in link formation. The introduction of consent in link formation imposes a...||||https://arxiv.org/abs/1910.11693v1||||cs||||
670||||None||||Shape Matters: Evidence from Machine Learning on Body Shape-Income Relationship||||arXiv.org||||2019/06/16||||Shape Matters: Evidence from Machine Learning on Body Shape-Income Relationship||||Song, Suyong || Baek, Stephen S.||||https://arxiv.org/pdf/1906.06747||||1906.06747||||We study the association between physical appearance and family income using a novel data which has 3-dimensional body scans to mitigate the issue of reporting errors and measurement errors observed in most previous studies. We apply machine learning to obtain intrinsic features consisting of human body and take into account a possible issue of endogenous body shapes. The estimation results show that there is a significant relationship between physical appearance and family income and the associations are different across the gender. This supports the hypothesis on the physical attractiveness premium and its heterogeneity across the gender.||||@arxiv||||2019/06/16||||Shape Matters: Evidence from Machine Learning on Body Shape-Income...||||We study the association between physical appearance and family income using a novel data which has 3-dimensional body scans to mitigate the issue of reporting errors and measurement errors...||||https://arxiv.org/abs/1906.06747v1||||econ||||
671||||None||||A new concept of technology with systemic-purposeful perpsective: theory, examples and empirical application||||arXiv.org||||2019/09/11||||A new concept of technology with systemic-purposeful perpsective: theory, examples and empirical application||||Coccia, Mario||||https://arxiv.org/pdf/1909.05689||||1909.05689||||Although definitions of technology exist to explain the patterns of technological innovations, there is no general definition that explain the role of technology for humans and other animal species in environment. The goal of this study is to suggest a new concept of technology with a systemic-purposeful perspective for technology analysis. Technology here is a complex system of artifact, made and_or used by living systems, that is composed of more than one entity or sub-system and a relationship that holds between each entity and at least one other entity in the system, selected considering practical, technical and_or economic characteristics to satisfy needs, achieve goals and_or solve problems of users for purposes of adaptation and_or survival in environment. Technology T changes current modes of cognition and action to enable makers and_or users to take advantage of important opportunities or to cope with consequential environmental threats. Technology, as a complex system, is formed by different elements given by incremental and radical innovations. Technological change generates the progress from a system T1 to T2, T3, etc. driven by changes of technological trajectories and technological paradigms. Several examples illustrate here these concepts and a simple model with a preliminary empirical analysis shows how to operationalize the suggested definition of technology. Overall, then, the role of adaptation (i.e. reproductive advantage) can be explained as a main driver of technology use for adopters to take advantage of important opportunities or to cope with environmental threats. This study begins the process of clarifying and generalizing, as far as possible, the concept of technology with a new perspective that it can lay a foundation for the development of more sophisticated concepts and theories to explain technological and economic change in environment.||||@arxiv||||2019/09/11||||A new concept of technology with systemic-purposeful perpsective:...||||Although definitions of technology exist to explain the patterns of technological innovations, there is no general definition that explain the role of technology for humans and other animal...||||https://arxiv.org/abs/1909.05689v1||||econ||||
672||||None||||Bilinear form test statistics for extremum estimation||||arXiv.org||||2019/12/03||||Bilinear form test statistics for extremum estimation||||Crudu, Federico || Osorio, Felipe||||https://arxiv.org/pdf/1912.01410||||1912.01410||||This paper develops a set of test statistics based on bilinear forms in the context of the extremum estimation framework with particular interest in nonlinear hypothesis. We show that the proposed statistic converges to a conventional chi-square limit. A Monte Carlo experiment suggests that the test statistic works well in finite samples.||||@arxiv||||2019/12/03||||Bilinear form test statistics for extremum estimation||||This paper develops a set of test statistics based on bilinear forms in the context of the extremum estimation framework with particular interest in nonlinear hypothesis. We show that the proposed...||||https://arxiv.org/abs/1912.01410v1||||econ||||
673||||None||||Statistical inference for autoregressive models under heteroscedasticity of unknown form||||arXiv.org||||2018/08/09||||Statistical inference for autoregressive models under heteroscedasticity of unknown form||||Zhu, Ke||||https://arxiv.org/pdf/1804.02348||||1804.02348||||This paper provides an entire inference procedure for the autoregressive model under (conditional) heteroscedasticity of unknown form with a finite variance. We first establish the asymptotic normality of the weighted least absolute deviations estimator (LADE) for the model. Second, we develop the random weighting (RW) method to estimate its asymptotic covariance matrix, leading to the implementation of the Wald test. Third, we construct a portmanteau test for model checking, and use the RW method to obtain its critical values. As a special weighted LADE, the feasible adaptive LADE (ALADE) is proposed and proved to have the same efficiency as its infeasible counterpart. The importance of our entire methodology based on the feasible ALADE is illustrated by simulation results and the real data analysis on three U.S. economic data sets.||||@arxiv||||2018/04/06||||Statistical inference for autoregressive models under...||||This paper provides an entire inference procedure for the autoregressive model under (conditional) heteroscedasticity of unknown form with a finite variance. We first establish the asymptotic...||||https://arxiv.org/abs/1804.02348v2||||econ||||
674||||None||||The Time Importance for Prospect Theory||||arXiv.org||||2019/08/05||||The Time Importance for Prospect Theory||||Nascimento, José Cláudio do||||https://arxiv.org/pdf/1908.01709||||1908.01709||||A theory usually comprises assumptions and deduced predictions from them. In this paper, empirical evidences corroborate with assumptions about time for a decision making facing known probabilities and outcomes.||||@arxiv||||2019/08/05||||The Time Importance for Prospect Theory||||A theory usually comprises assumptions and deduced predictions from them. In this paper, empirical evidences corroborate with assumptions about time for a decision making facing known...||||https://arxiv.org/abs/1908.01709v1||||econ||||
675||||None||||Identification and Estimation of Group-Level Partial Effects||||arXiv.org||||2018/11/01||||Identification and Estimation of Group-Level Partial Effects||||Nagasawa, Kenichi||||https://arxiv.org/pdf/1811.00667||||1811.00667||||This paper presents a new identification result for causal effects of group-level variables when agents select into groups. The model allows for group selection to be based on individual unobserved heterogeneity. This feature leads to correlation between group-level covariates and unobserved individual heterogeneity. Whereas many of the existing identification strategies rely on instrumental variables for group selection, I introduce alternative identifying conditions which involve individual-level covariates that "shift" the distribution of unobserved heterogeneity. I use these conditions to construct a valid control function. The key identifying requirements on the observable "shifter" variables are likely to hold in settings where a rich array of individual characteristics are observed. The identification strategy is constructive and leads to a semiparametric, regression-based estimator of group-level causal effects, which I show to be consistent and asymptotically normal. A simulation study indicates good finite-sample properties of this estimator. I use my results to re-analyze the effects of school/neighborhood characteristics on student outcomes, following the work of Altonji and Mansfield (2018).||||@arxiv||||2018/11/01||||Identification and Estimation of Group-Level Partial Effects||||This paper presents a new identification result for causal effects of group-level variables when agents select into groups. The model allows for group selection to be based on individual...||||https://arxiv.org/abs/1811.00667v1||||econ||||
676||||None||||Uncertainty in the Hot Hand Fallacy: Detecting Streaky Alternatives in Random Bernoulli Sequences||||arXiv.org||||2019/08/22||||Uncertainty in the Hot Hand Fallacy: Detecting Streaky Alternatives in Random Bernoulli Sequences||||Ritzwoller, David M. || Romano, Joseph P.||||https://arxiv.org/pdf/1908.01406||||1908.01406||||We study a class of tests of the randomness of Bernoulli sequences and their application to analyses of the human tendency to perceive streaks as overly representative of positive dependence-the hot hand fallacy. In particular, we study tests of randomness (i.e., that trials are i.i.d.) based on test statistics that compare the proportion of successes that directly follow k consecutive successes with either the overall proportion of successes or the proportion of successes that directly follow k consecutive failures. We derive the asymptotic distributions of these test statistics and their permutation distributions under randomness and under general models of streakiness, which allows us to evaluate their local asymptotic power. The results are applied to revisit tests of the hot hand fallacy implemented on data from a basketball shooting experiment, whose conclusions are disputed by Gilovich, Vallone, and Tversky (1985) and Miller and Sanjurjo (2018a). We establish that the tests are insufficiently powered to distinguish randomness from alternatives consistent with the variation in NBA shooting percentages. While multiple testing procedures reveal that one shooter can be inferred to exhibit shooting significantly inconsistent with randomness, we find that participants in a survey of basketball fans over-estimate an average player's streakiness, corroborating the empirical support for the hot hand fallacy.||||@arxiv||||2019/08/04||||Uncertainty in the Hot Hand Fallacy: Detecting Streaky...||||We study a class of tests of the randomness of Bernoulli sequences and their application to analyses of the human tendency to perceive streaks as overly representative of positive dependence-the...||||https://arxiv.org/abs/1908.01406v2||||econ||||
677||||None||||Competition of noise and collectivity in global cryptocurrency trading: route to a self-contained market||||arXiv.org||||2020/02/11||||Competition of noise and collectivity in global cryptocurrency trading: route to a self-contained market||||Drożdż, Stanisław || Minati, Ludovico || Oświęcimka, Paweł || Stanuszek, Marek || Wątorek, Marcin||||https://arxiv.org/pdf/1911.08944||||1911.08944||||Cross-correlations in fluctuations of the daily exchange rates within the basket of the 100 highest-capitalization cryptocurrencies over the period October 1, 2015, through March 31, 2019, are studied. The corresponding dynamics predominantly involve one leading eigenvalue of the correlation matrix, while the others largely coincide with those of Wishart random matrices. However, the magnitude of the principal eigenvalue, and thus the degree of collectivity, strongly depends on which cryptocurrency is used as a base. It is largest when the base is the most peripheral cryptocurrency; when more significant ones are taken into consideration, its magnitude systematically decreases, nevertheless preserving a sizable gap with respect to the random bulk, which in turn indicates that the organization of correlations becomes more heterogeneous. This finding provides a criterion for recognizing which currencies or cryptocurrencies play a dominant role in the global crypto-market. The present study shows that over the period under consideration, the Bitcoin (BTC) predominates, hallmarking exchange rate dynamics at least as influential as the US dollar. The BTC started dominating around the year 2017, while further cryptocurrencies, like the Ethereum (ETH) and even Ripple (XRP), assumed similar trends. At the same time, the USD, an original value determinant for the cryptocurrency market, became increasingly disconnected, its related characteristics eventually approaching those of a fictitious currency. These results are strong indicators of incipient independence of the global cryptocurrency market, delineating a self-contained trade resembling the Forex.||||@arxiv||||2019/11/20||||Competition of noise and collectivity in global cryptocurrency...||||Cross-correlations in fluctuations of the daily exchange rates within the basket of the 100 highest-capitalization cryptocurrencies over the period October 1, 2015, through March 31, 2019, are...||||https://arxiv.org/abs/1911.08944v2||||cs||||
678||||None||||Variational Bayesian Inference for Mixed Logit Models with Unobserved Inter- and Intra-Individual Heterogeneity||||arXiv.org||||2020/01/16||||Variational Bayesian Inference for Mixed Logit Models with Unobserved Inter- and Intra-Individual Heterogeneity||||Krueger, Rico || Bansal, Prateek || Bierlaire, Michel || Daziano, Ricardo A. || Rashidi, Taha H.||||https://arxiv.org/pdf/1905.00419||||1905.00419||||Variational Bayes (VB), a method originating from machine learning, enables fast and scalable estimation of complex probabilistic models. Thus far, applications of VB in discrete choice analysis have been limited to mixed logit models with unobserved inter-individual taste heterogeneity. However, such a model formulation may be too restrictive in panel data settings, since tastes may vary both between individuals as well as across choice tasks encountered by the same individual. In this paper, we derive a VB method for posterior inference in mixed logit models with unobserved inter- and intra-individual heterogeneity. In a simulation study, we benchmark the performance of the proposed VB method against maximum simulated likelihood (MSL) and Markov chain Monte Carlo (MCMC) methods in terms of parameter recovery, predictive accuracy and computational efficiency. The simulation study shows that VB can be a fast, scalable and accurate alternative to MSL and MCMC estimation, especially in applications in which fast predictions are paramount. VB is observed to be between 2.8 and 17.7 times faster than the two competing methods, while affording comparable or superior accuracy. Besides, the simulation study demonstrates that a parallelised implementation of the MSL estimator with analytical gradients is a viable alternative to MCMC in terms of both estimation accuracy and computational efficiency, as the MSL estimator is observed to be between 0.9 and 2.1 times faster than MCMC.||||@arxiv||||2019/05/01||||Variational Bayesian Inference for Mixed Logit Models with...||||Variational Bayes (VB), a method originating from machine learning, enables fast and scalable estimation of complex probabilistic models. Thus far, applications of VB in discrete choice analysis...||||https://arxiv.org/abs/1905.00419v3||||econ||||
679||||None||||Outgroup Homogeneity Bias Causes Ingroup Favoritism||||arXiv.org||||2019/08/22||||Outgroup Homogeneity Bias Causes Ingroup Favoritism||||Montrey, Marcel || Shultz, Thomas R.||||https://arxiv.org/pdf/1908.08203||||1908.08203||||Ingroup favoritism, the tendency to favor ingroup over outgroup, is often explained as a product of intergroup conflict, or correlations between group tags and behavior. Such accounts assume that group membership is meaningful, whereas human data show that ingroup favoritism occurs even when it confers no advantage and groups are transparently arbitrary. Another possibility is that ingroup favoritism arises due to perceptual biases like outgroup homogeneity, the tendency for humans to have greater difficulty distinguishing outgroup members than ingroup ones. We present a prisoner's dilemma model, where individuals use Bayesian inference to learn how likely others are to cooperate, and then act rationally to maximize expected utility. We show that, when such individuals exhibit outgroup homogeneity bias, ingroup favoritism between arbitrary groups arises through direct reciprocity. However, this outcome may be mitigated by: (1) raising the benefits of cooperation, (2) increasing population diversity, and (3) imposing a more restrictive social structure.||||@arxiv||||2019/08/22||||Outgroup Homogeneity Bias Causes Ingroup Favoritism||||Ingroup favoritism, the tendency to favor ingroup over outgroup, is often explained as a product of intergroup conflict, or correlations between group tags and behavior. Such accounts assume that...||||https://arxiv.org/abs/1908.08203v1||||econ||||
680||||None||||Optimal Climate Strategy with Mitigation, Carbon Removal, and Solar Geoengineering||||arXiv.org||||2019/03/05||||Optimal Climate Strategy with Mitigation, Carbon Removal, and Solar Geoengineering||||Belaia, Mariia||||https://arxiv.org/pdf/1903.02043||||1903.02043||||Until recently, analysis of optimal global climate policy has focused on mitigation. Exploration of policies to meet the 1.5°C target have brought carbon dioxide removal (CDR), a second instrument, into the climate policy mainstream. Far less agreement exists regarding the role of solar geoengineering (SG), a third instrument to limit global climate risk. Integrated assessment modelling (IAM) studies offer little guidance on trade-offs between these three instruments because they have dealt with CDR and SG in isolation. Here, I extend the Dynamic Integrated model of Climate and Economy (DICE) to include both CDR and SG to explore the temporal ordering of the three instruments. Contrary to implicit assumptions that SG would be employed only after mitigation and CDR are exhausted, I find that SG is introduced parallel to mitigation temporary reducing climate risks during the era of peak CO2 concentrations. CDR reduces concentrations after mitigation is exhausted, enabling SG phasing out.||||@arxiv||||2019/03/05||||Optimal Climate Strategy with Mitigation, Carbon Removal, and...||||Until recently, analysis of optimal global climate policy has focused on mitigation. Exploration of policies to meet the 1.5°C target have brought carbon dioxide removal (CDR), a second...||||https://arxiv.org/abs/1903.02043v1||||econ||||
681||||None||||Gains in evolutionary dynamics: A unifying and intuitive approach to linking static and dynamic stability||||arXiv.org||||2019/09/17||||Gains in evolutionary dynamics: A unifying and intuitive approach to linking static and dynamic stability||||Zusai, Dai||||https://arxiv.org/pdf/1805.04898||||1805.04898||||This paper presents a universal and economically intuitive approach to linking static and dynamic stability. In economics, static stability is traditionally defined by negative relationship between endogenous and exogenous variables in a model: for a population game, this is characterized by negative semidefiniteness of the Jacobian matrix of the payoff function. We consider economically reasonable dynamics, in which we can justify agents' choices of new strategies as optimal choices possibly by introducing additional costs and constraints. This class of dynamics covers major payoff-based (non-imitative) evolutionary dynamics. The key is expected net gains (payoff improvements) from strategy revisions after paying switching costs. Static stability implies that the aggregate net gain diminishes over time under economic reasonable dynamics and thus can be used as a Lyapunov function. While our analysis here is confined to myopic evolutionary dynamics in population games, our approach is applicable to more complex situations.||||@arxiv||||2018/05/13||||Gains in evolutionary dynamics: A unifying and intuitive approach...||||This paper presents a universal and economically intuitive approach to linking static and dynamic stability. In economics, static stability is traditionally defined by negative relationship...||||https://arxiv.org/abs/1805.04898v5||||cs||||
682||||None||||Boosting High Dimensional Predictive Regressions with Time Varying Parameters||||arXiv.org||||2019/10/07||||Boosting High Dimensional Predictive Regressions with Time Varying Parameters||||Yousuf, Kashif || Ng, Serena||||https://arxiv.org/pdf/1910.03109||||1910.03109||||High dimensional predictive regressions are useful in wide range of applications. However, the theory is mainly developed assuming that the model is stationary with time invariant parameters. This is at odds with the prevalent evidence for parameter instability in economic time series, but theories for parameter instability are mainly developed for models with a small number of covariates. In this paper, we present two $L_2$ boosting algorithms for estimating high dimensional models in which the coefficients are modeled as functions evolving smoothly over time and the predictors are locally stationary. The first method uses componentwise local constant estimators as base learner, while the second relies on componentwise local linear estimators. We establish consistency of both methods, and address the practical issues of choosing the bandwidth for the base learners and the number of boosting iterations. In an extensive application to macroeconomic forecasting with many potential predictors, we find that the benefits to modeling time variation are substantial and they increase with the forecast horizon. Furthermore, the timing of the benefits suggests that the Great Moderation is associated with substantial instability in the conditional mean of various economic series.||||@arxiv||||2019/10/07||||Boosting High Dimensional Predictive Regressions with Time Varying...||||High dimensional predictive regressions are useful in wide range of applications. However, the theory is mainly developed assuming that the model is stationary with time invariant parameters. This...||||https://arxiv.org/abs/1910.03109v1||||econ||||
683||||None||||The speed of sequential asymptotic learning||||arXiv.org||||2017/11/24||||The speed of sequential asymptotic learning||||Hann-Caruthers, Wade || Martynov, Vadim V. || Tamuz, Omer||||https://arxiv.org/pdf/1707.02689||||1707.02689||||In the classical herding literature, agents receive a private signal regarding a binary state of nature, and sequentially choose an action, after observing the actions of their predecessors. When the informativeness of private signals is unbounded, it is known that agents converge to the correct action and correct belief. We study how quickly convergence occurs, and show that it happens more slowly than it does when agents observe signals. However, we also show that the speed of learning from actions can be arbitrarily close to the speed of learning from signals. In particular, the expected time until the agents stop taking the wrong action can be either finite or infinite, depending on the private signal distribution. In the canonical case of Gaussian private signals we calculate the speed of convergence precisely, and show explicitly that, in this case, learning from actions is significantly slower than learning from signals.||||@arxiv||||2017/07/10||||The speed of sequential asymptotic learning||||In the classical herding literature, agents receive a private signal regarding a binary state of nature, and sequentially choose an action, after observing the actions of their predecessors. When...||||https://arxiv.org/abs/1707.02689v3||||econ||||
684||||None||||An Integrated Panel Data Approach to Modelling Economic Growth||||arXiv.org||||2019/03/19||||An Integrated Panel Data Approach to Modelling Economic Growth||||Feng, Guohua || Gao, Jiti || Peng, Bin||||https://arxiv.org/pdf/1903.07948||||1903.07948||||Empirical growth analysis has three major problems --- variable selection, parameter heterogeneity and cross-sectional dependence --- which are addressed independently from each other in most studies. The purpose of this study is to propose an integrated framework that extends the conventional linear growth regression model to allow for parameter heterogeneity and cross-sectional error dependence, while simultaneously performing variable selection. We also derive the asymptotic properties of the estimator under both low and high dimensions, and further investigate the finite sample performance of the estimator through Monte Carlo simulations. We apply the framework to a dataset of 89 countries over the period from 1960 to 2014. Our results reveal some cross-country patterns not found in previous studies (e.g., "middle income trap hypothesis", "natural resources curse hypothesis", "religion works via belief, not practice", etc.).||||@arxiv||||2019/03/19||||An Integrated Panel Data Approach to Modelling Economic Growth||||Empirical growth analysis has three major problems --- variable selection, parameter heterogeneity and cross-sectional dependence --- which are addressed independently from each other in most...||||https://arxiv.org/abs/1903.07948v1||||econ||||
685||||None||||Survival investment strategies in a continuous-time market model with competition||||arXiv.org||||2019/09/04||||Survival investment strategies in a continuous-time market model with competition||||Zhitlukhin, Mikhail||||https://arxiv.org/pdf/1811.12491||||1811.12491||||We consider a stochastic game-theoretic model of an investment market in continuous time with short-lived assets and study strategies, called survival, which guarantee that the relative wealth of an investor who uses such a strategy remains bounded away from zero. The main results consist in obtaining a sufficient condition for a strategy to be survival and showing that all survival strategies are asymptotically close to each other. It is also proved that a survival strategy allows an investor to accumulate wealth in a certain sense faster than competitors.||||@arxiv||||2018/11/29||||Survival investment strategies in a continuous-time market model...||||We consider a stochastic game-theoretic model of an investment market in continuous time with short-lived assets and study strategies, called survival, which guarantee that the relative wealth of...||||https://arxiv.org/abs/1811.12491v2||||econ||||
686||||None||||A Consumer Behavior Based Approach to Multi-Stage EV Charging Station Placement||||arXiv.org||||2018/01/07||||A Consumer Behavior Based Approach to Multi-Stage EV Charging Station Placement||||Luo, Chao || Huang, Yih-Fang || Gupta, Vijay||||https://arxiv.org/pdf/1801.02135||||1801.02135||||This paper presents a multi-stage approach to the placement of charging stations under the scenarios of different electric vehicle (EV) penetration rates. The EV charging market is modeled as the oligopoly. A consumer behavior based approach is applied to forecast the charging demand of the charging stations using a nested logit model. The impacts of both the urban road network and the power grid network on charging station planning are also considered. At each planning stage, the optimal station placement strategy is derived through solving a Bayesian game among the service providers. To investigate the interplay of the travel pattern, the consumer behavior, urban road network, power grid network, and the charging station placement, a simulation platform (The EV Virtual City 1.0) is developed using Java on Repast.We conduct a case study in the San Pedro District of Los Angeles by importing the geographic and demographic data of that region into the platform. The simulation results demonstrate a strong consistency between the charging station placement and the traffic flow of EVs. The results also reveal an interesting phenomenon that service providers prefer clustering instead of spatial separation in this oligopoly market.||||@arxiv||||2018/01/07||||A Consumer Behavior Based Approach to Multi-Stage EV Charging...||||This paper presents a multi-stage approach to the placement of charging stations under the scenarios of different electric vehicle (EV) penetration rates. The EV charging market is modeled as the...||||https://arxiv.org/abs/1801.02135v1||||cs||||
687||||None||||Meeting Global Cooling Demand with Photovoltaics during the 21st Century||||arXiv.org||||2019/02/24||||Meeting Global Cooling Demand with Photovoltaics during the 21st Century||||Laine, Hannu S. || Salpakari, Jyri || Looney, Erin E. || Savin, Hele || Peters, Ian Marius || Buonassisi, Tonio||||https://arxiv.org/pdf/1902.10080||||1902.10080||||Space conditioning, and cooling in particular, is a key factor in human productivity and well-being across the globe. During the 21st century, global cooling demand is expected to grow significantly due to the increase in wealth and population in sunny nations across the globe and the advance of global warming. The same locations that see high demand for cooling are also ideal for electricity generation via photovoltaics (PV). Despite the apparent synergy between cooling demand and PV generation, the potential of the cooling sector to sustain PV generation has not been assessed on a global scale. Here, we perform a global assessment of increased PV electricity adoption enabled by the residential cooling sector during the 21st century. Already today, utilizing PV production for cooling could facilitate an additional installed PV capacity of approximately 540 GW, more than the global PV capacity of today. Using established scenarios of population and income growth, as well as accounting for future global warming, we further project that the global residential cooling sector could sustain an added PV capacity between 20-200 GW each year for most of the 21st century, on par with the current global manufacturing capacity of 100 GW. Furthermore, we find that without storage, PV could directly power approximately 50% of cooling demand, and that this fraction is set to increase from 49% to 56% during the 21st century, as cooling demand grows in locations where PV and cooling have a higher synergy. With this geographic shift in demand, the potential of distributed storage also grows. We simulate that with a 1 m$^3$ water-based latent thermal storage per household, the fraction of cooling demand met with PV would increase from 55% to 70% during the century. These results show that the synergy between cooling and PV is notable and could significantly accelerate the growth of the global PV industry.||||@arxiv||||2019/02/24||||Meeting Global Cooling Demand with Photovoltaics during the 21st Century||||Space conditioning, and cooling in particular, is a key factor in human productivity and well-being across the globe. During the 21st century, global cooling demand is expected to grow...||||https://arxiv.org/abs/1902.10080v1||||econ||||
688||||None||||Inference on causal and structural parameters using many moment inequalities||||arXiv.org||||2018/10/18||||Inference on causal and structural parameters using many moment inequalities||||Chernozhukov, Victor || Chetverikov, Denis || Kato, Kengo||||https://arxiv.org/pdf/1312.7614||||1312.7614||||This paper considers the problem of testing many moment inequalities where the number of moment inequalities, denoted by $p$, is possibly much larger than the sample size $n$. There is a variety of economic applications where solving this problem allows to carry out inference on causal and structural parameters, a notable example is the market structure model of Ciliberto and Tamer (2009) where $p=2^{m+1}$ with $m$ being the number of firms that could possibly enter the market. We consider the test statistic given by the maximum of $p$ Studentized (or $t$-type) inequality-specific statistics, and analyze various ways to compute critical values for the test statistic. Specifically, we consider critical values based upon (i) the union bound combined with a moderate deviation inequality for self-normalized sums, (ii) the multiplier and empirical bootstraps, and (iii) two-step and three-step variants of (i) and (ii) by incorporating the selection of uninformative inequalities that are far from being binding and a novel selection of weakly informative inequalities that are potentially binding but do not provide first order information. We prove validity of these methods, showing that under mild conditions, they lead to tests with the error in size decreasing polynomially in $n$ while allowing for $p$ being much larger than $n$, indeed $p$ can be of order $\exp (n^{c})$ for some $c > 0$. Importantly, all these results hold without any restriction on the correlation structure between $p$ Studentized statistics, and also hold uniformly with respect to suitably large classes of underlying distributions. Moreover, in the online supplement, we show validity of a test based on the block multiplier bootstrap in the case of dependent data under some general mixing conditions.||||@arxiv||||2013/12/30||||Inference on causal and structural parameters using many moment...||||This paper considers the problem of testing many moment inequalities where the number of moment inequalities, denoted by $p$, is possibly much larger than the sample size $n$. There is a variety...||||https://arxiv.org/abs/1312.7614v6||||econ||||
689||||None||||Methods, Models, and the Evolution of Moral Psychology||||arXiv.org||||2019/09/09||||Methods, Models, and the Evolution of Moral Psychology||||O'Connor, Cailin||||https://arxiv.org/pdf/1909.09198||||1909.09198||||Why are we good? Why are we bad? Questions regarding the evolution of morality have spurred an astoundingly large interdisciplinary literature. Some significant subset of this body of work addresses questions regarding our moral psychology: how did humans evolve the psychological properties which underpin our systems of ethics and morality? Here I do three things. First, I discuss some methodological issues, and defend particularly effective methods for addressing many research questions in this area. Second, I give an in-depth example, describing how an explanation can be given for the evolution of guilt---one of the core moral emotions---using the methods advocated here. Last, I lay out which sorts of strategic scenarios generally are the ones that our moral psychology evolved to `solve', and thus which models are the most useful in further exploring this evolution.||||@arxiv||||2019/09/09||||Methods, Models, and the Evolution of Moral Psychology||||Why are we good? Why are we bad? Questions regarding the evolution of morality have spurred an astoundingly large interdisciplinary literature. Some significant subset of this body of work...||||https://arxiv.org/abs/1909.09198v1||||econ||||
690||||None||||Graphical potential games||||arXiv.org||||2016/03/23||||Graphical potential games||||Babichenko, Yakov || Tamuz, Omer||||https://arxiv.org/pdf/1405.1481||||1405.1481||||We study the class of potential games that are also graphical games with respect to a given graph $G$ of connections between the players. We show that, up to strategic equivalence, this class of games can be identified with the set of Markov random fields on $G$.   From this characterization, and from the Hammersley-Clifford theorem, it follows that the potentials of such games can be decomposed to local potentials. We use this decomposition to strongly bound the number of strategy changes of a single player along a better response path. This result extends to generalized graphical potential games, which are played on infinite graphs.||||@arxiv||||2014/05/07||||Graphical potential games||||We study the class of potential games that are also graphical games with respect to a given graph $G$ of connections between the players. We show that, up to strategic equivalence, this class of...||||https://arxiv.org/abs/1405.1481v2||||cs||||
691||||None||||CO2 mitigation model for China's residential building sector||||arXiv.org||||2019/09/03||||CO2 mitigation model for China's residential building sector||||Ma, Minda || Cai, Weiguang||||https://arxiv.org/pdf/1909.01249||||1909.01249||||This paper aims to investigate the factors that can mitigate carbon-dioxide (CO2) intensity and further assess CMRBS in China based on a household scale via decomposition analysis. Here we show that: Three types of housing economic indicators and the final emission factor significantly contributed to the decrease in CO2 intensity in the residential building sector. In addition, the CMRBS from 2001-2016 was 1816.99 MtCO2, and the average mitigation intensity during this period was 266.12 kgCO2/household/year. Furthermore, the energy-conservation and emission-mitigation strategy caused CMRBS to effectively increase and is the key to promoting a more significant emission mitigation in the future. Overall, this paper covers the CMRBS assessment gap in China, and the proposed assessment model can be regarded as a reference for other countries and cities for measuring the retrospective CO2 mitigation effect in residential buildings.||||@arxiv||||2019/09/03||||CO2 mitigation model for China's residential building sector||||This paper aims to investigate the factors that can mitigate carbon-dioxide (CO2) intensity and further assess CMRBS in China based on a household scale via decomposition analysis. Here we show...||||https://arxiv.org/abs/1909.01249v1||||econ||||
692||||None||||The Cobb-Douglas production function revisited||||arXiv.org||||2019/10/20||||The Cobb-Douglas production function revisited||||Smirnov, Roman G. || Wang, Kunpeng||||https://arxiv.org/pdf/1910.06739||||1910.06739||||Charles Cobb and Paul Douglas in 1928 used data from the US manufacturing sector for 1899-1922 to introduce what is known today as the Cobb-Douglas production function that has been widely used in economic theory for decades. We employ the R programming language to fit the formulas for the parameters of the Cobb-Douglas production function generated by the authors recently via the bi-Hamiltonian approach to the same data set utilized by Cobb and Douglas. We conclude that the formulas for the output elasticities and total factor productivity are compatible with the original 1928 data.||||@arxiv||||2019/10/11||||The Cobb-Douglas production function revisited||||Charles Cobb and Paul Douglas in 1928 used data from the US manufacturing sector for 1899-1922 to introduce what is known today as the Cobb-Douglas production function that has been widely used in...||||https://arxiv.org/abs/1910.06739v2||||econ||||
693||||None||||Semi-Parametric Efficient Policy Learning with Continuous Actions||||arXiv.org||||2019/07/20||||Semi-Parametric Efficient Policy Learning with Continuous Actions||||Demirer, Mert || Syrgkanis, Vasilis || Lewis, Greg || Chernozhukov, Victor||||https://arxiv.org/pdf/1905.10116||||1905.10116||||We consider off-policy evaluation and optimization with continuous action spaces. We focus on observational data where the data collection policy is unknown and needs to be estimated. We take a semi-parametric approach where the value function takes a known parametric form in the treatment, but we are agnostic on how it depends on the observed contexts. We propose a doubly robust off-policy estimate for this setting and show that off-policy optimization based on this estimate is robust to estimation errors of the policy function or the regression model. Our results also apply if the model does not satisfy our semi-parametric form, but rather we measure regret in terms of the best projection of the true value function to this functional space. Our work extends prior approaches of policy optimization from observational data that only considered discrete actions. We provide an experimental evaluation of our method in a synthetic data example motivated by optimal personalized pricing and costly resource allocation.||||@arxiv||||2019/05/24||||Semi-Parametric Efficient Policy Learning with Continuous Actions||||We consider off-policy evaluation and optimization with continuous action spaces. We focus on observational data where the data collection policy is unknown and needs to be estimated. We take a...||||https://arxiv.org/abs/1905.10116v2||||cs||||
694||||None||||Dissection of Bitcoin's Multiscale Bubble History from January 2012 to February 2018||||arXiv.org||||2019/05/30||||Dissection of Bitcoin's Multiscale Bubble History from January 2012 to February 2018||||Gerlach, Jan-Christian || Demos, Guilherme || Sornette, Didier||||https://arxiv.org/pdf/1804.06261||||1804.06261||||We present a detailed bubble analysis of the Bitcoin to US Dollar price dynamics from January 2012 to February 2018. We introduce a robust automatic peak detection method that classifies price time series into periods of uninterrupted market growth (drawups) and regimes of uninterrupted market decrease (drawdowns). In combination with the Lagrange Regularisation Method for detecting the beginning of a new market regime, we identify 3 major peaks and 10 additional smaller peaks, that have punctuated the dynamics of Bitcoin price during the analyzed time period. We explain this classification of long and short bubbles by a number of quantitative metrics and graphs to understand the main socio-economic drivers behind the ascent of Bitcoin over this period. Then, a detailed analysis of the growing risks associated with the three long bubbles using the Log-Periodic Power Law Singularity (LPPLS) model is based on the LPPLS Confidence Indicators, defined as the fraction of qualified fits of the LPPLS model over multiple time windows. Furthermore, for various fictitious 'present' times $t_2$ before the crashes, we employ a clustering method to group the predicted critical times $t_c$ of the LPPLS fits over different time scales, where $t_c$ is the most probable time for the ending of the bubble. Each cluster is proposed as a plausible scenario for the subsequent Bitcoin price evolution. We present these predictions for the three long bubbles and the four short bubbles that our time scale of analysis was able to resolve. Overall, our predictive scheme provides useful information to warn of an imminent crash risk.||||@arxiv||||2018/04/17||||Dissection of Bitcoin's Multiscale Bubble History from January...||||We present a detailed bubble analysis of the Bitcoin to US Dollar price dynamics from January 2012 to February 2018. We introduce a robust automatic peak detection method that classifies price...||||https://arxiv.org/abs/1804.06261v4||||econ||||
695||||None||||Randomization tests of copula symmetry||||arXiv.org||||2019/11/13||||Randomization tests of copula symmetry||||Beare, Brendan K. || Seo, Juwon||||https://arxiv.org/pdf/1911.05307||||1911.05307||||New nonparametric tests of copula exchangeability and radial symmetry are proposed. The novel aspect of the tests is a resampling procedure that exploits group invariance conditions associated with the relevant symmetry hypothesis. They may be viewed as feasible versions of randomization tests of symmetry, the latter being inapplicable due to the unobservability of margins. Our tests are simple to compute, control size asymptotically, consistently detect arbitrary forms of asymmetry, and do not require the specification of a tuning parameter. Simulations indicate excellent small sample properties compared to existing procedures involving the multiplier bootstrap.||||@arxiv||||2019/11/13||||Randomization tests of copula symmetry||||New nonparametric tests of copula exchangeability and radial symmetry are proposed. The novel aspect of the tests is a resampling procedure that exploits group invariance conditions associated...||||https://arxiv.org/abs/1911.05307v1||||econ||||
696||||None||||High-Dimensional $L_2$Boosting: Rate of Convergence||||arXiv.org||||2016/11/05||||High-Dimensional $L_2$Boosting: Rate of Convergence||||Luo, Ye || Spindler, Martin||||https://arxiv.org/pdf/1602.08927||||1602.08927||||Boosting is one of the most significant developments in machine learning. This paper studies the rate of convergence of $L_2$Boosting, which is tailored for regression, in a high-dimensional setting. Moreover, we introduce so-called \textquotedblleft post-Boosting\textquotedblright. This is a post-selection estimator which applies ordinary least squares to the variables selected in the first stage by $L_2$Boosting. Another variant is \textquotedblleft Orthogonal Boosting\textquotedblright\ where after each step an orthogonal projection is conducted. We show that both post-$L_2$Boosting and the orthogonal boosting achieve the same rate of convergence as LASSO in a sparse, high-dimensional setting. We show that the rate of convergence of the classical $L_2$Boosting depends on the design matrix described by a sparse eigenvalue constant. To show the latter results, we derive new approximation results for the pure greedy algorithm, based on analyzing the revisiting behavior of $L_2$Boosting. We also introduce feasible rules for early stopping, which can be easily implemented and used in applied work. Our results also allow a direct comparison between LASSO and boosting which has been missing from the literature. Finally, we present simulation studies and applications to illustrate the relevance of our theoretical results and to provide insights into the practical aspects of boosting. In these simulation studies, post-$L_2$Boosting clearly outperforms LASSO.||||@arxiv||||2016/02/29||||High-Dimensional $L_2$Boosting: Rate of Convergence||||Boosting is one of the most significant developments in machine learning. This paper studies the rate of convergence of $L_2$Boosting, which is tailored for regression, in a high-dimensional...||||https://arxiv.org/abs/1602.08927v2||||cs||||
697||||None||||The Politics of News Personalization||||arXiv.org||||2019/11/11||||The Politics of News Personalization||||Hu, Lin || Li, Anqi || Segal, Ilya||||https://arxiv.org/pdf/1910.11405||||1910.11405||||We study how news personalization affects policy polarization. In a two-candidate electoral competition model, an attention-maximizing infomediary aggregates information about candidate valence into news, whereas voters decide whether to consume news, trading off the expected utility gain from improved expressive voting against the attention cost. Broadcast news attracts a broad audience by offering a symmetric signal. Personalized news serves extreme voters with skewed signals featuring own-party bias and occasional big surprise. Rational news aggregation yields policy polarization even if candidates are office-motivated. Personalization makes extreme voters the disciplining entity for equilibrium polarization and increases polarization through occasional big surprise.||||@arxiv||||2019/10/24||||The Politics of News Personalization||||We study how news personalization affects policy polarization. In a two-candidate electoral competition model, an attention-maximizing infomediary aggregates information about candidate valence...||||https://arxiv.org/abs/1910.11405v3||||econ||||
698||||None||||The story of conflict and cooperation||||arXiv.org||||2018/08/21||||The story of conflict and cooperation||||Ismail, Mehmet S.||||https://arxiv.org/pdf/1808.06750||||1808.06750||||The story of conflict and cooperation has started millions of years ago, and now it is everywhere: In biology, computer science, economics, humanities, law, philosophy, political science, and psychology. Wars, airline alliances, trade, oligopolistic cartels, evolution of species and genes, and team sports are examples of games of conflict and cooperation. However, Nash (1951)'s noncooperative games - in which each player acts independently without collaboration with any of the others - has become the dominant ideology in economics, game theory, and related fields. A simple falsification of this noncooperative theory is scientific publication: It is a rather competitive game, yet collaboration is widespread. In this paper, I propose a novel way to rationally play games of conflict and cooperation under the Principle of Free Will - players are free to cooperate to coordinate their actions or act independently. Anyone with a basic game theory background will be familiar with the setup in this paper, which is based on simple game trees. In fact, one hardly needs any mathematics to follow the arguments.||||@arxiv||||2018/08/21||||The story of conflict and cooperation||||The story of conflict and cooperation has started millions of years ago, and now it is everywhere: In biology, computer science, economics, humanities, law, philosophy, political science, and...||||https://arxiv.org/abs/1808.06750v1||||cs||||
699||||None||||Reading Macroeconomics From the Yield Curve: The Turkish Case||||arXiv.org||||2019/12/23||||Reading Macroeconomics From the Yield Curve: The Turkish Case||||Turker, Ipek || Cakir, Bayram||||https://arxiv.org/pdf/1912.12351||||1912.12351||||This paper aims to analyze the relationship between yield curve -being a line of the interests in various maturities at a given time- and GDP growth in Turkey. The paper focuses on analyzing the yield curve in relation to its predictive power on Turkish macroeconomic dynamics using the linear regression model. To do so, the interest rate spreads of different maturities are used as a proxy of the yield curve. Findings of the OLS regression are similar to that found in the literature and supports the positive relation between slope of yield curve and GDP growth in Turkey. Moreover, the predicted values of the GDP growth from interest rate spread closely follow the actual GDP growth in Turkey, indicating its predictive power on the economic activity.||||@arxiv||||2019/12/23||||Reading Macroeconomics From the Yield Curve: The Turkish Case||||This paper aims to analyze the relationship between yield curve -being a line of the interests in various maturities at a given time- and GDP growth in Turkey. The paper focuses on analyzing the...||||https://arxiv.org/abs/1912.12351v1||||econ||||
700||||None||||A Unified Framework for Efficient Estimation of General Treatment Models||||arXiv.org||||2018/08/17||||A Unified Framework for Efficient Estimation of General Treatment Models||||Ai, Chunrong || Linton, Oliver || Motegi, Kaiji || Zhang, Zheng||||https://arxiv.org/pdf/1808.04936||||1808.04936||||This paper presents a weighted optimization framework that unifies the binary,multi-valued, continuous, as well as mixture of discrete and continuous treatment, under the unconfounded treatment assignment. With a general loss function, the framework includes the average, quantile and asymmetric least squares causal effect of treatment as special cases. For this general framework, we first derive the semiparametric efficiency bound for the causal effect of treatment, extending the existing bound results to a wider class of models. We then propose a generalized optimization estimation for the causal effect with weights estimated by solving an expanding set of equations. Under some sufficient conditions, we establish consistency and asymptotic normality of the proposed estimator of the causal effect and show that the estimator attains our semiparametric efficiency bound, thereby extending the existing literature on efficient estimation of causal effect to a wider class of applications. Finally, we discuss etimation of some causal effect functionals such as the treatment effect curve and the average outcome. To evaluate the finite sample performance of the proposed procedure, we conduct a small scale simulation study and find that the proposed estimation has practical value. To illustrate the applicability of the procedure, we revisit the literature on campaign advertise and campaign contributions. Unlike the existing procedures which produce mixed results, we find no evidence of campaign advertise on campaign contribution.||||@arxiv||||2018/08/15||||A Unified Framework for Efficient Estimation of General Treatment Models||||This paper presents a weighted optimization framework that unifies the binary,multi-valued, continuous, as well as mixture of discrete and continuous treatment, under the unconfounded treatment...||||https://arxiv.org/abs/1808.04936v2||||econ||||
701||||None||||What does the financial market pricing do? A simulation analysis with a view to systemic volatility, exuberance and vagary||||arXiv.org||||2013/12/28||||What does the financial market pricing do? A simulation analysis with a view to systemic volatility, exuberance and vagary||||Biondi, Yuri || Righi, Simone||||https://arxiv.org/pdf/1312.7460||||1312.7460||||Biondi et al. (2012) develop an analytical model to examine the emergent dynamic properties of share market price formation over time, capable to capture important stylized facts. These latter properties prove to be sensitive to regulatory regimes for fundamental information provision, as well as to market confidence conditions among actual and potential investors. Regimes based upon mark-to-market (fair value) measurement of traded security, while generating higher linear correlation between market prices and fundamental signals, also involve higher market instability and volatility. These regimes also incur more relevant episodes of market exuberance and vagary in some regions of the market confidence space, where lower market liquidity further occurs.||||@arxiv||||2013/12/28||||What does the financial market pricing do? A simulation analysis...||||Biondi et al. (2012) develop an analytical model to examine the emergent dynamic properties of share market price formation over time, capable to capture important stylized facts. These latter...||||https://arxiv.org/abs/1312.7460v1||||econ||||
702||||None||||Non-Parametric Inference Adaptive to Intrinsic Dimension||||arXiv.org||||2019/06/18||||Non-Parametric Inference Adaptive to Intrinsic Dimension||||Khosravi, Khashayar || Lewis, Greg || Syrgkanis, Vasilis||||https://arxiv.org/pdf/1901.03719||||1901.03719||||We consider non-parametric estimation and inference of conditional moment models in high dimensions. We show that even when the dimension $D$ of the conditioning variable is larger than the sample size $n$, estimation and inference is feasible as long as the distribution of the conditioning variable has small intrinsic dimension $d$, as measured by locally low doubling measures. Our estimation is based on a sub-sampled ensemble of the $k$-nearest neighbors ($k$-NN) $Z$-estimator. We show that if the intrinsic dimension of the covariate distribution is equal to $d$, then the finite sample estimation error of our estimator is of order $n^{-1/(d+2)}$ and our estimate is $n^{1/(d+2)}$-asymptotically normal, irrespective of $D$. The sub-sampling size required for achieving these results depends on the unknown intrinsic dimension $d$. We propose an adaptive data-driven approach for choosing this parameter and prove that it achieves the desired rates. We discuss extensions and applications to heterogeneous treatment effect estimation.||||@arxiv||||2019/01/11||||Non-Parametric Inference Adaptive to Intrinsic Dimension||||We consider non-parametric estimation and inference of conditional moment models in high dimensions. We show that even when the dimension $D$ of the conditioning variable is larger than the sample...||||https://arxiv.org/abs/1901.03719v3||||cs||||
703||||None||||Bayesian MIDAS Penalized Regressions: Estimation, Selection, and Prediction||||arXiv.org||||2020/01/02||||Bayesian MIDAS Penalized Regressions: Estimation, Selection, and Prediction||||Mogliani, Matteo || Simoni, Anna||||https://arxiv.org/pdf/1903.08025||||1903.08025||||We propose a new approach to mixed-frequency regressions in a high-dimensional environment that resorts to Group Lasso penalization and Bayesian techniques for estimation and inference. In particular, to improve the prediction properties of the model and its sparse recovery ability, we consider a Group Lasso with a spike-and-slab prior. Penalty hyper-parameters governing the model shrinkage are automatically tuned via an adaptive MCMC algorithm. We establish good frequentist asymptotic properties of the posterior of the in-sample and out-of-sample prediction error, we recover the optimal posterior contraction rate, and we show optimality of the posterior predictive density. Simulations show that the proposed models have good selection and forecasting performance in small samples, even when the design matrix presents cross-correlation. When applied to forecasting U.S. GDP, our penalized regressions can outperform many strong competitors. Results suggest that financial variables may have some, although very limited, short-term predictive content.||||@arxiv||||2019/03/19||||Bayesian MIDAS Penalized Regressions: Estimation, Selection, and Prediction||||We propose a new approach to mixed-frequency regressions in a high-dimensional environment that resorts to Group Lasso penalization and Bayesian techniques for estimation and inference. In...||||https://arxiv.org/abs/1903.08025v2||||econ||||
704||||None||||Efficient Estimation by Fully Modified GLS with an Application to the Environmental Kuznets Curve||||arXiv.org||||2019/08/07||||Efficient Estimation by Fully Modified GLS with an Application to the Environmental Kuznets Curve||||Lin, Yicong || Reuvers, Hanno||||https://arxiv.org/pdf/1908.02552||||1908.02552||||This paper develops the asymptotic theory of a Fully Modified Generalized Least Squares (FMGLS) estimator for multivariate cointegrating polynomial regressions. Such regressions allow for deterministic trends, stochastic trends and integer powers of stochastic trends to enter the cointegrating relations. Our fully modified estimator incorporates: (1) the direct estimation of the inverse autocovariance matrix of the multidimensional errors, and (2) second order bias corrections. The resulting estimator has the intuitive interpretation of applying a weighted least squares objective function to filtered data series. Moreover, the required second order bias corrections are convenient byproducts of our approach and lead to standard asymptotic inference. The FMGLS framework also provides two new KPSS tests for the null of cointegration. A comprehensive simulation study shows good performance of the FMGLS estimator and the related tests. As a practical illustration, we test the Environmental Kuznets Curve (EKC) hypothesis for six early industrialized countries. The more efficient and more powerful FMGLS approach raises important questions concerning the standard model specification for EKC analysis.||||@arxiv||||2019/08/07||||Efficient Estimation by Fully Modified GLS with an Application to...||||This paper develops the asymptotic theory of a Fully Modified Generalized Least Squares (FMGLS) estimator for multivariate cointegrating polynomial regressions. Such regressions allow for...||||https://arxiv.org/abs/1908.02552v1||||econ||||
705||||None||||Minimax Linear Estimation at a Boundary Point||||arXiv.org||||2017/10/18||||Minimax Linear Estimation at a Boundary Point||||Gao, Wayne Yuan||||https://arxiv.org/pdf/1710.06809||||1710.06809||||This paper characterizes the minimax linear estimator of the value of an unknown function at a boundary point of its domain in a Gaussian white noise model under the restriction that the first-order derivative of the unknown function is Lipschitz continuous (the second-order Hölder class). The result is then applied to construct the minimax optimal estimator for the regression discontinuity design model, where the parameter of interest involves function values at boundary points.||||@arxiv||||2017/10/18||||Minimax Linear Estimation at a Boundary Point||||This paper characterizes the minimax linear estimator of the value of an unknown function at a boundary point of its domain in a Gaussian white noise model under the restriction that the...||||https://arxiv.org/abs/1710.06809v1||||econ||||
706||||None||||Structural Estimation of Behavioral Heterogeneity||||arXiv.org||||2018/06/12||||Structural Estimation of Behavioral Heterogeneity||||Shi, Zhentao || Zheng, Huanhuan||||https://arxiv.org/pdf/1802.03735||||1802.03735||||We develop a behavioral asset pricing model in which agents trade in a market with information friction. Profit-maximizing agents switch between trading strategies in response to dynamic market conditions. Due to noisy private information about the fundamental value, the agents form different evaluations about heterogeneous strategies. We exploit a thin set---a small sub-population---to pointly identify this nonlinear model, and estimate the structural parameters using extended method of moments. Based on the estimated parameters, the model produces return time series that emulate the moments of the real data. These results are robust across different sample periods and estimation methods.||||@arxiv||||2018/02/11||||Structural Estimation of Behavioral Heterogeneity||||We develop a behavioral asset pricing model in which agents trade in a market with information friction. Profit-maximizing agents switch between trading strategies in response to dynamic market...||||https://arxiv.org/abs/1802.03735v2||||econ||||
707||||None||||Using generalized estimating equations to estimate nonlinear models with spatial data||||arXiv.org||||2018/10/13||||Using generalized estimating equations to estimate nonlinear models with spatial data||||Lu, Cuicui || Wang, Weining || Wooldridge, Jeffrey M.||||https://arxiv.org/pdf/1810.05855||||1810.05855||||In this paper, we study estimation of nonlinear models with cross sectional data using two-step generalized estimating equations (GEE) in the quasi-maximum likelihood estimation (QMLE) framework. In the interest of improving efficiency, we propose a grouping estimator to account for the potential spatial correlation in the underlying innovations. We use a Poisson model and a Negative Binomial II model for count data and a Probit model for binary response data to demonstrate the GEE procedure. Under mild weak dependency assumptions, results on estimation consistency and asymptotic normality are provided. Monte Carlo simulations show efficiency gain of our approach in comparison of different estimation methods for count data and binary response data. Finally we apply the GEE approach to study the determinants of the inflow foreign direct investment (FDI) to China.||||@arxiv||||2018/10/13||||Using generalized estimating equations to estimate nonlinear...||||In this paper, we study estimation of nonlinear models with cross sectional data using two-step generalized estimating equations (GEE) in the quasi-maximum likelihood estimation (QMLE) framework....||||https://arxiv.org/abs/1810.05855v1||||econ||||
708||||None||||The Role of Strategic Load Participants in Two-Stage Settlement Electricity Markets||||arXiv.org||||2019/09/15||||The Role of Strategic Load Participants in Two-Stage Settlement Electricity Markets||||You, Pengcheng || Gayme, Dennice F. || Mallada, Enrique||||https://arxiv.org/pdf/1903.08341||||1903.08341||||Two-stage electricity market clearing is designed to maintain market efficiency under ideal conditions, e.g., perfect forecast and nonstrategic generation. This work demonstrates that the individual strategic behavior of inelastic load participants in a two-stage settlement electricity market can deteriorate efficiency. Our analysis further implies that virtual bidding can play a role in alleviating this loss of efficiency by mitigating the market power of strategic load participants. We use real-world market data from New York ISO to validate our theory.||||@arxiv||||2019/03/20||||The Role of Strategic Load Participants in Two-Stage Settlement...||||Two-stage electricity market clearing is designed to maintain market efficiency under ideal conditions, e.g., perfect forecast and nonstrategic generation. This work demonstrates that the...||||https://arxiv.org/abs/1903.08341v2||||econ||||
709||||None||||Retrofitting a two-way peg between blockchains||||arXiv.org||||2019/08/12||||Retrofitting a two-way peg between blockchains||||Teutsch, Jason || Straka, Michael || Boneh, Dan||||https://arxiv.org/pdf/1908.03999||||1908.03999||||In December 2015, a bounty emerged to establish both reliable communication and secure transfer of value between the Dogecoin and Ethereum blockchains. This prized "Dogethereum bridge" would allow parties to "lock" a DOGE coin on Dogecoin and in exchange receive a newly minted WOW token in Ethereum. Any subsequent owner of the WOW token could burn it and, in exchange, earn the right to "unlock" a DOGE on Dogecoin.   We describe an efficient, trustless, and retrofitting Dogethereum construction which requires no fork but rather employs economic collateral to achieve a "lock" operation in Dogecoin. The protocol relies on bulletproofs, Truebit, and parametrized tokens to efficiently and trustlessly relay events from the "true" Dogecoin blockchain into Ethereum. The present construction not only enables cross-platform exchange but also allows Ethereum smart contracts to trustlessly access Dogecoin. A similar technique adds Ethereum-based smart contracts to Bitcoin and Bitcoin data to Ethereum smart contracts.||||@arxiv||||2019/08/12||||Retrofitting a two-way peg between blockchains||||In December 2015, a bounty emerged to establish both reliable communication and secure transfer of value between the Dogecoin and Ethereum blockchains. This prized "Dogethereum bridge" would allow...||||https://arxiv.org/abs/1908.03999v1||||cs||||
710||||None||||iCurrency?||||arXiv.org||||2019/11/12||||iCurrency?||||Kakushadze, Zura || Yu, Willie||||https://arxiv.org/pdf/1911.01272||||1911.01272||||We discuss the idea of a purely algorithmic universal world iCurrency set forth in [Kakushadze and Liew, 2014] (https://ssrn.com/abstract=2542541) and expanded in [Kakushadze and Liew, 2017] (https://ssrn.com/abstract=3059330) in light of recent developments, including Libra. Is Libra a contender to become iCurrency? Among other things, we analyze the Libra proposal, including the stability and volatility aspects, and discuss various issues that must be addressed. For instance, one cannot expect a cryptocurrency such as Libra to trade in a narrow band without a robust monetary policy. The presentation in the main text of the paper is intentionally nontechnical. It is followed by an extensive appendix with a mathematical description of the dynamics of (crypto)currency exchange rates in target zones, mechanisms for keeping the exchange rate from breaching the band, the role of volatility, etc.||||@arxiv||||2019/10/28||||iCurrency?||||We discuss the idea of a purely algorithmic universal world iCurrency set forth in [Kakushadze and Liew, 2014] (https://ssrn.com/abstract=2542541) and expanded in [Kakushadze and Liew, 2017]...||||https://arxiv.org/abs/1911.01272v2||||econ||||
711||||None||||Prediction of Shared Bicycle Demand with Wavelet Thresholding||||arXiv.org||||2018/02/08||||Prediction of Shared Bicycle Demand with Wavelet Thresholding||||Westland, J. Christopher || Mou, Jian || Yin, Dafei||||https://arxiv.org/pdf/1802.02683||||1802.02683||||Consumers are creatures of habit, often periodic, tied to work, shopping and other schedules. We analyzed one month of data from the world's largest bike-sharing company to elicit demand behavioral cycles, initially using models from animal tracking that showed large customers fit an Ornstein-Uhlenbeck model with demand peaks at periodicities of 7, 12, 24 hour and 7-days. Lorenz curves of bicycle demand showed that the majority of customer usage was infrequent, and demand cycles from time-series models would strongly overfit the data yielding unreliable models. Analysis of thresholded wavelets for the space-time tensor of bike-sharing contracts was able to compress the data into a 56-coefficient model with little loss of information, suggesting that bike-sharing demand behavior is exceptionally strong and regular. Improvements to predicted demand could be made by adjusting for 'noise' filtered by our model from air quality and weather information and demand from infrequent riders.||||@arxiv||||2018/02/08||||Prediction of Shared Bicycle Demand with Wavelet Thresholding||||Consumers are creatures of habit, often periodic, tied to work, shopping and other schedules. We analyzed one month of data from the world's largest bike-sharing company to elicit demand...||||https://arxiv.org/abs/1802.02683v1||||econ||||
712||||None||||Variety, Complexity and Economic Development||||arXiv.org||||2019/03/19||||Variety, Complexity and Economic Development||||van Dam, Alje || Frenken, Koen||||https://arxiv.org/pdf/1903.07997||||1903.07997||||We propose a combinatorial model of economic development. An economy develops by acquiring new capabilities allowing for the production of an ever greater variety of products of increasingly complex products. Taking into account that economies abandon the least complex products as they develop over time, we show that variety first increases and then decreases in the course of economic development. This is consistent with the empirical pattern known as 'the hump'. Our results question the common association of variety with complexity. We further discuss the implications of our model for future research.||||@arxiv||||2019/03/19||||Variety, Complexity and Economic Development||||We propose a combinatorial model of economic development. An economy develops by acquiring new capabilities allowing for the production of an ever greater variety of products of increasingly...||||https://arxiv.org/abs/1903.07997v1||||econ||||
713||||None||||Estimation and Applications of Quantile Regression for Binary Longitudinal Data||||arXiv.org||||2019/09/12||||Estimation and Applications of Quantile Regression for Binary Longitudinal Data||||Rahman, Mohammad Arshad || Vossmeyer, Angela||||https://arxiv.org/pdf/1909.05560||||1909.05560||||This paper develops a framework for quantile regression in binary longitudinal data settings. A novel Markov chain Monte Carlo (MCMC) method is designed to fit the model and its computational efficiency is demonstrated in a simulation study. The proposed approach is flexible in that it can account for common and individual-specific parameters, as well as multivariate heterogeneity associated with several covariates. The methodology is applied to study female labor force participation and home ownership in the United States. The results offer new insights at the various quantiles, which are of interest to policymakers and researchers alike.||||@arxiv||||2019/09/12||||Estimation and Applications of Quantile Regression for Binary...||||This paper develops a framework for quantile regression in binary longitudinal data settings. A novel Markov chain Monte Carlo (MCMC) method is designed to fit the model and its computational...||||https://arxiv.org/abs/1909.05560v1||||econ||||
714||||None||||Nonfractional Memory: Filtering, Antipersistence, and Forecasting||||arXiv.org||||2018/01/20||||Nonfractional Memory: Filtering, Antipersistence, and Forecasting||||Vera-Valdés, J. Eduardo||||https://arxiv.org/pdf/1801.06677||||1801.06677||||The fractional difference operator remains to be the most popular mechanism to generate long memory due to the existence of efficient algorithms for their simulation and forecasting. Nonetheless, there is no theoretical argument linking the fractional difference operator with the presence of long memory in real data. In this regard, one of the most predominant theoretical explanations for the presence of long memory is cross-sectional aggregation of persistent micro units. Yet, the type of processes obtained by cross-sectional aggregation differs from the one due to fractional differencing. Thus, this paper develops fast algorithms to generate and forecast long memory by cross-sectional aggregation. Moreover, it is shown that the antipersistent phenomenon that arises for negative degrees of memory in the fractional difference literature is not present for cross-sectionally aggregated processes. Pointedly, while the autocorrelations for the fractional difference operator are negative for negative degrees of memory by construction, this restriction does not apply to the cross-sectional aggregated scheme. We show that this has implications for long memory tests in the frequency domain, which will be misspecified for cross-sectionally aggregated processes with negative degrees of memory. Finally, we assess the forecast performance of high-order $AR$ and $ARFIMA$ models when the long memory series are generated by cross-sectional aggregation. Our results are of interest to practitioners developing forecasts of long memory variables like inflation, volatility, and climate data, where aggregation may be the source of long memory.||||@arxiv||||2018/01/20||||Nonfractional Memory: Filtering, Antipersistence, and Forecasting||||The fractional difference operator remains to be the most popular mechanism to generate long memory due to the existence of efficient algorithms for their simulation and forecasting. Nonetheless,...||||https://arxiv.org/abs/1801.06677v1||||econ||||
715||||None||||Adjusted QMLE for the spatial autoregressive parameter||||arXiv.org||||2019/09/17||||Adjusted QMLE for the spatial autoregressive parameter||||Martellosio, Federico || Hillier, Grant||||https://arxiv.org/pdf/1909.08141||||1909.08141||||One simple, and often very effective, way to attenuate the impact of nuisance parameters on maximum likelihood estimation of a parameter of interest is to recenter the profile score for that parameter. We apply this general principle to the quasi-maximum likelihood estimator (QMLE) of the autoregressive parameter $λ$ in a spatial autoregression. The resulting estimator for $λ$ has better finite sample properties compared to the QMLE for $λ$, especially in the presence of a large number of covariates. It can also solve the incidental parameter problem that arises, for example, in social interaction models with network fixed effects, or in spatial panel models with individual or time fixed effects. However, spatial autoregressions present specific challenges for this type of adjustment, because recentering the profile score may cause the adjusted estimate to be outside the usual parameter space for $λ$. Conditions for this to happen are given, and implications are discussed. For inference, we propose confidence intervals based on a Lugannani--Rice approximation to the distribution of the adjusted QMLE of $λ$. Based on our simulations, the coverage properties of these intervals are excellent even in models with a large number of covariates.||||@arxiv||||2019/09/17||||Adjusted QMLE for the spatial autoregressive parameter||||One simple, and often very effective, way to attenuate the impact of nuisance parameters on maximum likelihood estimation of a parameter of interest is to recenter the profile score for that...||||https://arxiv.org/abs/1909.08141v1||||econ||||
716||||None||||Quantile-Regression Inference With Adaptive Control of Size||||arXiv.org||||2019/09/26||||Quantile-Regression Inference With Adaptive Control of Size||||Escanciano, Juan Carlos || Goh, Chuan||||https://arxiv.org/pdf/1807.06977||||1807.06977||||Regression quantiles have asymptotic variances that depend on the conditional densities of the response variable given regressors. This paper develops a new estimate of the asymptotic variance of regression quantiles that leads any resulting Wald-type test or confidence region to behave as well in large samples as its infeasible counterpart in which the true conditional response densities are embedded. We give explicit guidance on implementing the new variance estimator to control adaptively the size of any resulting Wald-type test. Monte Carlo evidence indicates the potential of our approach to deliver powerful tests of heterogeneity of quantile treatment effects in covariates with good size performance over different quantile levels, data-generating processes and sample sizes. We also include an empirical example. Supplementary material is available online.||||@arxiv||||2018/07/18||||Quantile-Regression Inference With Adaptive Control of Size||||Regression quantiles have asymptotic variances that depend on the conditional densities of the response variable given regressors. This paper develops a new estimate of the asymptotic variance of...||||https://arxiv.org/abs/1807.06977v2||||econ||||
717||||None||||On Policy Evaluation with Aggregate Time-Series Shocks||||arXiv.org||||2019/05/31||||On Policy Evaluation with Aggregate Time-Series Shocks||||Arkhangelsky, Dmitry || Korovkin, Vasily||||https://arxiv.org/pdf/1905.13660||||1905.13660||||In this paper we construct a parsimonious causal model that addresses multiple issues researchers face when trying to use aggregate time-series shocks for policy evaluation: (a) potential unobserved aggregate confounders, (b) availability of various unit-level characteristics, (c) time and unit-level heterogeneity in treatment effects. We develop a new estimation algorithm that uses insights from treatment effects, panel, and time-series literature. We construct a variance estimator that is robust to arbitrary clustering pattern across geographical units. We achieve this by considering a finite population framework, where potential outcomes are treated as fixed, and all randomness comes from the exogenous shocks. Finally, we illustrate our approach using data from a study on the causal relationship between foreign aid and conflict conducted in Nunn and Qian [2014].||||@arxiv||||2019/05/31||||On Policy Evaluation with Aggregate Time-Series Shocks||||In this paper we construct a parsimonious causal model that addresses multiple issues researchers face when trying to use aggregate time-series shocks for policy evaluation: (a) potential...||||https://arxiv.org/abs/1905.13660v1||||econ||||
718||||None||||A New Approach to Fair Distribution of Welfare||||arXiv.org||||2019/09/25||||A New Approach to Fair Distribution of Welfare||||Babaioff, Moshe || Feige, Uriel||||https://arxiv.org/pdf/1909.11346||||1909.11346||||We consider transferable-utility profit-sharing games that arise from settings in which agents need to jointly choose one of several alternatives, and may use transfers to redistribute the welfare generated by the chosen alternative. One such setting is the Shared-Rental problem, in which students jointly rent an apartment and need to decide which bedroom to allocate to each student, depending on the student's preferences. Many solution concepts have been proposed for such settings, ranging from mechanisms without transfers, such as Random Priority and the Eating mechanism, to mechanisms with transfers, such as envy free solutions, the Shapley value, and the Kalai-Smorodinsky bargaining solution. We seek a solution concept that satisfies three natural properties, concerning efficiency, fairness and decomposition. We observe that every solution concept known (to us) fails to satisfy at least one of the three properties. We present a new solution concept, designed so as to satisfy the three properties. A certain submodularity condition (which holds in interesting special cases such as the Shared-Rental setting) implies both existence and uniqueness of our solution concept.||||@arxiv||||2019/09/25||||A New Approach to Fair Distribution of Welfare||||We consider transferable-utility profit-sharing games that arise from settings in which agents need to jointly choose one of several alternatives, and may use transfers to redistribute the welfare...||||https://arxiv.org/abs/1909.11346v1||||cs||||
719||||None||||Latent Agents in Networks: Estimation and Pricing||||arXiv.org||||2018/12/12||||Latent Agents in Networks: Estimation and Pricing||||Ata, Baris || Belloni, Alexandre || Candogan, Ozan||||https://arxiv.org/pdf/1808.04878||||1808.04878||||We focus on a setting where agents in a social network consume a product that exhibits positive local network externalities. A seller has access to data on past consumption decisions/prices for a subset of observable agents, and can target these agents with appropriate discounts to exploit network effects and increase her revenues. A novel feature of the model is that the observable agents potentially interact with additional latent agents. These latent agents can purchase the same product from a different channel, and are not observed by the seller. Observable agents influence each other both directly and indirectly through the influence they exert on the latent agents. The seller knows the connection structure of neither the observable nor the latent part of the network.   Due to the presence of network externalities, an agent's consumption decision depends not only on the price offered to her, but also on the consumption decisions of (and in turn the prices offered to) her neighbors in the underlying network. We investigate how the seller can use the available data to estimate the matrix that captures the dependence of observable agents' consumption decisions on the prices offered to them. We provide an algorithm for estimating this matrix under an approximate sparsity condition, and obtain convergence rates for the proposed estimator despite the high dimensionality that allows more agents than observations. Importantly, we then show that this approximate sparsity condition holds under standard conditions present in the literature and hence our algorithms are applicable to a large class of networks. We establish that by using the estimated matrix the seller can construct prices that lead to a small revenue loss relative to revenue-maximizing prices under complete information, and the optimality gap vanishes relative to the size of the network.||||@arxiv||||2018/08/14||||Latent Agents in Networks: Estimation and Pricing||||We focus on a setting where agents in a social network consume a product that exhibits positive local network externalities. A seller has access to data on past consumption decisions/prices for a...||||https://arxiv.org/abs/1808.04878v2||||cs||||
720||||None||||Kernel Estimation for Panel Data with Heterogeneous Dynamics||||arXiv.org||||2019/05/27||||Kernel Estimation for Panel Data with Heterogeneous Dynamics||||Okui, Ryo || Yanagi, Takahide||||https://arxiv.org/pdf/1802.08825||||1802.08825||||This paper proposes nonparametric kernel-smoothing estimation for panel data to examine the degree of heterogeneity across cross-sectional units. We first estimate the sample mean, autocovariances, and autocorrelations for each unit and then apply kernel smoothing to compute their density functions. The dependence of the kernel estimator on bandwidth makes asymptotic bias of very high order affect the required condition on the relative magnitudes of the cross-sectional sample size (N) and the time-series length (T). In particular, it makes the condition on N and T stronger and more complicated than those typically observed in the long-panel literature without kernel smoothing. We also consider a split-panel jackknife method to correct bias and construction of confidence intervals. An empirical application and Monte Carlo simulations illustrate our procedure in finite samples.||||@arxiv||||2018/02/24||||Kernel Estimation for Panel Data with Heterogeneous Dynamics||||This paper proposes nonparametric kernel-smoothing estimation for panel data to examine the degree of heterogeneity across cross-sectional units. We first estimate the sample mean,...||||https://arxiv.org/abs/1802.08825v4||||econ||||
721||||None||||Elusive Longer-Run Impacts of Head Start: Replications Within and Across Cohorts||||arXiv.org||||2020/02/02||||Elusive Longer-Run Impacts of Head Start: Replications Within and Across Cohorts||||Pages, Remy J. -C. || Lukes, Dylan J. || Bailey, Drew H. || Duncan, Greg J.||||https://arxiv.org/pdf/1903.01954||||1903.01954||||Using an additional decade of CNLSY data, this study replicated and extended Deming's (2009) evaluation of Head Start's life-cycle skill formation impacts in three ways. Extending the measurement interval for Deming's adulthood outcomes, we found no statistically significant impacts on earnings and mixed evidence of impacts on other adult outcomes. Applying Deming's sibling comparison framework to more recent birth cohorts born to CNLSY mothers revealed mostly negative Head Start impacts. Combining all cohorts shows generally null impacts on school-age and early adulthood outcomes.||||@arxiv||||2019/03/05||||Elusive Longer-Run Impacts of Head Start: Replications Within and...||||Using an additional decade of CNLSY data, this study replicated and extended Deming's (2009) evaluation of Head Start's life-cycle skill formation impacts in three ways. Extending the measurement...||||https://arxiv.org/abs/1903.01954v4||||econ||||
722||||None||||Shrinkage for Categorical Regressors||||arXiv.org||||2019/01/07||||Shrinkage for Categorical Regressors||||Heiler, Phillip || Mareckova, Jana||||https://arxiv.org/pdf/1901.01898||||1901.01898||||This paper introduces a flexible regularization approach that reduces point estimation risk of group means stemming from e.g. categorical regressors, (quasi-)experimental data or panel data models. The loss function is penalized by adding weighted squared l2-norm differences between group location parameters and informative first-stage estimates. Under quadratic loss, the penalized estimation problem has a simple interpretable closed-form solution that nests methods established in the literature on ridge regression, discretized support smoothing kernels and model averaging methods. We derive risk-optimal penalty parameters and propose a plug-in approach for estimation. The large sample properties are analyzed in an asymptotic local to zero framework by introducing a class of sequences for close and distant systems of locations that is sufficient for describing a large range of data generating processes. We provide the asymptotic distributions of the shrinkage estimators under different penalization schemes. The proposed plug-in estimator uniformly dominates the ordinary least squares in terms of asymptotic risk if the number of groups is larger than three. Monte Carlo simulations reveal robust improvements over standard methods in finite samples. Real data examples of estimating time trends in a panel and a difference-in-differences study illustrate potential applications.||||@arxiv||||2019/01/07||||Shrinkage for Categorical Regressors||||This paper introduces a flexible regularization approach that reduces point estimation risk of group means stemming from e.g. categorical regressors, (quasi-)experimental data or panel data...||||https://arxiv.org/abs/1901.01898v1||||econ||||
723||||None||||The dynamic impact of monetary policy on regional housing prices in the US: Evidence based on factor-augmented vector autoregressions||||arXiv.org||||2018/02/16||||The dynamic impact of monetary policy on regional housing prices in the US: Evidence based on factor-augmented vector autoregressions||||Fischer, Manfred M. || Huber, Florian || Pfarrhofer, Michael || Staufer-Steinnocher, Petra||||https://arxiv.org/pdf/1802.05870||||1802.05870||||In this study interest centers on regional differences in the response of housing prices to monetary policy shocks in the US. We address this issue by analyzing monthly home price data for metropolitan regions using a factor-augmented vector autoregression (FAVAR) model. Bayesian model estimation is based on Gibbs sampling with Normal-Gamma shrinkage priors for the autoregressive coefficients and factor loadings, while monetary policy shocks are identified using high-frequency surprises around policy announcements as external instruments. The empirical results indicate that monetary policy actions typically have sizeable and significant positive effects on regional housing prices, revealing differences in magnitude and duration. The largest effects are observed in regions located in states on both the East and West Coasts, notably California, Arizona and Florida.||||@arxiv||||2018/02/16||||The dynamic impact of monetary policy on regional housing prices...||||In this study interest centers on regional differences in the response of housing prices to monetary policy shocks in the US. We address this issue by analyzing monthly home price data for...||||https://arxiv.org/abs/1802.05870v1||||econ||||
724||||None||||Comprehensive Time-Series Regression Models Using GRETL -- U.S. GDP and Government Consumption Expenditures & Gross Investment from 1980 to 2013||||arXiv.org||||2019/08/17||||Comprehensive Time-Series Regression Models Using GRETL -- U.S. GDP and Government Consumption Expenditures & Gross Investment from 1980 to 2013||||Shi, Juehui||||https://arxiv.org/pdf/1412.5397||||1412.5397||||Using Gretl, I apply ARMA, Vector ARMA, VAR, state-space model with a Kalman filter, transfer-function and intervention models, unit root tests, cointegration test, volatility models (ARCH, GARCH, ARCH-M, GARCH-M, Taylor-Schwert GARCH, GJR, TARCH, NARCH, APARCH, EGARCH) to analyze quarterly time series of GDP and Government Consumption Expenditures & Gross Investment (GCEGI) from 1980 to 2013. The article is organized as: (I) Definition; (II) Regression Models; (III) Discussion. Additionally, I discovered a unique interaction between GDP and GCEGI in both the short-run and the long-run and provided policy makers with some suggestions. For example in the short run, GDP responded positively and very significantly (0.00248) to GCEGI, while GCEGI reacted positively but not too significantly (0.08051) to GDP. In the long run, current GDP responded negatively and permanently (0.09229) to a shock in past GCEGI, while current GCEGI reacted negatively yet temporarily (0.29821) to a shock in past GDP. Therefore, policy makers should not adjust current GCEGI based merely on the condition of current and past GDP. Although increasing GCEGI does help GDP in the short-term, significantly abrupt increase in GCEGI might not be good to the long-term health of GDP. Instead, a balanced, sustainable, and economically viable solution is recommended, so that the short-term benefits to the current economy from increasing GCEGI often largely secured by the long-term loan outweigh or at least equal to the negative effect to the future economy from the long-term debt incurred by the loan. Finally, I found that non-normally distributed volatility models generally perform better than normally distributed ones. More specifically, TARCH-GED performs the best in the group of non-normally distributed, while GARCH-M does the best in the group of normally distributed.||||@arxiv||||2014/12/17||||Comprehensive Time-Series Regression Models Using GRETL -- U.S....||||Using Gretl, I apply ARMA, Vector ARMA, VAR, state-space model with a Kalman filter, transfer-function and intervention models, unit root tests, cointegration test, volatility models (ARCH, GARCH,...||||https://arxiv.org/abs/1412.5397v3||||econ||||
725||||None||||On the Effect of Imputation on the 2SLS Variance||||arXiv.org||||2019/03/26||||On the Effect of Imputation on the 2SLS Variance||||Farbmacher, Helmut || Kann, Alexander||||https://arxiv.org/pdf/1903.11004||||1903.11004||||Endogeneity and missing data are common issues in empirical research. We investigate how both jointly affect inference on causal parameters. Conventional methods to estimate the variance, which treat the imputed data as if it was observed in the first place, are not reliable. We derive the asymptotic variance and propose a heteroskedasticity robust variance estimator for two-stage least squares which accounts for the imputation. Monte Carlo simulations support our theoretical findings.||||@arxiv||||2019/03/26||||On the Effect of Imputation on the 2SLS Variance||||Endogeneity and missing data are common issues in empirical research. We investigate how both jointly affect inference on causal parameters. Conventional methods to estimate the variance, which...||||https://arxiv.org/abs/1903.11004v1||||econ||||
726||||None||||Revenue Sharing in the Internet: A Moral Hazard Approach and a Net-neutrality Perspective||||arXiv.org||||2019/08/26||||Revenue Sharing in the Internet: A Moral Hazard Approach and a Net-neutrality Perspective||||Malik, Fehmina || ~Hanawal, Manjesh K. || Hayel, Yezekael || Nair, Jayakrishnan||||https://arxiv.org/pdf/1908.09580||||1908.09580||||Revenue sharing contracts between Content Providers (CPs) and Internet Service Providers (ISPs) can act as leverage for enhancing the infrastructure of the Internet. ISPs can be incentivized to make investments in network infrastructure that improve Quality of Service (QoS) for users if attractive contracts are negotiated between them and CPs. The idea here is that part of the net profit gained by CPs are given to ISPs to invest in the network. The Moral Hazard economic framework is used to model such an interaction, in which a principal determines a contract, and an agent reacts by adapting her effort. In our setting, several competitive CPs interact through one common ISP. Two cases are studied: (i) the ISP differentiates between the CPs and makes a (potentially) different investment to improve the QoS of each CP, and (ii) the ISP does not differentiate between CPs and makes a common investment for both. The last scenario can be viewed as \emph{network neutral behavior} on the part of the ISP. We analyse the optimal contracts and show that the CP that can better monetize its demand always prefers the non-neutral regime. Interestingly, ISP revenue, as well as social utility, are also found to be higher under the non-neutral regime.||||@arxiv||||2019/08/26||||Revenue Sharing in the Internet: A Moral Hazard Approach and a...||||Revenue sharing contracts between Content Providers (CPs) and Internet Service Providers (ISPs) can act as leverage for enhancing the infrastructure of the Internet. ISPs can be incentivized to...||||https://arxiv.org/abs/1908.09580v1||||cs||||
727||||None||||Bounding Causes of Effects with Mediators||||arXiv.org||||2019/06/30||||Bounding Causes of Effects with Mediators||||Dawid, Philip || Humphreys, Macartan || Musio, Monica||||https://arxiv.org/pdf/1907.00399||||1907.00399||||Suppose X and Y are binary exposure and outcome variables, and we have full knowledge of the distribution of Y, given application of X. From this we know the average causal effect of X on Y. We are now interested in assessing, for a case that was exposed and exhibited a positive outcome, whether it was the exposure that caused the outcome. The relevant "probability of causation", PC, typically is not identified by the distribution of Y given X, but bounds can be placed on it, and these bounds can be improved if we have further information about the causal process. Here we consider cases where we know the probabilistic structure for a sequence of complete mediators between X and Y. We derive a general formula for calculating bounds on PC for any pattern of data on the mediators (including the case with no data). We show that the largest and smallest upper and lower bounds that can result from any complete mediation process can be obtained in processes with at most two steps. We also consider homogeneous processes with many mediators. PC can sometimes be identified as 0 with negative data, but it cannot be identified at 1 even with positive data on an infinite set of mediators. The results have implications for learning about causation from knowledge of general processes and of data on cases.||||@arxiv||||2019/06/30||||Bounding Causes of Effects with Mediators||||Suppose X and Y are binary exposure and outcome variables, and we have full knowledge of the distribution of Y, given application of X. From this we know the average causal effect of X on Y. We...||||https://arxiv.org/abs/1907.00399v1||||econ||||
728||||None||||Heterogeneous Choice Sets and Preferences||||arXiv.org||||2019/07/04||||Heterogeneous Choice Sets and Preferences||||Barseghyan, Levon || Coughlin, Maura || Molinari, Francesca || Teitelbaum, Joshua C.||||https://arxiv.org/pdf/1907.02337||||1907.02337||||We propose a robust method of discrete choice analysis when agents' choice sets are unobserved. Our core model assumes nothing about agents' choice sets apart from their minimum size. Importantly, it leaves unrestricted the dependence, conditional on observables, between agents' choice sets and their preferences. We first establish that the model is partially identified and characterize its sharp identification region. We also show how the model can be used to assess the welfare cost of limited choice sets. We then apply our theoretical findings to learn about households' risk preferences and choice sets from data on their deductible choices in auto collision insurance. We find that the data can be explained by expected utility theory with relatively low levels of risk aversion and heterogeneous choice sets. We also find that a mixed logit model, as well as some familiar models of choice set formation, are rejected in our data.||||@arxiv||||2019/07/04||||Heterogeneous Choice Sets and Preferences||||We propose a robust method of discrete choice analysis when agents' choice sets are unobserved. Our core model assumes nothing about agents' choice sets apart from their minimum size. Importantly,...||||https://arxiv.org/abs/1907.02337v1||||econ||||
729||||None||||Post-Selection and Post-Regularization Inference in Linear Models with Many Controls and Instruments||||arXiv.org||||2015/01/13||||Post-Selection and Post-Regularization Inference in Linear Models with Many Controls and Instruments||||Chernozhukov, Victor || Hansen, Christian || Spindler, Martin||||https://arxiv.org/pdf/1501.03185||||1501.03185||||In this note, we offer an approach to estimating causal/structural parameters in the presence of many instruments and controls based on methods for estimating sparse high-dimensional models. We use these high-dimensional methods to select both which instruments and which control variables to use. The approach we take extends BCCH2012, which covers selection of instruments for IV models with a small number of controls, and extends BCH2014, which covers selection of controls in models where the variable of interest is exogenous conditional on observables, to accommodate both a large number of controls and a large number of instruments. We illustrate the approach with a simulation and an empirical example. Technical supporting material is available in a supplementary online appendix.||||@arxiv||||2015/01/13||||Post-Selection and Post-Regularization Inference in Linear Models...||||In this note, we offer an approach to estimating causal/structural parameters in the presence of many instruments and controls based on methods for estimating sparse high-dimensional models. We...||||https://arxiv.org/abs/1501.03185v1||||econ||||
730||||None||||Estimating Dynamic Conditional Spread Densities to Optimise Daily Storage Trading of Electricity||||arXiv.org||||2019/03/09||||Estimating Dynamic Conditional Spread Densities to Optimise Daily Storage Trading of Electricity||||Abramova, Ekaterina || Bunn, Derek||||https://arxiv.org/pdf/1903.06668||||1903.06668||||This paper formulates dynamic density functions, based upon skewed-t and similar representations, to model and forecast electricity price spreads between different hours of the day. This supports an optimal day ahead storage and discharge schedule, and thereby facilitates a bidding strategy for a merchant arbitrage facility into the day-ahead auctions for wholesale electricity. The four latent moments of the density functions are dynamic and conditional upon exogenous drivers, thereby permitting the mean, variance, skewness and kurtosis of the densities to respond hourly to such factors as weather and demand forecasts. The best specification for each spread is selected based on the Pinball Loss function, following the closed form analytical solutions of the cumulative density functions. Those analytical properties also allow the calculation of risk associated with the spread arbitrages. From these spread densities, the optimal daily operation of a battery storage facility is determined.||||@arxiv||||2019/03/09||||Estimating Dynamic Conditional Spread Densities to Optimise Daily...||||This paper formulates dynamic density functions, based upon skewed-t and similar representations, to model and forecast electricity price spreads between different hours of the day. This supports...||||https://arxiv.org/abs/1903.06668v1||||cs||||
731||||None||||Identifying Effects of Multivalued Treatments||||arXiv.org||||2018/04/30||||Identifying Effects of Multivalued Treatments||||Lee, Sokbae || Salanié, Bernard||||https://arxiv.org/pdf/1805.00057||||1805.00057||||Multivalued treatment models have typically been studied under restrictive assumptions: ordered choice, and more recently unordered monotonicity. We show how treatment effects can be identified in a more general class of models that allows for multidimensional unobserved heterogeneity. Our results rely on two main assumptions: treatment assignment must be a measurable function of threshold-crossing rules, and enough continuous instruments must be available. We illustrate our approach for several classes of models.||||@arxiv||||2018/04/30||||Identifying Effects of Multivalued Treatments||||Multivalued treatment models have typically been studied under restrictive assumptions: ordered choice, and more recently unordered monotonicity. We show how treatment effects can be identified in...||||https://arxiv.org/abs/1805.00057v1||||econ||||
732||||None||||Elicitation of ambiguous beliefs with mixing bets||||arXiv.org||||2019/10/22||||Elicitation of ambiguous beliefs with mixing bets||||Schmidt, Patrick||||https://arxiv.org/pdf/1902.07447||||1902.07447||||I consider the elicitation of ambiguous beliefs about an event and show how to identify the interval of relevant probabilities (representing ambiguity perception) for several classes of ambiguity averse preferences. The agent reveals her preference for mixing binarized bets on the uncertain event and its complement under varying betting odds. Under ambiguity aversion, mixing is informative about the interval of beliefs. In particular, the mechanism allows to distinguish ambiguous beliefs from point beliefs, and identifies the belief interval for maxmin preferences. For ambiguity averse smooth second order and variational preferences, the mechanism reveals inner bounds for the belief interval, which are sharp under additional assumptions. In an experimental study, participants perceive almost as much ambiguity for natural events (generated by the stock exchange and by a prisoners dilemma game) as for the Ellsberg Urn, indicating that ambiguity may play a role in real-world decision making.||||@arxiv||||2019/02/20||||Elicitation of ambiguous beliefs with mixing bets||||I consider the elicitation of ambiguous beliefs about an event and show how to identify the interval of relevant probabilities (representing ambiguity perception) for several classes of ambiguity...||||https://arxiv.org/abs/1902.07447v2||||econ||||
733||||None||||On a gap between rational annuitization price for producer and price for customer||||arXiv.org||||2018/09/24||||On a gap between rational annuitization price for producer and price for customer||||Dokuchaev, Nikolai||||https://arxiv.org/pdf/1809.08960||||1809.08960||||The paper studies pricing of insurance products focusing on the pricing of annuities under uncertainty. This pricing problem is crucial for financial decision making and was studied intensively, however, many open questions still remain. In particular, there is a so-called "annuity puzzle" related to certain inconsistency of existing financial theory with the empirical observations for the annuities market. The paper suggests a pricing method based on the risk minimization such that both producer and customer seek to minimize the mean square hedging error accepted as a measure of risk. This leads to two different versions of the pricing problem: the selection of the annuity price given the rate of regular payments, and the selection of the rate of payments given the annuity price. It appears that solutions of these two problems are different. This can contribute to explanation for the "annuity puzzle".||||@arxiv||||2018/09/24||||On a gap between rational annuitization price for producer and...||||The paper studies pricing of insurance products focusing on the pricing of annuities under uncertainty. This pricing problem is crucial for financial decision making and was studied intensively,...||||https://arxiv.org/abs/1809.08960v1||||econ||||
734||||None||||Entropic Decision Making||||arXiv.org||||2020/01/01||||Entropic Decision Making||||Rebei, Adnan||||https://arxiv.org/pdf/2001.00122||||2001.00122||||Using results from neurobiology on perceptual decision making and value-based decision making, the problem of decision making between lotteries is reformulated in an abstract space where uncertain prospects are mapped to corresponding active neuronal representations. This mapping allows us to maximize non-extensive entropy in the new space with some constraints instead of a utility function. To achieve good agreements with behavioral data, the constraints must include at least constraints on the weighted average of the stimulus and on its variance. Both constraints are supported by the adaptability of neuronal responses to an external stimulus. By analogy with thermodynamic and information engines, we discuss the dynamics of choice between two lotteries as they are being processed simultaneously in the brain by rate equations that describe the transfer of attention between lotteries and within the various prospects of each lottery. This model is able to give new insights on risk aversion and on behavioral anomalies not accounted for by Prospect Theory.||||@arxiv||||2020/01/01||||Entropic Decision Making||||Using results from neurobiology on perceptual decision making and value-based decision making, the problem of decision making between lotteries is reformulated in an abstract space where uncertain...||||https://arxiv.org/abs/2001.00122v1||||econ||||
735||||None||||Mean-shift least squares model averaging||||arXiv.org||||2019/12/03||||Mean-shift least squares model averaging||||McAlinn, Kenichiro || Takanashi, Kosaku||||https://arxiv.org/pdf/1912.01194||||1912.01194||||This paper proposes a new estimator for selecting weights to average over least squares estimates obtained from a set of models. Our proposed estimator builds on the Mallows model average (MMA) estimator of Hansen (2007), but, unlike MMA, simultaneously controls for location bias and regression error through a common constant. We show that our proposed estimator-- the mean-shift Mallows model average (MSA) estimator-- is asymptotically optimal to the original MMA estimator in terms of mean squared error. A simulation study is presented, where we show that our proposed estimator uniformly outperforms the MMA estimator.||||@arxiv||||2019/12/03||||Mean-shift least squares model averaging||||This paper proposes a new estimator for selecting weights to average over least squares estimates obtained from a set of models. Our proposed estimator builds on the Mallows model average (MMA)...||||https://arxiv.org/abs/1912.01194v1||||econ||||
736||||None||||A Theory of Dichotomous Valuation with Applications to Variable Selection||||arXiv.org||||2019/01/15||||A Theory of Dichotomous Valuation with Applications to Variable Selection||||Hu, Xingwei||||https://arxiv.org/pdf/1808.00131||||1808.00131||||An econometric or statistical model may undergo a marginal gain if we admit a new variable to the model, and a marginal loss if we remove an existing variable from the model. Assuming equality of opportunity among all candidate variables, we derive a few evaluation methodologies by the expected marginal gain and marginal loss in all potential modeling scenarios. However, marginal gain and loss are not symmetric; thus, we introduce several unbiased solutions. Simulation studies show that our approaches significantly outperform a few variable selection methods used in practice. The results also explore several novel features of the Shapley value.||||@arxiv||||2018/08/01||||A Theory of Dichotomous Valuation with Applications to Variable Selection||||An econometric or statistical model may undergo a marginal gain if we admit a new variable to the model, and a marginal loss if we remove an existing variable from the model. Assuming equality of...||||https://arxiv.org/abs/1808.00131v3||||cs||||
737||||None||||Can Mobility-on-Demand services do better after discerning reliability preferences of riders?||||arXiv.org||||2019/04/16||||Can Mobility-on-Demand services do better after discerning reliability preferences of riders?||||Bansal, Prateek || Liu, Yang || Daziano, Ricardo || Samaranayake, Samitha||||https://arxiv.org/pdf/1904.07987||||1904.07987||||We formalize one aspect of reliability in the context of Mobility-on-Demand (MoD) systems by acknowledging the uncertainty in the pick-up time of these services. This study answers two key questions: i) how the difference between the stated and actual pick-up times affect the propensity of a passenger to choose an MoD service? ii) how an MoD service provider can leverage this information to increase its ridership? We conduct a discrete choice experiment in New York to answer the former question and adopt a micro-simulation-based optimization method to answer the latter question. In our experiments, the ridership of an MoD service could be increased by up to 10\% via displaying the predicted wait time strategically.||||@arxiv||||2019/04/16||||Can Mobility-on-Demand services do better after discerning...||||We formalize one aspect of reliability in the context of Mobility-on-Demand (MoD) systems by acknowledging the uncertainty in the pick-up time of these services. This study answers two key...||||https://arxiv.org/abs/1904.07987v1||||econ||||
738||||None||||Stable and Efficient Structures in Multigroup Network Formation||||arXiv.org||||2020/01/28||||Stable and Efficient Structures in Multigroup Network Formation||||Mohagheghi, Shadi || Ma, Jingying || Bullo, Francesco||||https://arxiv.org/pdf/2001.10627||||2001.10627||||In this work we present a strategic network formation model predicting the emergence of multigroup structures. Individuals decide to form or remove links based on the benefits and costs those connections carry; we focus on bilateral consent for link formation. An exogenous system specifies the frequency of coordination issues arising among the groups. We are interested in structures that arise to resolve coordination issues and, specifically, structures in which groups are linked through bridging, redundant, and co-membership interconnections. We characterize the conditions under which certain structures are stable and study their efficiency as well as the convergence of formation dynamics.||||@arxiv||||2020/01/28||||Stable and Efficient Structures in Multigroup Network Formation||||In this work we present a strategic network formation model predicting the emergence of multigroup structures. Individuals decide to form or remove links based on the benefits and costs those...||||https://arxiv.org/abs/2001.10627v1||||cs||||
739||||None||||A Multicriteria Macroeconomic Model with Intertemporal Equity and Spatial Spillovers||||arXiv.org||||2019/11/19||||A Multicriteria Macroeconomic Model with Intertemporal Equity and Spatial Spillovers||||Kunze, Herb || La Torre, Davide || Marsiglio, Simone||||https://arxiv.org/pdf/1911.08247||||1911.08247||||We analyze a macroeconomic model with intergenerational equity considerations and spatial spillovers, which gives rise to a multicriteria optimization problem. Intergenerational equity requires to add in the definition of social welfare a long run sustainability criterion to the traditional discounted utilitarian criterion. The spatial structure allows for the possibility of heterogeneiity and spatial diffusion implies that all locations within the spatial domain are interconnected via spatial spillovers. We rely on different techniques (scalarization, $ε$-constraint method and goal programming) to analyze such a spatial multicriteria problem, relying on numerical approaches to illustrate the nature of the trade-off between the discounted utilitarian and the sustainability criteria.||||@arxiv||||2019/11/19||||A Multicriteria Macroeconomic Model with Intertemporal Equity and...||||We analyze a macroeconomic model with intergenerational equity considerations and spatial spillovers, which gives rise to a multicriteria optimization problem. Intergenerational equity requires to...||||https://arxiv.org/abs/1911.08247v1||||econ||||
740||||None||||Synthetic Control Methods and Big Data||||arXiv.org||||2018/02/28||||Synthetic Control Methods and Big Data||||Kinn, Daniel||||https://arxiv.org/pdf/1803.00096||||1803.00096||||Many macroeconomic policy questions may be assessed in a case study framework, where the time series of a treated unit is compared to a counterfactual constructed from a large pool of control units. I provide a general framework for this setting, tailored to predict the counterfactual by minimizing a tradeoff between underfitting (bias) and overfitting (variance). The framework nests recently proposed structural and reduced form machine learning approaches as special cases. Furthermore, difference-in-differences with matching and the original synthetic control are restrictive cases of the framework, in general not minimizing the bias-variance objective. Using simulation studies I find that machine learning methods outperform traditional methods when the number of potential controls is large or the treated unit is substantially different from the controls. Equipped with a toolbox of approaches, I revisit a study on the effect of economic liberalisation on economic growth. I find effects for several countries where no effect was found in the original study. Furthermore, I inspect how a systematically important bank respond to increasing capital requirements by using a large pool of banks to estimate the counterfactual. Finally, I assess the effect of a changing product price on product sales using a novel scanner dataset.||||@arxiv||||2018/02/28||||Synthetic Control Methods and Big Data||||Many macroeconomic policy questions may be assessed in a case study framework, where the time series of a treated unit is compared to a counterfactual constructed from a large pool of control...||||https://arxiv.org/abs/1803.00096v1||||econ||||
741||||None||||Robonomics: The Study of Robot-Human Peer-to-Peer Financial Transactions and Agreements||||arXiv.org||||2019/08/18||||Robonomics: The Study of Robot-Human Peer-to-Peer Financial Transactions and Agreements||||Cardenas, Irvin Steve || Kim, Jong-Hoon||||https://arxiv.org/pdf/1908.07393||||1908.07393||||The concept of a blockchain has given way to the development of cryptocurrencies, enabled smart contracts, and unlocked a plethora of other disruptive technologies. But, beyond its use case in cryptocurrencies, and in network coordination and automation, blockchain technology may have serious sociotechnical implications in the future co-existence of robots and humans. Motivated by the recent explosion of interest around blockchains, and our extensive work on open-source blockchain technology and its integration into robotics - this paper provides insights in ways in which blockchains and other decentralized technologies can impact our interactions with robot agents and the social integration of robots into human society.||||@arxiv||||2019/08/18||||Robonomics: The Study of Robot-Human Peer-to-Peer Financial...||||The concept of a blockchain has given way to the development of cryptocurrencies, enabled smart contracts, and unlocked a plethora of other disruptive technologies. But, beyond its use case in...||||https://arxiv.org/abs/1908.07393v1||||cs||||
742||||None||||Characterizing Assumption of Rationality by Incomplete Information||||arXiv.org||||2018/01/15||||Characterizing Assumption of Rationality by Incomplete Information||||Liu, Shuige||||https://arxiv.org/pdf/1801.04714||||1801.04714||||We characterize common assumption of rationality of 2-person games within an incomplete information framework. We use the lexicographic model with incomplete information and show that a belief hierarchy expresses common assumption of rationality within a complete information framework if and only if there is a belief hierarchy within the corresponding incomplete information framework that expresses common full belief in caution, rationality, every good choice is supported, and prior belief in the original utility functions.||||@arxiv||||2018/01/15||||Characterizing Assumption of Rationality by Incomplete Information||||We characterize common assumption of rationality of 2-person games within an incomplete information framework. We use the lexicographic model with incomplete information and show that a belief...||||https://arxiv.org/abs/1801.04714v1||||econ||||
743||||None||||A Primal-dual Learning Algorithm for Personalized Dynamic Pricing with an Inventory Constraint||||arXiv.org||||2018/12/20||||A Primal-dual Learning Algorithm for Personalized Dynamic Pricing with an Inventory Constraint||||Chen, Ningyuan || Gallego, Guillermo||||https://arxiv.org/pdf/1812.09234||||1812.09234||||A firm is selling a product to different types (based on the features such as education backgrounds, ages, etc.) of customers over a finite season with non-replenishable initial inventory. The type label of an arriving customer can be observed but the demand function associated with each type is initially unknown. The firm sets personalized prices dynamically for each type and attempts to maximize the revenue over the season. We provide a learning algorithm that is near-optimal when the demand and capacity scale in proportion. The algorithm utilizes the primal-dual formulation of the problem and learns the dual optimal solution explicitly. It allows the algorithm to overcome the curse of dimensionality (the rate of regret is independent of the number of types) and sheds light on novel algorithmic designs for learning problems with resource constraints.||||@arxiv||||2018/12/20||||A Primal-dual Learning Algorithm for Personalized Dynamic Pricing...||||A firm is selling a product to different types (based on the features such as education backgrounds, ages, etc.) of customers over a finite season with non-replenishable initial inventory. The...||||https://arxiv.org/abs/1812.09234v1||||cs||||
744||||None||||Structural Change Analysis of Active Cryptocurrency Market||||arXiv.org||||2019/09/24||||Structural Change Analysis of Active Cryptocurrency Market||||Tan, C. Y. || Koh, Y. B. || Ng, K. H. || Ng, K. H.||||https://arxiv.org/pdf/1909.10679||||1909.10679||||Structural Change Analysis of Active Cryptocurrency Market||||@arxiv||||2019/09/24||||Structural Change Analysis of Active Cryptocurrency Market||||Structural Change Analysis of Active Cryptocurrency Market||||https://arxiv.org/abs/1909.10679v1||||econ||||
745||||None||||Cheating with (Recursive) Models||||arXiv.org||||2019/11/04||||Cheating with (Recursive) Models||||Eliaz, Kfir || Spiegler, Ran || Weiss, Yair||||https://arxiv.org/pdf/1911.01251||||1911.01251||||To what extent can agents with misspecified subjective models predict false correlations? We study an "analyst" who utilizes models that take the form of a recursive system of linear regression equations. The analyst fits each equation to minimize the sum of squared errors against an arbitrarily large sample. We characterize the maximal pairwise correlation that the analyst can predict given a generic objective covariance matrix, subject to the constraint that the estimated model does not distort the mean and variance of individual variables. We show that as the number of variables in the model grows, the false pairwise correlation can become arbitrarily close to one, regardless of the true correlation.||||@arxiv||||2019/11/04||||Cheating with (Recursive) Models||||To what extent can agents with misspecified subjective models predict false correlations? We study an "analyst" who utilizes models that take the form of a recursive system of linear regression...||||https://arxiv.org/abs/1911.01251v1||||econ||||
746||||None||||Interaction of a Hydrogen Refueling Station Network for Heavy-Duty Vehicles and the Power System in Germany for 2050||||arXiv.org||||2019/08/27||||Interaction of a Hydrogen Refueling Station Network for Heavy-Duty Vehicles and the Power System in Germany for 2050||||Kluschke, Philipp || Neumann, Fabian||||https://arxiv.org/pdf/1908.10119||||1908.10119||||A potential solution to reduce greenhouse gas (GHG) emissions in the transport sector is to use alternatively fueled vehicles (AFV). Heavy-duty vehicles (HDV) emit a large share of GHG emissions in the transport sector and are therefore the subject of growing attention from global regulators. Fuel cell and green hydrogen technologies are a promising option to decarbonize HDVs, as their fast refueling and long vehicle ranges are in line with current logistic operation concepts. Moreover, the application of green hydrogen in transport could enable more effective integration of renewable energies (RE) across different energy sectors. This paper explores the interplay between HDV Hydrogen Refueling Stations (HRS) that produce hydrogen locally and the power system by combining an infrastructure location planning model and an energy system optimization model that takes grid expansion options into account. Two scenarios - one sizing refueling stations in symbiosis with the power system and one sizing them independently of it - are assessed regarding their impacts on the total annual energy system costs, regional RE integration and the levelized cost of hydrogen (LCOH). The impacts are calculated based on locational marginal pricing for 2050. Depending on the integration scenario, we find average LCOH of between 5.66 euro/kg and 6.20 euro/kg, for which nodal electricity prices are the main determining factor as well as a strong difference in LCOH between north and south Germany. From a system perspective, investing in HDV-HRS in symbiosis with the power system rather than independently promises cost savings of around one billion-euros per annum. We therefore conclude that the co-optimization of multiple energy sectors is important for investment planning and has the potential to exploit synergies.||||@arxiv||||2019/08/27||||Interaction of a Hydrogen Refueling Station Network for Heavy-Duty...||||A potential solution to reduce greenhouse gas (GHG) emissions in the transport sector is to use alternatively fueled vehicles (AFV). Heavy-duty vehicles (HDV) emit a large share of GHG emissions...||||https://arxiv.org/abs/1908.10119v1||||econ||||
747||||None||||A Mathematical Analysis of an Election System Proposed by Gottlob Frege||||arXiv.org||||2019/09/30||||A Mathematical Analysis of an Election System Proposed by Gottlob Frege||||Harrenstein, Paul || Lackner, Marie-Louise || Lackner, Martin||||https://arxiv.org/pdf/1907.03643||||1907.03643||||We provide a mathematical analysis of an election system proposed by the eminent logician Gottlob Frege (1848--1925). His proposal was written presumably in 1918, was (re)discovered around the turn of the millennium, and published for the first time in the original German in 2000. A remarkable feature of Frege's proposal is its concern for the representation of minorities and its sensitivity to past election results. Frege's proposal is based on some highly original and relevant ideas; his core idea is that the votes of unelected candidates are carried over to the next election. All candidates thus accumulate votes over time and eventually each candidate is elected at some point. We provide a mathematical formulation of Frege's election system and investigate how well it achieves its aim of a fair representation of all political opinions in a community over time. We can prove that this goal is fulfilled remarkably well. However, we also show that, in other aspects, it falls short of Frege's high ambition that no voter's vote be lost. We propose a slight modification of his voting rule, the modified Frege method, that remedies these shortcomings. We analyse both methods from the perspective of modern social choice and apportionment theory, and can show that they are novel contributions with noteworthy proportionality properties over time.||||@arxiv||||2019/07/08||||A Mathematical Analysis of an Election System Proposed by Gottlob Frege||||We provide a mathematical analysis of an election system proposed by the eminent logician Gottlob Frege (1848--1925). His proposal was written presumably in 1918, was (re)discovered around the...||||https://arxiv.org/abs/1907.03643v2||||cs||||
748||||None||||Panel Data Quantile Regression with Grouped Fixed Effects||||arXiv.org||||2018/08/05||||Panel Data Quantile Regression with Grouped Fixed Effects||||Gu, Jiaying || Volgushev, Stanislav||||https://arxiv.org/pdf/1801.05041||||1801.05041||||This paper introduces estimation methods for grouped latent heterogeneity in panel data quantile regression. We assume that the observed individuals come from a heterogeneous population with a finite number of types. The number of types and group membership is not assumed to be known in advance and is estimated by means of a convex optimization problem. We provide conditions under which group membership is estimated consistently and establish asymptotic normality of the resulting estimators. Simulations show that the method works well in finite samples when T is reasonably large. To illustrate the proposed methodology we study the effects of the adoption of Right-to-Carry concealed weapon laws on violent crime rates using panel data of 51 U.S. states from 1977 - 2010.||||@arxiv||||2018/01/15||||Panel Data Quantile Regression with Grouped Fixed Effects||||This paper introduces estimation methods for grouped latent heterogeneity in panel data quantile regression. We assume that the observed individuals come from a heterogeneous population with a...||||https://arxiv.org/abs/1801.05041v2||||econ||||
749||||None||||Currency Based on Time Standard||||arXiv.org||||2019/10/17||||Currency Based on Time Standard||||Kala, Tomas||||https://arxiv.org/pdf/1910.07859||||1910.07859||||The Total Economic Time Capacity of a Year 525600 minutes is postulated as a time standard for a new Monetary Minute currency in this evaluation study. Consequently, the Monetary Minute MonMin is defined as a 1/525600 part of the Total Economic Time Capacity of a Year. The Value CMonMin of the Monetary Minute MonMin is equal to a 1/525600 part of the GDP, p.c., expressed in a specific state currency C. There is described how the Monetary Minutes MonMin are determined, and how their values CMonMin are calculated based on the GDP and all the population in specific economies. The Monetary Minutes trace different aggregate productivity, i.e. exploitation of the total time capacity of a year for generating of the GDP in economies of different states.||||@arxiv||||2019/10/17||||Currency Based on Time Standard||||The Total Economic Time Capacity of a Year 525600 minutes is postulated as a time standard for a new Monetary Minute currency in this evaluation study. Consequently, the Monetary Minute MonMin is...||||https://arxiv.org/abs/1910.07859v1||||econ||||
750||||None||||Distributional conformal prediction||||arXiv.org||||2019/09/17||||Distributional conformal prediction||||Chernozhukov, Victor || Wüthrich, Kaspar || Zhu, Yinchu||||https://arxiv.org/pdf/1909.07889||||1909.07889||||We propose a robust method for constructing conditionally valid prediction intervals based on regression models for conditional distributions such as quantile and distribution regression. Our approach exploits the probability integral transform and relies on permuting estimated ``ranks'' instead of regression residuals. Unlike residuals, these ranks are independent of the covariates, which allows us to establish the conditional validity of the resulting prediction intervals under consistent estimation of the conditional distributions. We also establish theoretical performance guarantees under arbitrary model misspecification. The usefulness of the proposed method is illustrated based on two applications. First, we study the problem of predicting daily returns using realized volatility. Second, we consider a synthetic control setting where the goal is to predict a country's counterfactual GDP growth rate based on the contemporaneous GDP growth rates of other countries.||||@arxiv||||2019/09/17||||Distributional conformal prediction||||We propose a robust method for constructing conditionally valid prediction intervals based on regression models for conditional distributions such as quantile and distribution regression. Our...||||https://arxiv.org/abs/1909.07889v1||||econ||||
751||||None||||Fundamental Limits of Testing the Independence of Irrelevant Alternatives in Discrete Choice||||arXiv.org||||2020/01/20||||Fundamental Limits of Testing the Independence of Irrelevant Alternatives in Discrete Choice||||Seshadri, Arjun || Ugander, Johan||||https://arxiv.org/pdf/2001.07042||||2001.07042||||The Multinomial Logit (MNL) model and the axiom it satisfies, the Independence of Irrelevant Alternatives (IIA), are together the most widely used tools of discrete choice. The MNL model serves as the workhorse model for a variety of fields, but is also widely criticized, with a large body of experimental literature claiming to document real-world settings where IIA fails to hold. Statistical tests of IIA as a modelling assumption have been the subject of many practical tests focusing on specific deviations from IIA over the past several decades, but the formal size properties of hypothesis testing IIA are still not well understood. In this work we replace some of the ambiguity in this literature with rigorous pessimism, demonstrating that any general test for IIA with low worst-case error would require a number of samples exponential in the number of alternatives of the choice problem. A major benefit of our analysis over previous work is that it lies entirely in the finite-sample domain, a feature crucial to understanding the behavior of tests in the common data-poor settings of discrete choice. Our lower bounds are structure-dependent, and as a potential cause for optimism, we find that if one restricts the test of IIA to violations that can occur in a specific collection of choice sets (e.g., pairs), one obtains structure-dependent lower bounds that are much less pessimistic. Our analysis of this testing problem is unorthodox in being highly combinatorial, counting Eulerian orientations of cycle decompositions of a particular bipartite graph constructed from a data set of choices. By identifying fundamental relationships between the comparison structure of a given testing problem and its sample efficiency, we hope these relationships will help lay the groundwork for a rigorous rethinking of the IIA testing problem as well as other testing problems in discrete choice.||||@arxiv||||2020/01/20||||Fundamental Limits of Testing the Independence of Irrelevant...||||The Multinomial Logit (MNL) model and the axiom it satisfies, the Independence of Irrelevant Alternatives (IIA), are together the most widely used tools of discrete choice. The MNL model serves as...||||https://arxiv.org/abs/2001.07042v1||||econ||||
752||||None||||Fast Algorithms for the Quantile Regression Process||||arXiv.org||||2019/09/12||||Fast Algorithms for the Quantile Regression Process||||Chernozhukov, Victor || Fernández-Val, Iván || Melly, Blaise||||https://arxiv.org/pdf/1909.05782||||1909.05782||||The widespread use of quantile regression methods depends crucially on the existence of fast algorithms. Despite numerous algorithmic improvements, the computation time is still non-negligible because researchers often estimate many quantile regressions and use the bootstrap for inference. We suggest two new fast algorithms for the estimation of a sequence of quantile regressions at many quantile indexes. The first algorithm applies the preprocessing idea of Portnoy and Koenker (1997) but exploits a previously estimated quantile regression to guess the sign of the residuals. This step allows for a reduction of the effective sample size. The second algorithm starts from a previously estimated quantile regression at a similar quantile index and updates it using a single Newton-Raphson iteration. The first algorithm is exact, while the second is only asymptotically equivalent to the traditional quantile regression estimator. We also apply the preprocessing idea to the bootstrap by using the sample estimates to guess the sign of the residuals in the bootstrap sample. Simulations show that our new algorithms provide very large improvements in computation time without significant (if any) cost in the quality of the estimates. For instance, we divide by 100 the time required to estimate 99 quantile regressions with 20 regressors and 50,000 observations.||||@arxiv||||2019/09/12||||Fast Algorithms for the Quantile Regression Process||||The widespread use of quantile regression methods depends crucially on the existence of fast algorithms. Despite numerous algorithmic improvements, the computation time is still non-negligible...||||https://arxiv.org/abs/1909.05782v1||||econ||||
753||||None||||Noncooperative dynamics in election interference||||arXiv.org||||2020/01/09||||Noncooperative dynamics in election interference||||Dewhurst, David Rushing || Danforth, Christopher M. || Dodds, Peter Sheridan||||https://arxiv.org/pdf/1908.02793||||1908.02793||||Foreign power interference in domestic elections is an existential threat to societies. Manifested through myriad methods from war to words, such interference is a timely example of strategic interaction between economic and political agents. We model this interaction between rational game players as a continuous-time differential game, constructing an analytical model of this competition with a variety of payoff structures. All-or-nothing attitudes by only one player regarding the outcome of the game lead to an arms race in which both countries spend increasing amounts on interference and counter-interference operations. We then confront our model with data pertaining to the Russian interference in the 2016 United States presidential election contest. We introduce and estimate a Bayesian structural time series model of election polls and social media posts by Russian Twitter troll accounts. Our analytical model, while purposefully abstract and simple, adequately captures many temporal characteristics of the election and social media activity. We close with a discussion of our model's shortcomings and suggestions for future research.||||@arxiv||||2019/08/07||||Noncooperative dynamics in election interference||||Foreign power interference in domestic elections is an existential threat to societies. Manifested through myriad methods from war to words, such interference is a timely example of strategic...||||https://arxiv.org/abs/1908.02793v4||||econ||||
754||||None||||Hiring in the substance use disorder treatment related sector during the first five years of Medicaid expansion||||arXiv.org||||2019/08/01||||Hiring in the substance use disorder treatment related sector during the first five years of Medicaid expansion||||Scrivner, Olga || Nguyen, Thuy || Simon, Kosali || Middaugh, Esmé || Taska, Bledi || Börner, Katy||||https://arxiv.org/pdf/1908.00216||||1908.00216||||Effective treatment strategies exist for substance use disorder (SUD), however severe hurdles remain in ensuring adequacy of the SUD treatment (SUDT) workforce as well as improving SUDT affordability, access and stigma. Although evidence shows recent increases in SUD medication access from expanding Medicaid availability under the Affordable Care Act, it is yet unknown whether these policies also led to a growth in the changes in the nature of hiring in SUDT related workforce, partly due to poor data availability. Our study uses novel data to shed light on recent trends in a fast-evolving and policy-relevant labor market, and contributes to understanding the current SUDT related workforce and the effect of Medicaid expansion on hiring attempts in this sector. We examine attempts over 2010-2018 at hiring in the SUDT and related behavioral health sector as background for estimating the causal effect of the 2014-and-beyond state Medicaid expansion on these outcomes through "difference-in-difference" econometric models. We use Burning Glass Technologies (BGT) data covering virtually all U.S. job postings by employers. Nationally, we find little growth in the sector's hiring attempts in 2010-2018 relative to the rest of the economy or to health care as a whole. However, this masks diverging trends in subsectors, which saw reduction in hospital based hiring attempts, increases towards outpatient facilities, and changes in occupational hiring demand shifting from medical personnel towards counselors and social workers. Although Medicaid expansion did not lead to any statistically significant or meaningful change in overall hiring attempts, there was a shift in the hiring landscape.||||@arxiv||||2019/08/01||||Hiring in the substance use disorder treatment related sector...||||Effective treatment strategies exist for substance use disorder (SUD), however severe hurdles remain in ensuring adequacy of the SUD treatment (SUDT) workforce as well as improving SUDT...||||https://arxiv.org/abs/1908.00216v1||||econ||||
755||||None||||LM-BIC Model Selection in Semiparametric Models||||arXiv.org||||2018/11/26||||LM-BIC Model Selection in Semiparametric Models||||Korolev, Ivan||||https://arxiv.org/pdf/1811.10676||||1811.10676||||This paper studies model selection in semiparametric econometric models. It develops a consistent series-based model selection procedure based on a Bayesian Information Criterion (BIC) type criterion to select between several classes of models. The procedure selects a model by minimizing the semiparametric Lagrange Multiplier (LM) type test statistic from Korolev (2018) but additionally rewards simpler models. The paper also develops consistent upward testing (UT) and downward testing (DT) procedures based on the semiparametric LM type specification test. The proposed semiparametric LM-BIC and UT procedures demonstrate good performance in simulations. To illustrate the use of these semiparametric model selection procedures, I apply them to the parametric and semiparametric gasoline demand specifications from Yatchew and No (2001). The LM-BIC procedure selects the semiparametric specification that is nonparametric in age but parametric in all other variables, which is in line with the conclusions in Yatchew and No (2001). The results of the UT and DT procedures heavily depend on the choice of tuning parameters and assumptions about the model errors.||||@arxiv||||2018/11/26||||LM-BIC Model Selection in Semiparametric Models||||This paper studies model selection in semiparametric econometric models. It develops a consistent series-based model selection procedure based on a Bayesian Information Criterion (BIC) type...||||https://arxiv.org/abs/1811.10676v1||||econ||||
756||||None||||Two-Step Estimation and Inference with Possibly Many Included Covariates||||arXiv.org||||2018/07/26||||Two-Step Estimation and Inference with Possibly Many Included Covariates||||Cattaneo, Matias D. || Jansson, Michael || Ma, Xinwei||||https://arxiv.org/pdf/1807.10100||||1807.10100||||We study the implications of including many covariates in a first-step estimate entering a two-step estimation procedure. We find that a first order bias emerges when the number of \textit{included} covariates is "large" relative to the square-root of sample size, rendering standard inference procedures invalid. We show that the jackknife is able to estimate this "many covariates" bias consistently, thereby delivering a new automatic bias-corrected two-step point estimator. The jackknife also consistently estimates the standard error of the original two-step point estimator. For inference, we develop a valid post-bias-correction bootstrap approximation that accounts for the additional variability introduced by the jackknife bias-correction. We find that the jackknife bias-corrected point estimator and the bootstrap post-bias-correction inference perform excellent in simulations, offering important improvements over conventional two-step point estimators and inference procedures, which are not robust to including many covariates. We apply our results to an array of distinct treatment effect, policy evaluation, and other applied microeconomics settings. In particular, we discuss production function and marginal treatment effect estimation in detail.||||@arxiv||||2018/07/26||||Two-Step Estimation and Inference with Possibly Many Included Covariates||||We study the implications of including many covariates in a first-step estimate entering a two-step estimation procedure. We find that a first order bias emerges when the number of...||||https://arxiv.org/abs/1807.10100v1||||econ||||
757||||None||||The Prosumer Economy -- Being Like a Forest||||arXiv.org||||2019/03/16||||The Prosumer Economy -- Being Like a Forest||||Ozesmi, Uygar||||https://arxiv.org/pdf/1903.07615||||1903.07615||||Planetary life support systems are collapsing due to climate change and the biodiversity crisis. The root cause is the existing consumer economy, coupled with profit maximisation based on ecological and social externalities. Trends can be reversed, civilisation may be saved by transforming the profit maximising consumer economy into an ecologically and socially just economy, which we call the prosumer economy. Prosumer economy is a macro scale circular economy with minimum negative or positive ecological and social impact, an ecosystem of producers and prosumers, who have synergistic and circular relationships with deepened circular supply chains, networks, where leakage of wealth out of the system is minimised. In a prosumer economy there is no waste, no lasting negative impacts on the ecology and no social exploitation. The prosumer economy is like a lake or a forest, an economic ecosystem that is productive and supportive of the planet. We are already planting this forest through Good4Trust.org, started in Turkey. Good4Trust is a community platform bringing together ecologically and socially just producers and prosumers. Prosumers come together around a basic ethical tenet the golden rule and share on the platform their good deeds. The relationship are already deepening and circularity is forming to create a prosumer economy. The platforms software to structure the economy is open source, and is available to be licenced to start Good4Trust anywhere on the planet. Complexity theory tells us that if enough agents in a given system adopt simple rules which they all follow, the system may shift. The shift from a consumer economy to a prosumer economy has already started, the future is either ecologically and socially just or bust.||||@arxiv||||2019/03/16||||The Prosumer Economy -- Being Like a Forest||||Planetary life support systems are collapsing due to climate change and the biodiversity crisis. The root cause is the existing consumer economy, coupled with profit maximisation based on...||||https://arxiv.org/abs/1903.07615v1||||econ||||
758||||None||||Complexity Theory, Game Theory, and Economics: The Barbados Lectures||||arXiv.org||||2020/02/08||||Complexity Theory, Game Theory, and Economics: The Barbados Lectures||||Roughgarden, Tim||||https://arxiv.org/pdf/1801.00734||||1801.00734||||This document collects the lecture notes from my mini-course "Complexity Theory, Game Theory, and Economics," taught at the Bellairs Research Institute of McGill University, Holetown, Barbados, February 19--23, 2017, as the 29th McGill Invitational Workshop on Computational Complexity.   The goal of this mini-course is twofold: (i) to explain how complexity theory has helped illuminate several barriers in economics and game theory; and (ii) to illustrate how game-theoretic questions have led to new and interesting complexity theory, including recent several breakthroughs. It consists of two five-lecture sequences: the Solar Lectures, focusing on the communication and computational complexity of computing equilibria; and the Lunar Lectures, focusing on applications of complexity theory in game theory and economics. No background in game theory is assumed.||||@arxiv||||2018/01/02||||Complexity Theory, Game Theory, and Economics: The Barbados Lectures||||This document collects the lecture notes from my mini-course "Complexity Theory, Game Theory, and Economics," taught at the Bellairs Research Institute of McGill University, Holetown, Barbados,...||||https://arxiv.org/abs/1801.00734v3||||cs||||
759||||None||||Price Setting on a Network||||arXiv.org||||2019/04/14||||Price Setting on a Network||||Hinnosaar, Toomas||||https://arxiv.org/pdf/1904.06757||||1904.06757||||Most products are produced and sold by supply chain networks, where an interconnected network of producers and intermediaries set prices to maximize their profits. I show that there exists a unique equilibrium in a price-setting game on a network. The key distortion reducing both total profits and social welfare is multiple-marginalization, which is magnified by strategic interactions. Individual profits are proportional to influentiality, which is a new measure of network centrality defined by the equilibrium characterization. The results emphasize the importance of the network structure when considering policy questions such as mergers or trade tariffs.||||@arxiv||||2019/04/14||||Price Setting on a Network||||Most products are produced and sold by supply chain networks, where an interconnected network of producers and intermediaries set prices to maximize their profits. I show that there exists a...||||https://arxiv.org/abs/1904.06757v1||||cs||||
760||||None||||Practical and robust $t$-test based inference for synthetic control and related methods||||arXiv.org||||2020/02/01||||Practical and robust $t$-test based inference for synthetic control and related methods||||Chernozhukov, Victor || Wuthrich, Kaspar || Zhu, Yinchu||||https://arxiv.org/pdf/1812.10820||||1812.10820||||This paper proposes a practical and robust method for making inference on average treatment effects estimated by synthetic control and related methods. We develop a $K$-fold cross-fitting procedure for bias-correction. To avoid the difficult estimation of the long-run variance, inference is based on a self-normalized $t$-statistic, which has an asymptotically pivotal $t$-distribution. Our procedure only requires consistent (in $\ell_2$-norm) estimation of the parameters, which can be verified for many popular estimators. The proposed method is easy to implement, provably robust against misspecification, more efficient than difference-in-differences, valid with non-stationary data, and demonstrates an excellent small sample performance.||||@arxiv||||2018/12/27||||Practical and robust $t$-test based inference for synthetic...||||This paper proposes a practical and robust method for making inference on average treatment effects estimated by synthetic control and related methods. We develop a $K$-fold cross-fitting...||||https://arxiv.org/abs/1812.10820v3||||econ||||
761||||None||||Prediction in locally stationary time series||||arXiv.org||||2020/01/04||||Prediction in locally stationary time series||||Dette, Holger || Wu, Weichi||||https://arxiv.org/pdf/2001.00419||||2001.00419||||We develop an estimator for the high-dimensional covariance matrix of a locally stationary process with a smoothly varying trend and use this statistic to derive consistent predictors in non-stationary time series. In contrast to the currently available methods for this problem the predictor developed here does not rely on fitting an autoregressive model and does not require a vanishing trend. The finite sample properties of the new methodology are illustrated by means of a simulation study and a financial indices study.||||@arxiv||||2020/01/02||||Prediction in locally stationary time series||||We develop an estimator for the high-dimensional covariance matrix of a locally stationary process with a smoothly varying trend and use this statistic to derive consistent predictors in...||||https://arxiv.org/abs/2001.00419v2||||econ||||
762||||None||||Forecasting with Dynamic Panel Data Models||||arXiv.org||||2017/09/28||||Forecasting with Dynamic Panel Data Models||||Liu, Laura || Moon, Hyungsik Roger || Schorfheide, Frank||||https://arxiv.org/pdf/1709.10193||||1709.10193||||This paper considers the problem of forecasting a collection of short time series using cross sectional information in panel data. We construct point predictors using Tweedie's formula for the posterior mean of heterogeneous coefficients under a correlated random effects distribution. This formula utilizes cross-sectional information to transform the unit-specific (quasi) maximum likelihood estimator into an approximation of the posterior mean under a prior distribution that equals the population distribution of the random coefficients. We show that the risk of a predictor based on a non-parametric estimate of the Tweedie correction is asymptotically equivalent to the risk of a predictor that treats the correlated-random-effects distribution as known (ratio-optimality). Our empirical Bayes predictor performs well compared to various competitors in a Monte Carlo study. In an empirical application we use the predictor to forecast revenues for a large panel of bank holding companies and compare forecasts that condition on actual and severely adverse macroeconomic conditions.||||@arxiv||||2017/09/28||||Forecasting with Dynamic Panel Data Models||||This paper considers the problem of forecasting a collection of short time series using cross sectional information in panel data. We construct point predictors using Tweedie's formula for the...||||https://arxiv.org/abs/1709.10193v1||||econ||||
763||||None||||Information processing constraints in travel behaviour modelling: A generative learning approach||||arXiv.org||||2019/07/25||||Information processing constraints in travel behaviour modelling: A generative learning approach||||Wong, Melvin || Farooq, Bilal||||https://arxiv.org/pdf/1907.07036||||1907.07036||||Travel decisions tend to exhibit sensitivity to uncertainty and information processing constraints. These behavioural conditions can be characterized by a generative learning process. We propose a data-driven generative model version of rational inattention theory to emulate these behavioural representations. We outline the methodology of the generative model and the associated learning process as well as provide an intuitive explanation of how this process captures the value of prior information in the choice utility specification. We demonstrate the effects of information heterogeneity on a travel choice, analyze the econometric interpretation, and explore the properties of our generative model. Our findings indicate a strong correlation with rational inattention behaviour theory, which suggest that individuals may ignore certain exogenous variables and rely on prior information for evaluating decisions under uncertainty. Finally, the principles demonstrated in this study can be formulated as a generalized entropy and utility based multinomial logit model.||||@arxiv||||2019/07/16||||Information processing constraints in travel behaviour modelling:...||||Travel decisions tend to exhibit sensitivity to uncertainty and information processing constraints. These behavioural conditions can be characterized by a generative learning process. We propose a...||||https://arxiv.org/abs/1907.07036v2||||econ||||
764||||None||||Identification of Conduit Countries and Community Structures in the Withholding Tax Networks||||arXiv.org||||2018/06/03||||Identification of Conduit Countries and Community Structures in the Withholding Tax Networks||||Nakamoto, Tembo || Ikeda, Yuichi||||https://arxiv.org/pdf/1806.00799||||1806.00799||||Due to economic globalization, each country's economic law, including tax laws and tax treaties, has been forced to work as a single network. However, each jurisdiction (country or region) has not made its economic law under the assumption that its law functions as an element of one network, so it has brought unexpected results. We thought that the results are exactly international tax avoidance. To contribute to the solution of international tax avoidance, we tried to investigate which part of the network is vulnerable. Specifically, focusing on treaty shopping, which is one of international tax avoidance methods, we attempt to identified which jurisdiction are likely to be used for treaty shopping from tax liabilities and the relationship between jurisdictions which are likely to be used for treaty shopping and others. For that purpose, based on withholding tax rates imposed on dividends, interest, and royalties by jurisdictions, we produced weighted multiple directed graphs, computed the centralities and detected the communities. As a result, we clarified the jurisdictions that are likely to be used for treaty shopping and pointed out that there are community structures. The results of this study suggested that fewer jurisdictions need to introduce more regulations for prevention of treaty abuse worldwide.||||@arxiv||||2018/06/03||||Identification of Conduit Countries and Community Structures in...||||Due to economic globalization, each country's economic law, including tax laws and tax treaties, has been forced to work as a single network. However, each jurisdiction (country or region) has not...||||https://arxiv.org/abs/1806.00799v1||||econ||||
765||||None||||Science Quality and the Value of Inventions||||arXiv.org||||2019/04/03||||Science Quality and the Value of Inventions||||Poege, Felix || Harhoff, Dietmar || Gaessler, Fabian || Baruffaldi, Stefano||||https://arxiv.org/pdf/1903.05020||||1903.05020||||Despite decades of research, the relationship between the quality of science and the value of inventions has remained unclear. We present the result of a large-scale matching exercise between 4.8 million patent families and 43 million publication records. We find a strong positive relationship between quality of scientific contributions referenced in patents and the value of the respective inventions. We rank patents by the quality of the science they are linked to. Strikingly, high-rank patents are twice as valuable as low-rank patents, which in turn are about as valuable as patents without direct science link. We show this core result for various science quality and patent value measures. The effect of science quality on patent value remains relevant even when science is linked indirectly through other patents. Our findings imply that what is considered "excellent" within the science sector also leads to outstanding outcomes in the technological or commercial realm.||||@arxiv||||2019/03/12||||Science Quality and the Value of Inventions||||Despite decades of research, the relationship between the quality of science and the value of inventions has remained unclear. We present the result of a large-scale matching exercise between 4.8...||||https://arxiv.org/abs/1903.05020v2||||cs||||
766||||None||||Quantile Factor Models||||arXiv.org||||2019/11/06||||Quantile Factor Models||||Chen, Liang || Dolado, Juan Jose || Gonzalo, Jesus||||https://arxiv.org/pdf/1911.02173||||1911.02173||||Quantile Factor Models (QFM) represent a new class of factor models for high-dimensional panel data. Unlike Approximate Factor Models (AFM), where only location-shifting factors can be extracted, QFM also allow to recover unobserved factors shifting other relevant parts of the distributions of observed variables. A quantile regression approach, labeled Quantile Factor Analysis (QFA), is proposed to consistently estimate all the quantile-dependent factors and loadings. Their asymptotic distribution is then derived using a kernel-smoothed version of the QFA estimators. Two consistent model selection criteria, based on information criteria and rank minimization, are developed to determine the number of factors at each quantile. Moreover, in contrast to the conditions required for the use of Principal Components Analysis in AFM, QFA estimation remains valid even when the idiosyncratic errors have heavy-tailed distributions. Three empirical applications (regarding macroeconomic, climate and finance panel data) provide evidence that extra factors shifting the quantiles other than the means could be relevant in practice.||||@arxiv||||2019/11/06||||Quantile Factor Models||||Quantile Factor Models (QFM) represent a new class of factor models for high-dimensional panel data. Unlike Approximate Factor Models (AFM), where only location-shifting factors can be extracted,...||||https://arxiv.org/abs/1911.02173v1||||econ||||
767||||None||||Linear Social Learning in Networks with Rational Agents||||arXiv.org||||2020/02/11||||Linear Social Learning in Networks with Rational Agents||||Dasaratha, Krishna || He, Kevin||||https://arxiv.org/pdf/1911.10116||||1911.10116||||We consider a sequential social-learning environment with rational agents and Gaussian private signals, focusing on how the observation network affects the speed of learning. Agents learn about a binary state and take turns choosing actions based on own signals and network neighbors' behavior. Equilibrium learning may be slow when agents do not observe all predecessors, as agents compromise between incorporating the signals of the observed neighbors and not over-counting the confounding signals of the unobserved early movers. We show that on any network, equilibrium actions are a log-linear function of observations and each agent's accuracy admits a signal-counting interpretation. Adding links to the observation network can harm agents even without introducing new confounds. We then consider a network structure where agents move in generations and observe some members of the previous generation. When this observation structure is sufficiently symmetric, the additional information aggregated by each generation is asymptotically equivalent to fewer than two independent signals, even when generations are arbitrarily large. When agents observe all predecessors from the previous generation, social learning aggregates no more than three signals per generation starting from the third generation, and the long-run learning rate is slower when generations are larger.||||@arxiv||||2019/11/22||||Linear Social Learning in Networks with Rational Agents||||We consider a sequential social-learning environment with rational agents and Gaussian private signals, focusing on how the observation network affects the speed of learning. Agents learn about a...||||https://arxiv.org/abs/1911.10116v4||||cs||||
768||||None||||A New Form of Banking -- Concept and Mathematical Model of Venture Banking||||arXiv.org||||2020/01/29||||A New Form of Banking -- Concept and Mathematical Model of Venture Banking||||Hanley, Brian P||||https://arxiv.org/pdf/1810.00516||||1810.00516||||This model contains concept, equations, and graphical results for venture banking. A system of 27 equations describes the behavior of the venture-bank and underwriter system allowing phase-space type graphs that show where profits and losses occur. These results confirm and expand those obtained from the original spreadsheet based model. An example investment in a castle at a loss is provided to clarify concept. This model requires that all investments are in enterprises that create new utility value. The assessed utility value created is the new money out of which the venture bank and underwriter are paid. The model presented chooses parameters that ensure that the venture-bank experiences losses before the underwriter does. Parameters are: DIN Premium, 0.05; Clawback lien fraction, 0.77; Clawback bonds and equity futures discount, 1.5 x (USA 12 month LIBOR); Range of clawback bonds sold, 0 to 100%; Range of equity futures sold 0 to 70%.||||@arxiv||||2018/10/01||||A New Form of Banking -- Concept and Mathematical Model of Venture Banking||||This model contains concept, equations, and graphical results for venture banking. A system of 27 equations describes the behavior of the venture-bank and underwriter system allowing phase-space...||||https://arxiv.org/abs/1810.00516v7||||econ||||
769||||None||||Efficient Minimum Distance Estimation of Pareto Exponent from Top Income Shares||||arXiv.org||||2019/01/08||||Efficient Minimum Distance Estimation of Pareto Exponent from Top Income Shares||||Toda, Alexis Akira || Wang, Yulong||||https://arxiv.org/pdf/1901.02471||||1901.02471||||We propose an efficient estimation method for the income Pareto exponent when only certain top income shares are observable. Our estimator is based on the asymptotic theory of weighted sums of order statistics and the efficient minimum distance estimator. Simulations show that our estimator has excellent finite sample properties. We apply our estimation method to the U.S. top income share data and find that the Pareto exponent has been stable at around 1.5 since 1985, suggesting that the rise in inequality during the last three decades is mainly driven by redistribution between the rich and poor, not among the rich.||||@arxiv||||2019/01/08||||Efficient Minimum Distance Estimation of Pareto Exponent from Top...||||We propose an efficient estimation method for the income Pareto exponent when only certain top income shares are observable. Our estimator is based on the asymptotic theory of weighted sums of...||||https://arxiv.org/abs/1901.02471v1||||econ||||
